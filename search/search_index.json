{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"MeasureIA Documentation Welcome to the fledgling documentation site of MeasureIA , a tool to measure intrinsic alignment correlation functions in hydrodynamic simulations These docs will be updated in due time. Please feel free to contact me (m.l.vanheukelum@uu.nl) if you have any questions or create an issue on GitHub. You can also view the GitHub Pages Home Installation Usage Output filestructure Roadmap API Reference MeasureIABox MeasureIALightcone MeasureIABase MeasureJackknife MeasureWBox MeasureMultipolesBox MeasureWBoxJackknife MeasureMBoxJackknife SimInfo ReadData Contributions Bugs If you find a bug, please report it in a GitHub issue . Features If you would like a feature added that is not already on the Roadmap or in an issue on Github, please create an issue with the request. Within the issue, we can discuss how best to proceed and what the timeline will be. Pull requests that have not been discussed beforehand will not be accepted. Note that the issues on GitHub contain a priority. Please comment on those you would like to have added, if there is enough interest, I will consider increasing the priority status. Citation Please use the CITATION file to cite this package properly. Note that a DOI will be added to this soon. If you would like to cite this package and the DOI is not there, please contact me. Licence MIT","title":"Home"},{"location":"#measureia-documentation","text":"Welcome to the fledgling documentation site of MeasureIA , a tool to measure intrinsic alignment correlation functions in hydrodynamic simulations These docs will be updated in due time. Please feel free to contact me (m.l.vanheukelum@uu.nl) if you have any questions or create an issue on GitHub. You can also view the GitHub","title":"MeasureIA Documentation"},{"location":"#pages","text":"Home Installation Usage Output filestructure Roadmap API Reference MeasureIABox MeasureIALightcone MeasureIABase MeasureJackknife MeasureWBox MeasureMultipolesBox MeasureWBoxJackknife MeasureMBoxJackknife SimInfo ReadData","title":"Pages"},{"location":"#contributions","text":"","title":"Contributions"},{"location":"#bugs","text":"If you find a bug, please report it in a GitHub issue .","title":"Bugs"},{"location":"#features","text":"If you would like a feature added that is not already on the Roadmap or in an issue on Github, please create an issue with the request. Within the issue, we can discuss how best to proceed and what the timeline will be. Pull requests that have not been discussed beforehand will not be accepted. Note that the issues on GitHub contain a priority. Please comment on those you would like to have added, if there is enough interest, I will consider increasing the priority status.","title":"Features"},{"location":"#citation","text":"Please use the CITATION file to cite this package properly. Note that a DOI will be added to this soon. If you would like to cite this package and the DOI is not there, please contact me.","title":"Citation"},{"location":"#licence","text":"MIT","title":"Licence"},{"location":"installation/","text":"Installation You can install MeasureIA via: pip install measureia Note: the package depends on kmeans_radec, which is not pip\u2011installable. You need to install it manually ( see https://github.com/esheldon/kmeans_radec). Alternatively, you can use uv for dependency management: git clone https://github.com/MarloesvL/measure_IA.git cd measure_IA uv sync uv run [script_name].py If not using uv or pip, install dependencies via requirements.txt, remembering the kmeans_radec external dependency.","title":"Installation"},{"location":"installation/#installation","text":"You can install MeasureIA via: pip install measureia Note: the package depends on kmeans_radec, which is not pip\u2011installable. You need to install it manually ( see https://github.com/esheldon/kmeans_radec). Alternatively, you can use uv for dependency management: git clone https://github.com/MarloesvL/measure_IA.git cd measure_IA uv sync uv run [script_name].py If not using uv or pip, install dependencies via requirements.txt, remembering the kmeans_radec external dependency.","title":"Installation"},{"location":"output_structure/","text":"Output file structure Your output file with your own input of [output_file_name, snapshot, dataset_name, num_jk] will have the following structure: [output_file_name] \u2514\u2500\u2500 Snapshot_[snapshot] Optional. If input [snapshot] is None, this group is omitted. \u251c\u2500\u2500 w_gg \u2502 \u251c\u2500\u2500 [dataset_name] w_gg values for each r_p bin \u2502 \u251c\u2500\u2500 [dataset_name]_rp r_p mean bin values \u2502 \u251c\u2500\u2500 [dataset_name]_mean_[num_jk] mean w_gg value of all jackknife realisations \u2502 \u251c\u2500\u2500 [dataset_name]_jackknife_cov_[num_jk] jackknife estimate of covariance matrix \u2502 \u251c\u2500\u2500 [dataset_name]_jackknife_[num_jk] sqrt of diagonal of covariance matrix (size of errorbars) \u2502 \u2514\u2500\u2500 [dataset_name]_jk[num_jk] group containing all jackknife realisations for this dataset \u2502 \u251c\u2500\u2500 [dataset_name]_[i] jackknife realisations with i running from 0 to num_jk - 1 \u2502 \u2514\u2500\u2500 [dataset_name]_[i]_rp r_p bin values of each jackknife realistation \u251c\u2500\u2500 w_g_plus \u2502 \u251c\u2500\u2500 [dataset_name] w_g+ values for each r_p bin \u2502 \u251c\u2500\u2500 [dataset_name]_rp r_p mean bin values \u2502 \u251c\u2500\u2500 [dataset_name]_mean_[num_jk] mean w_g+ value of all jackknife realisations \u2502 \u251c\u2500\u2500 [dataset_name]_jackknife_cov_[num_jk] jackknife estimate of covariance matrix \u2502 \u251c\u2500\u2500 [dataset_name]_jackknife_[num_jk] sqrt of diagonal of covariance matrix (size of errorbars) \u2502 \u2514\u2500\u2500 [dataset_name]_jk[num_jk] group containing all jackknife realisations for this dataset \u2502 \u251c\u2500\u2500 [dataset_name]_[i] jackknife realisations with i running from 0 to num_jk - 1 \u2502 \u2514\u2500\u2500 [dataset_name]_[i]_rp r_p bin values of each jackknife realistation \u2514\u2500\u2500 w \u251c\u2500\u2500 xi_gg \u2502 \u251c\u2500\u2500 [dataset_name] xi_gg grid in (r_p,pi) \u2502 \u251c\u2500\u2500 [dataset_name]_rp r_p mean bin values \u2502 \u251c\u2500\u2500 [dataset_name]_pi pi mean bin values \u2502 \u251c\u2500\u2500 [dataset_name]_RR_gg RR grid in (r_p,pi) \u2502 \u251c\u2500\u2500 [dataset_name]_DD DD grid in (r_p,pi) (pair counts) \u2502 \u2514\u2500\u2500 [dataset_name]_jk[num_jk] group containing all jackknife realisations for this dataset \u2502 \u251c\u2500\u2500 [dataset_name]_[i] jackknife realisations with i running from 0 to num_jk - 1 \u2502 \u2514\u2500\u2500 [dataset_name]_[i]_[x] with x in [rp, pi, RR_gg, DD] as above \u251c\u2500\u2500 xi_g_plus \u2502 \u251c\u2500\u2500 [dataset_name] xi_g+ grid in (rp_,pi) \u2502 \u251c\u2500\u2500 [dataset_name]_rp r_p mean bin values \u2502 \u251c\u2500\u2500 [dataset_name]_pi pi mean bin values \u2502 \u251c\u2500\u2500 [dataset_name]_RR_g_plus RR grid in (r_p,pi) \u2502 \u251c\u2500\u2500 [dataset_name]_SplusD S+D grid in (r_p,pi) \u2502 \u2514\u2500\u2500 [dataset_name]_jk[num_jk] group containing all jackknife realisations for this dataset \u2502 \u251c\u2500\u2500 [dataset_name]_[i] jackknife realisations with i running from 0 to num_jk - 1 \u2502 \u2514\u2500\u2500 [dataset_name]_[i]_[x] with x in [rp, pi, RR_g_plus, SplusD] as above \u2514\u2500\u2500 xi_g_cross \u251c\u2500\u2500 [dataset_name] xi_gx grid in (r_p,pi) \u251c\u2500\u2500 [dataset_name]_rp r_p mean bin values \u251c\u2500\u2500 [dataset_name]_pi pi mean bin values \u251c\u2500\u2500 [dataset_name]_RR_g_cross RR grid in (r_p,pi) \u251c\u2500\u2500 [dataset_name]_ScrossD SxD grid in (r_p,pi) (pair counts) \u2514\u2500\u2500 [dataset_name]_jk[num_jk] group containing all jackknife realisations for this dataset \u251c\u2500\u2500 [dataset_name]_[i] jackknife realisations with i running from 0 to num_jk - 1 \u2514\u2500\u2500 [dataset_name]_[i]_[x] with x in [rp, pi, RR_g_cross, ScrossD] as above If you choose to measure multipoles instead of wg+, all 'w' will be replaced by 'multipoles' - or both will appear, if you have measured both. For the multipoles, all xi_g+, DD (etc) grids are in (r, mu_r), not in (r_p, pi) and the suffixes of the bin values are also replaced by '_r' and '_mu_r' accordingly. In one file, multiple redshift (snapshot) measurements can be saved without being overwritten, as well as the jackknife information for different numbers of jackknife realisations (num_jk) for the same dataset.","title":"Output"},{"location":"output_structure/#output-file-structure","text":"Your output file with your own input of [output_file_name, snapshot, dataset_name, num_jk] will have the following structure: [output_file_name] \u2514\u2500\u2500 Snapshot_[snapshot] Optional. If input [snapshot] is None, this group is omitted. \u251c\u2500\u2500 w_gg \u2502 \u251c\u2500\u2500 [dataset_name] w_gg values for each r_p bin \u2502 \u251c\u2500\u2500 [dataset_name]_rp r_p mean bin values \u2502 \u251c\u2500\u2500 [dataset_name]_mean_[num_jk] mean w_gg value of all jackknife realisations \u2502 \u251c\u2500\u2500 [dataset_name]_jackknife_cov_[num_jk] jackknife estimate of covariance matrix \u2502 \u251c\u2500\u2500 [dataset_name]_jackknife_[num_jk] sqrt of diagonal of covariance matrix (size of errorbars) \u2502 \u2514\u2500\u2500 [dataset_name]_jk[num_jk] group containing all jackknife realisations for this dataset \u2502 \u251c\u2500\u2500 [dataset_name]_[i] jackknife realisations with i running from 0 to num_jk - 1 \u2502 \u2514\u2500\u2500 [dataset_name]_[i]_rp r_p bin values of each jackknife realistation \u251c\u2500\u2500 w_g_plus \u2502 \u251c\u2500\u2500 [dataset_name] w_g+ values for each r_p bin \u2502 \u251c\u2500\u2500 [dataset_name]_rp r_p mean bin values \u2502 \u251c\u2500\u2500 [dataset_name]_mean_[num_jk] mean w_g+ value of all jackknife realisations \u2502 \u251c\u2500\u2500 [dataset_name]_jackknife_cov_[num_jk] jackknife estimate of covariance matrix \u2502 \u251c\u2500\u2500 [dataset_name]_jackknife_[num_jk] sqrt of diagonal of covariance matrix (size of errorbars) \u2502 \u2514\u2500\u2500 [dataset_name]_jk[num_jk] group containing all jackknife realisations for this dataset \u2502 \u251c\u2500\u2500 [dataset_name]_[i] jackknife realisations with i running from 0 to num_jk - 1 \u2502 \u2514\u2500\u2500 [dataset_name]_[i]_rp r_p bin values of each jackknife realistation \u2514\u2500\u2500 w \u251c\u2500\u2500 xi_gg \u2502 \u251c\u2500\u2500 [dataset_name] xi_gg grid in (r_p,pi) \u2502 \u251c\u2500\u2500 [dataset_name]_rp r_p mean bin values \u2502 \u251c\u2500\u2500 [dataset_name]_pi pi mean bin values \u2502 \u251c\u2500\u2500 [dataset_name]_RR_gg RR grid in (r_p,pi) \u2502 \u251c\u2500\u2500 [dataset_name]_DD DD grid in (r_p,pi) (pair counts) \u2502 \u2514\u2500\u2500 [dataset_name]_jk[num_jk] group containing all jackknife realisations for this dataset \u2502 \u251c\u2500\u2500 [dataset_name]_[i] jackknife realisations with i running from 0 to num_jk - 1 \u2502 \u2514\u2500\u2500 [dataset_name]_[i]_[x] with x in [rp, pi, RR_gg, DD] as above \u251c\u2500\u2500 xi_g_plus \u2502 \u251c\u2500\u2500 [dataset_name] xi_g+ grid in (rp_,pi) \u2502 \u251c\u2500\u2500 [dataset_name]_rp r_p mean bin values \u2502 \u251c\u2500\u2500 [dataset_name]_pi pi mean bin values \u2502 \u251c\u2500\u2500 [dataset_name]_RR_g_plus RR grid in (r_p,pi) \u2502 \u251c\u2500\u2500 [dataset_name]_SplusD S+D grid in (r_p,pi) \u2502 \u2514\u2500\u2500 [dataset_name]_jk[num_jk] group containing all jackknife realisations for this dataset \u2502 \u251c\u2500\u2500 [dataset_name]_[i] jackknife realisations with i running from 0 to num_jk - 1 \u2502 \u2514\u2500\u2500 [dataset_name]_[i]_[x] with x in [rp, pi, RR_g_plus, SplusD] as above \u2514\u2500\u2500 xi_g_cross \u251c\u2500\u2500 [dataset_name] xi_gx grid in (r_p,pi) \u251c\u2500\u2500 [dataset_name]_rp r_p mean bin values \u251c\u2500\u2500 [dataset_name]_pi pi mean bin values \u251c\u2500\u2500 [dataset_name]_RR_g_cross RR grid in (r_p,pi) \u251c\u2500\u2500 [dataset_name]_ScrossD SxD grid in (r_p,pi) (pair counts) \u2514\u2500\u2500 [dataset_name]_jk[num_jk] group containing all jackknife realisations for this dataset \u251c\u2500\u2500 [dataset_name]_[i] jackknife realisations with i running from 0 to num_jk - 1 \u2514\u2500\u2500 [dataset_name]_[i]_[x] with x in [rp, pi, RR_g_cross, ScrossD] as above If you choose to measure multipoles instead of wg+, all 'w' will be replaced by 'multipoles' - or both will appear, if you have measured both. For the multipoles, all xi_g+, DD (etc) grids are in (r, mu_r), not in (r_p, pi) and the suffixes of the bin values are also replaced by '_r' and '_mu_r' accordingly. In one file, multiple redshift (snapshot) measurements can be saved without being overwritten, as well as the jackknife information for different numbers of jackknife realisations (num_jk) for the same dataset.","title":"Output file structure"},{"location":"roadmap/","text":"Roadmap From the README, the planned developments include: Full documentation website (this is the start) More exhaustive docstrings and internal method docs Lightcone methods & validation Multiprocessing support for lightcone version Additional optional features (e.g. responsivity factor, alternate definitions) The issues on GitHub are also used as To Do's. Feel free to request features or comment on those already there to let me know you would like them to have a higher priority.","title":"Roadmap"},{"location":"roadmap/#roadmap","text":"From the README, the planned developments include: Full documentation website (this is the start) More exhaustive docstrings and internal method docs Lightcone methods & validation Multiprocessing support for lightcone version Additional optional features (e.g. responsivity factor, alternate definitions) The issues on GitHub are also used as To Do's. Feel free to request features or comment on those already there to let me know you would like them to have a higher priority.","title":"Roadmap"},{"location":"usage/","text":"Usage & Examples You can see examples in the repository under examples/ , e.g.: example_measure_IA_box.py example_measureIA_box.ipynb Here is a minimal usage snippet: from measureia import MeasureIABox import numpy as np data_dict = { \"Position\": np.array([]), \"Position_shape_sample\": np.array([]), \"Axis_Direction\": np.array([]), \"LOS\": 2, \"q\": np.array([]) } mi = MeasureIABox( data=data_dict, output_file_name=\"./outfile.hdf5\", boxsize=205.0, ) mi.measure_xi_w(dataset_name=\"ds1\", corr_type=\"both\", num_jk=27, temp_file_path='./') mi.measure_xi_multipoles(dataset_name=\"ds1\", corr_type=\"both\", num_jk=27, temp_file_path='./')","title":"Usage"},{"location":"usage/#usage-examples","text":"You can see examples in the repository under examples/ , e.g.: example_measure_IA_box.py example_measureIA_box.ipynb Here is a minimal usage snippet: from measureia import MeasureIABox import numpy as np data_dict = { \"Position\": np.array([]), \"Position_shape_sample\": np.array([]), \"Axis_Direction\": np.array([]), \"LOS\": 2, \"q\": np.array([]) } mi = MeasureIABox( data=data_dict, output_file_name=\"./outfile.hdf5\", boxsize=205.0, ) mi.measure_xi_w(dataset_name=\"ds1\", corr_type=\"both\", num_jk=27, temp_file_path='./') mi.measure_xi_multipoles(dataset_name=\"ds1\", corr_type=\"both\", num_jk=27, temp_file_path='./')","title":"Usage &amp; Examples"},{"location":"api/MeasureIABase/","text":"MeasureIABase measureia.MeasureIABase Bases: SimInfo Base class for MeasureIA package that includes some general methods used throughout the package. Attributes: Num_position ( int ) \u2013 Number of objects in the position sample. This value is updated in jackknife realisations. Num_shape ( int ) \u2013 Number of objects in the shape sample. This value is updated in jackknife realisations. r_min ( float ) \u2013 Minimum bound of (projected) separation length; bin edge. Default is 0.1. r_max ( float ) \u2013 Maximum bound of (projected) separation length; bin edge. Default is 20. r_bins ( ndarray ) \u2013 Bin edges of the (projected) separation length (r_p or r). pi_bins ( ndarray ) \u2013 Bin edges of the line of sight (pi). mu_r_bins ( ndarray ) \u2013 Bin edges of the mu_r. Methods: Name Description calculate_dot_product_arrays Calculates dot product of elements of two arrays get_ellipticity Given e and phi, e_+ and e_x components of ellipticity are returned. get_random_pairs Analytical RR for a (rp,pi) bin. get_volume_spherical_cap Volume of an (r,mu_r) bin. get_random_pairs_r_mur Analytical RR for a (r,mu_r) bin. setdiff2D Compares each row of a1 and a2 and returns the elements that do not overlap. setdiff_omit For rows in nested list a1, whose index is included in incl_ind, returns elements that do not overlap between the row in a1 and a2. _measure_w_g_i Measure wgg or wg+ from xi grid provided by MeasureWBox or MeasureWLightcone class methods. _measure_multipoles Measure multipoles (gg or g+) from xi grid provided by MeasureMultipolesBox or MeasureMultipolesLightcone class methods. _obs_estimator Combines elements (DD, RR, etc) of xi estimators into xi_gg or xi_g+ for MeasureIALightcone. assign_jackknife_patches Given positions of multiple samples, defines jackknife patches and returns index of every object in the sample. Notes Inherits attributes from 'SimInfo', where 'boxsize', 'L_0p5' and 'snap_group' are used in this class. Source code in src/measureia/measure_IA_base.py class MeasureIABase(SimInfo): \"\"\"Base class for MeasureIA package that includes some general methods used throughout the package. Attributes ---------- Num_position : int Number of objects in the position sample. This value is updated in jackknife realisations. Num_shape : int Number of objects in the shape sample. This value is updated in jackknife realisations. r_min : float Minimum bound of (projected) separation length; bin edge. Default is 0.1. r_max : float Maximum bound of (projected) separation length; bin edge. Default is 20. r_bins : ndarray Bin edges of the (projected) separation length (r_p or r). pi_bins : ndarray Bin edges of the line of sight (pi). mu_r_bins : ndarray Bin edges of the mu_r. Methods ------- calculate_dot_product_arrays() Calculates dot product of elements of two arrays get_ellipticity() Given e and phi, e_+ and e_x components of ellipticity are returned. get_random_pairs() Analytical RR for a (rp,pi) bin. get_volume_spherical_cap() Volume of an (r,mu_r) bin. get_random_pairs_r_mur() Analytical RR for a (r,mu_r) bin. setdiff2D() Compares each row of a1 and a2 and returns the elements that do not overlap. setdiff_omit() For rows in nested list a1, whose index is included in incl_ind, returns elements that do not overlap between the row in a1 and a2. _measure_w_g_i() Measure wgg or wg+ from xi grid provided by MeasureWBox or MeasureWLightcone class methods. _measure_multipoles() Measure multipoles (gg or g+) from xi grid provided by MeasureMultipolesBox or MeasureMultipolesLightcone class methods. _obs_estimator() Combines elements (DD, RR, etc) of xi estimators into xi_gg or xi_g+ for MeasureIALightcone. assign_jackknife_patches() Given positions of multiple samples, defines jackknife patches and returns index of every object in the sample. Notes ----- Inherits attributes from 'SimInfo', where 'boxsize', 'L_0p5' and 'snap_group' are used in this class. \"\"\" def __init__( self, data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True, ): \"\"\" The __init__ method of the MeasureIABase class. Parameters ---------- data : dict or NoneType Dictionary with data needed for calculations. For cartesian coordinates, the keywords are: 'Position' and 'Position_shape_sample': (N_p,3), (N_s,3) ndarrays with the x, y, z coordinates of the N_p, N_s objects in the position and shape samples, respectively. 'Axis_Direction': (N_s,2) ndarray with the two elements of the unit vectors describing the axis direction of the projected axis of the object shape. 'LOS': index referring back to the column number in the 'Position' samples that contains the line-of-sight coordinate. (e.g. if the shapes are projected over the z-axis, LOS=2) 'q': (N_s) array containing the axis ratio q=b/a for each object in the shape sample. For lightcone coordinates, the keywords are: 'Redshift' and 'Redshift_shape_sample': (N_p) and (N_s) ndarray with redshifts of position and shape samples. 'RA' and 'RA_shape_sample': (N_p) and (N_s) ndarray with RA coordinate of position and shape samples. 'DEC' and 'DEC_shape_sample': (N_p) and (N_s) ndarray with DEC coordinate of position and shape samples. 'e1' and 'e2': (N_s) arrays with the two ellipticity components e1 and e2 of the shape sample objects. output_file_name : str Name and filepath of the file where the output should be stored. Needs to be hdf5-type. simulation : str or NoneType, optional Indicator of simulation, obtaining correct boxsize in cMpc/h automatically. Choose from [TNG100, TNG100_2, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1, FLAMINGO_L2p8]. Default is None, in which case boxsize needs to be added manually; or in the case of observational data, the pi_max. snapshot : int or str or NoneType, optional Number of the snapshot, which, if given, will ensure that the output file to contains a group 'Snapshot_[snapshot]'. If None, the group is omitted from the output file structure. Default is None. separation_limits : iterable of 2 entries, optional Bounds of the (projected) separation vector length bins in cMpc/h (so, r or r_p). Default is [0.1,20]. num_bins_r : int, optional Number of bins for (projected) separation vector. Default is 8. num_bins_pi : int, optional Number of bins for line of sight (LOS) vector, pi or mu_r when multipoles are measured. Default is 20. pi_max : int or float, optional Bound for line of sight bins. Bounds will be [-pi_max, pi_max]. Default is None, in which case half the boxsize will be used. boxsize : int or float or NoneType, optional If simulation is not included in SimInfo, a manual boxsize can be added here. Make sure simulation=None and the boxsize units are equal to those in the data dictionary. Default is None. periodicity : bool, optional If True, the periodic boundary conditions of the simulation box are taken into account. If False, they are ignored. Note that because this code used analytical randoms for the simulations, the correlations will not be correct in this case and only DD and S+D terms should be studied. Non-periodic randoms can be measured by providing random data to the code and considering the DD term that is measured. Correlations and covariance matrix will need to be reconstructed from parts. [Please add a request for teh integration of this method of this if you would like to use this option often.] Default is True. \"\"\" SimInfo.__init__(self, simulation, snapshot, boxsize) self.data = data self.output_file_name = output_file_name self.periodicity = periodicity if periodicity: periodic = \"periodic \" else: periodic = \"\" try: self.Num_position = len(data[\"Position\"]) # number of halos in position sample self.Num_shape = len(data[\"Position_shape_sample\"]) # number of halos in shape sample except: try: self.Num_position = len(data[\"RA\"]) self.Num_shape = len(data[\"RA_shape_sample\"]) except: self.Num_position = 0 self.Num_shape = 0 print(\"Warning: no Postion or Position_shape_sample given.\") if self.Num_position > 0: try: weight = self.data[\"weight\"] except: self.data[\"weight\"] = np.ones(self.Num_position) try: weight = self.data[\"weight_shape_sample\"] except: self.data[\"weight_shape_sample\"] = np.ones(self.Num_shape) self.r_min = separation_limits[0] # cMpc/h self.r_max = separation_limits[1] # cMpc/h self.num_bins_r = num_bins_r self.num_bins_pi = num_bins_pi self.r_bins = np.logspace(np.log10(self.r_min), np.log10(self.r_max), self.num_bins_r + 1) if pi_max == None: if self.L_0p5 is None: raise ValueError( \"Both pi_max and boxsize are None. Provide input on one of them to determine the integration limit pi_max.\") else: pi_max = self.L_0p5 self.pi_bins = np.linspace(-pi_max, pi_max, self.num_bins_pi + 1) self.mu_r_bins = np.linspace(-1, 1, self.num_bins_pi + 1) if simulation == False: print(f\"MeasureIA object initialised with:\\n \\ observational data.\\n \\ There are {self.Num_shape} galaxies in the shape sample and {self.Num_position} galaxies in the position sample.\\n\\ The separation bin edges are given by {self.r_bins} Mpc.\\n \\ There are {num_bins_r} r or r_p bins and {num_bins_pi} pi bins.\\n \\ The maximum pi used for binning is {pi_max}.\\n \\ The data will be written to {self.output_file_name}\") else: print(f\"MeasureIA object initialised with:\\n \\ simulation {simulation} that has a {periodic}boxsize of {self.boxsize} cMpc/h.\\n \\ There are {self.Num_shape} galaxies in the shape sample and {self.Num_position} galaxies in the position sample.\\n\\ The separation bin edges are given by {self.r_bins} cMpc/h.\\n \\ There are {num_bins_r} r or r_p bins and {num_bins_pi} pi bins.\\n \\ The maximum pi used for binning is {pi_max}.\\n \\ The data will be written to {self.output_file_name}\") return @staticmethod def calculate_dot_product_arrays(a1, a2): \"\"\"Calculates the dot product over 2 2D arrays across axis 1 so that dot_product[i] = np.dot(a1[i],a2[i]) Parameters ---------- a1 : ndarray First array a2 : ndarray Second array Returns ------- ndarray Dot product of columns of arrays \"\"\" dot_product = np.zeros(np.shape(a1)[0]) for i in np.arange(0, np.shape(a1)[1]): dot_product += a1[:, i] * a2[:, i] return dot_product @staticmethod def get_ellipticity(e, phi): \"\"\"Calculates the radial and tangential components of the ellipticity, given the size of the ellipticty vector and the angle between the semimajor or semiminor axis and the separation vector. Parameters ---------- e : ndarray size of the ellipticity vector phi : ndarray angle between semimajor/semiminor axis and separation vector Returns ------- ndarray e_+ and e_x \"\"\" e_plus, e_cross = e * np.cos(2 * phi), e * np.sin(2 * phi) return e_plus, e_cross @staticmethod def get_random_pairs(rp_max, rp_min, pi_max, pi_min, L3, corrtype, Num_position, Num_shape): \"\"\"Returns analytical value of the number of pairs expected in an r_p, pi bin for a random uniform distribution. (Singh et al. 2023) Parameters ---------- rp_max : float Upper bound of projected separation vector bin rp_min : float Lower bound of projected separation vector bin pi_max : float Upper bound of line of sight vector bin pi_min : float Lower bound of line of sight vector bin L3 : float or int Volume of the simulation box corrtype : str Correlation type, auto or cross. RR for auto is RR_cross/2. Num_position : int Number of objects in the position sample. Num_shape : int Number of objects in the shape sample. Returns ------- float number of pairs in r_p, pi bin \"\"\" if corrtype == \"auto\": RR = ( (Num_position - 1.0) * Num_shape / 2.0 * np.pi * (rp_max ** 2 - rp_min ** 2) * abs(pi_max - pi_min) / L3 ) # volume is cylindrical pi*dr^2 * height elif corrtype == \"cross\": RR = Num_position * Num_shape * np.pi * (rp_max ** 2 - rp_min ** 2) * abs(pi_max - pi_min) / L3 else: raise ValueError(\"Unknown input for corrtype, choose from auto or cross.\") return RR @staticmethod def get_volume_spherical_cap(mur, r): \"\"\"Calculate the volume of a spherical cap. Parameters ---------- mur : float cos(theta), where theta is the polar angle between the apex and disk of the cap. r : float radius Returns ------- float Volume of the spherical cap. \"\"\" return np.pi / 3.0 * r ** 3 * (2 + mur) * (1 - mur) ** 2 def get_random_pairs_r_mur(self, r_max, r_min, mur_max, mur_min, L3, corrtype, Num_position, Num_shape): \"\"\"Returns analytical value of the number of pairs expected in an r, mu_r bin for a random uniform distribution. Parameters ---------- r_max : float Upper bound of separation vector bin r_min : float Lower bound of separation vector bin mur_max : float Upper bound of mu_r bin mur_min : float Lower bound of mu_r bin L3 : float Volume of the simulation box corrtype : str Correlation type, auto or cross. RR for auto is RR_cross/2. Num_position : int Number of objects in the position sample. Num_shape : int Number of objects in the shape sample. Returns ------- float number of pairs in r, mu_r bin \"\"\" if corrtype == \"auto\": RR = ( (Num_position - 1.0) / 2.0 * Num_shape * ( self.get_volume_spherical_cap(mur_min, r_max) - self.get_volume_spherical_cap(mur_max, r_max) - (self.get_volume_spherical_cap(mur_min, r_min) - self.get_volume_spherical_cap(mur_max, r_min)) ) / L3 ) # volume is big cap - small cap for large - small radius elif corrtype == \"cross\": RR = ( (Num_position - 1.0) * Num_shape * ( self.get_volume_spherical_cap(mur_min, r_max) - self.get_volume_spherical_cap(mur_max, r_max) - (self.get_volume_spherical_cap(mur_min, r_min) - self.get_volume_spherical_cap(mur_max, r_min)) ) / L3 ) else: raise ValueError(\"Unknown input for corrtype, choose from auto or cross.\") return abs(RR) @staticmethod def setdiff2D(a1, a2): \"\"\"Compares each row of a1 and a2 and returns the elements that do not overlap Parameters ---------- a1 : nested list List containing lists of elements to compare to a2 a2 : nested list List containing lists of elements to compare to a1 Returns ------- nested list For each row, the not-overlapping elements between a1 and a2 \"\"\" assert len(a1) == len(a2), \"Lengths of lists where each row is to be compared, are not the same.\" diff = [] for i in np.arange(0, len(a1)): setdiff = np.setdiff1d(a1[i], a2[i]) diff.append(setdiff) del setdiff return diff @staticmethod def setdiff_omit(a1, a2, incl_ind): \"\"\"For rows in nested list a1, whose index is included in incl_ind, returns elements that do not overlap between the row in a1 and a2. Parameters ---------- a1 : nested list List of lists or arrays where indicated rows need to be compared to a2 a2 : list or array Elements to be compared to the row in a1 [and not included in return values]. incl_ind : list or array Indices of rows in a1 to be compared to a2. Returns ------- nested list For each included row in a1, the not-overlapping elements between a1 and a2 \"\"\" diff = [] for i in np.arange(0, len(a1)): if np.isin(i, incl_ind): setdiff = np.setdiff1d(a1[i], a2) diff.append(setdiff) del setdiff return diff def _get_jackknife_region_indices(self, masks, L_subboxes): \"\"\" Split the box in L_subboxes^3 subboxes and return indices of which subbox objects are in for position and shape sample. Parameters ---------- masks: dict or NoneType Input in methods in MeasureIABox that masks the input data dictionary. L_subboxes: int Number of subboxes on one side of the box. L_subboxes^3 is the total number of jackknife realisations. Returns ------- ndarrays indices of jackknife region of position sample and indices of jackknife region of shape sample \"\"\" if masks == None: positions = self.data[\"Position\"] positions_shape_sample = self.data[\"Position_shape_sample\"] else: positions = self.data[\"Position\"][masks[\"Position\"]] positions_shape_sample = self.data[\"Position_shape_sample\"][masks[\"Position_shape_sample\"]] L_sub = self.L_0p5 * 2.0 / L_subboxes jackknife_region_indices_pos = np.zeros(len(positions)) jackknife_region_indices_shape = np.zeros(len(positions_shape_sample)) num_box = 0 for i in np.arange(0, L_subboxes): for j in np.arange(0, L_subboxes): for k in np.arange(0, L_subboxes): x_bounds = [i * L_sub, (i + 1) * L_sub] y_bounds = [j * L_sub, (j + 1) * L_sub] z_bounds = [k * L_sub, (k + 1) * L_sub] x_mask = (positions[:, 0] > x_bounds[0]) * (positions[:, 0] < x_bounds[1]) y_mask = (positions[:, 1] > y_bounds[0]) * (positions[:, 1] < y_bounds[1]) z_mask = (positions[:, 2] > z_bounds[0]) * (positions[:, 2] < z_bounds[1]) x_mask_shape = (positions_shape_sample[:, 0] > x_bounds[0]) * ( positions_shape_sample[:, 0] < x_bounds[1]) y_mask_shape = (positions_shape_sample[:, 1] > y_bounds[0]) * ( positions_shape_sample[:, 1] < y_bounds[1]) z_mask_shape = (positions_shape_sample[:, 2] > z_bounds[0]) * ( positions_shape_sample[:, 2] < z_bounds[1]) mask_position = x_mask * y_mask * z_mask # mask that is True for all positions in the subbox mask_shape = x_mask_shape * y_mask_shape * z_mask_shape # mask that is True for all positions not in the subbox jackknife_region_indices_pos[mask_position] = num_box jackknife_region_indices_shape[mask_shape] = num_box num_box += 1 return np.array(jackknife_region_indices_pos, dtype=int), np.array(jackknife_region_indices_shape, dtype=int) def _combine_jackknife_information(self, dataset_name, jk_group_name, corr_group, num_box, return_output=False): \"\"\" Combine jackknife realisations into a covariance matrix. Parameters ---------- dataset_name: str Name of the dataset in the output file. jk_group_name: str Name of the subgroup in the output file where the jackknife realisations are saved. corr_group: list of str Name of the subgroups in the output file denoting the correlation (e.g. w_g_plus, multipoles_gg etc). num_box: int Number of jackknife realisations. return_output: bool, optional When True, returns output, otherwise saves to output file. Returns ------- list of ndarrays list of covariances for each entry in corr_group and list of standard deviations for each entry in corr_group \"\"\" covs, stds = [], [] for d in np.arange(0, len(corr_group)): data_file = h5py.File(self.output_file_name, \"a\") group_multipoles = data_file[f\"{self.snap_group}/{corr_group[d]}/{jk_group_name}/\"] # calculating mean of the datavectors mean_multipoles = np.zeros(self.num_bins_r) for b in np.arange(0, num_box): mean_multipoles += group_multipoles[dataset_name + \"_\" + str(b)][:] mean_multipoles /= num_box # calculation the covariance matrix (multipoles) and the standard deviation (sqrt of diag of cov) cov = np.zeros((self.num_bins_r, self.num_bins_r)) std = np.zeros(self.num_bins_r) for b in np.arange(0, num_box): std += (group_multipoles[dataset_name + \"_\" + str(b)][:] - mean_multipoles) ** 2 for i in np.arange(self.num_bins_r): cov[:, i] += (group_multipoles[dataset_name + \"_\" + str(b)][:] - mean_multipoles) * ( group_multipoles[dataset_name + \"_\" + str(b)][i] - mean_multipoles[i] ) std *= (num_box - 1) / num_box # see Singh 2023 std = np.sqrt(std) # size of errorbars cov *= (num_box - 1) / num_box # cov not sqrt so to get std, sqrt of diag would need to be taken data_file.close() if return_output: covs.append(cov) stds.append(std) else: output_file = h5py.File(self.output_file_name, \"a\") group_multipoles = create_group_hdf5(output_file, f\"{self.snap_group}/\" + corr_group[d]) write_dataset_hdf5(group_multipoles, dataset_name + \"_mean_\" + str(num_box), data=mean_multipoles) write_dataset_hdf5(group_multipoles, dataset_name + \"_jackknife_\" + str(num_box), data=std) write_dataset_hdf5(group_multipoles, dataset_name + \"_jackknife_cov_\" + str(num_box), data=cov) output_file.close() if return_output: return covs, stds else: return def _measure_w_g_i(self, dataset_name, corr_type=\"both\", return_output=False, jk_group_name=\"\"): \"\"\"Measures w_gg or w_g+ for a given xi_gi dataset that has been calculated with the _measure_xi_rp_pi_sims methods. Integrates over pi bins via sum * dpi. Stores rp, and w_gg or w_g+. Parameters ---------- dataset_name : str Name of xi_gg or xi_g+ dataset and name given to w_gg or w_g+ dataset when stored. return_output : bool, optional Output is returned if True, saved to file if False. Default value = False corr_type : str, optional Type of correlation function. Choose from [g+,gg,both]. Default value = \"both\" jk_group_name : str, optional Name of subgroup in hdf5 file where jackknife realisations are stored. Default value = \"\" Returns ------- ndarray [rp, wgg] or [rp, wg+] if return_output is True \"\"\" if corr_type == \"both\": xi_data = [\"xi_g_plus\", \"xi_gg\"] wg_data = [\"w_g_plus\", \"w_gg\"] elif corr_type == \"g+\": xi_data = [\"xi_g_plus\"] wg_data = [\"w_g_plus\"] elif corr_type == \"gg\": xi_data = [\"xi_gg\"] wg_data = [\"w_gg\"] else: raise KeyError(\"Unknown value for corr_type. Choose from [g+, gg, both]\") for i in np.arange(0, len(xi_data)): correlation_data_file = h5py.File(self.output_file_name, \"a\") group = correlation_data_file[f\"{self.snap_group}/w/{xi_data[i]}/{jk_group_name}\"] correlation_data = group[dataset_name][:] pi = group[dataset_name + \"_pi\"] rp = group[dataset_name + \"_rp\"] dpi = (self.pi_bins[1:] - self.pi_bins[:-1]) pi_bins = self.pi_bins[:-1] + abs(dpi) / 2.0 # middle of bins # variance = group[dataset_name + \"_sigmasq\"][:] if sum(np.isin(pi, pi_bins)) == len(pi): dpi = np.array([dpi] * len(correlation_data[:, 0])) correlation_data = correlation_data * abs(dpi) # sigsq_el = variance * dpi ** 2 else: raise ValueError(\"Update pi bins in initialisation of object to match xi_g_plus dataset.\") w_g_i = np.sum(correlation_data, axis=1) # sum over pi values # sigsq = np.sum(sigsq_el, axis=1) if return_output: output_data = np.array([rp, w_g_i]).transpose() correlation_data_file.close() return output_data else: group_out = create_group_hdf5(correlation_data_file, f\"{self.snap_group}/{wg_data[i]}/{jk_group_name}\") write_dataset_hdf5(group_out, dataset_name + \"_rp\", data=rp) write_dataset_hdf5(group_out, dataset_name, data=w_g_i) # write_dataset_hdf5(group_out, dataset_name + \"_sigma\", data=np.sqrt(sigsq)) correlation_data_file.close() return def _measure_multipoles(self, dataset_name, corr_type=\"both\", return_output=False, jk_group_name=\"\"): \"\"\"Measures multipoles for a given xi_g+ or xi_gg measured by _measure_xi_r_pi_sims methods. The data assumes xi_g+ and xi_gg to be measured in bins of r and mu_r. Parameters ---------- dataset_name : str Name of xi_gg or xi_g+ dataset and name given to multipoles dataset when stored. corr_type : str, optional Type of correlation function. Choose from [g+,gg,both]. Default value = \"both\" return_output : bool, optional Output is returned if True, saved to file if False. Default value = False. jk_group_name : str, optional Name of subgroup in hdf5 file where jackknife realisations are stored. Default value = \"\" Returns ------- ndarray [r, multipoles_gg] or [r, multipoles_g+] if return_output is True \"\"\" correlation_data_file = h5py.File(self.output_file_name, \"a\") if corr_type == \"g+\": # todo: expand to include ++ option group = correlation_data_file[f\"{self.snap_group}/multipoles/xi_g_plus/{jk_group_name}\"] correlation_data_list = [group[dataset_name][:]] # xi_g+ in grid of r,mur r_list = [group[dataset_name + \"_r\"][:]] mu_r_list = [group[dataset_name + \"_mu_r\"][:]] sab_list = [2] l_list = sab_list corr_type_list = [\"g_plus\"] elif corr_type == \"gg\": group = correlation_data_file[f\"{self.snap_group}/multipoles/xi_gg/{jk_group_name}\"] correlation_data_list = [group[dataset_name][:]] # xi_g+ in grid of rp,pi r_list = [group[dataset_name + \"_r\"][:]] mu_r_list = [group[dataset_name + \"_mu_r\"][:]] sab_list = [0] l_list = sab_list corr_type_list = [\"gg\"] elif corr_type == \"both\": group = correlation_data_file[f\"{self.snap_group}/multipoles/xi_g_plus/{jk_group_name}\"] correlation_data_list = [group[dataset_name][:]] # xi_g+ in grid of rp,pi r_list = [group[dataset_name + \"_r\"][:]] mu_r_list = [group[dataset_name + \"_mu_r\"][:]] group = correlation_data_file[f\"{self.snap_group}/multipoles/xi_gg/{jk_group_name}\"] correlation_data_list.append(group[dataset_name][:]) # xi_g+ in grid of rp,pi r_list.append(group[dataset_name + \"_r\"][:]) mu_r_list.append(group[dataset_name + \"_mu_r\"][:]) sab_list = [2, 0] l_list = sab_list corr_type_list = [\"g_plus\", \"gg\"] else: raise KeyError(\"Unknown value for corr_type. Choose from [g+, gg, both]\") for i in np.arange(0, len(sab_list)): corr_type_i = corr_type_list[i] correlation_data = correlation_data_list[i] r = r_list[i] mu_r = mu_r_list[i] sab = sab_list[i] l = l_list[i] L = np.zeros((len(r), len(mu_r))) mu_r = np.array(list(mu_r) * len(r)).reshape((len(r), len(mu_r))) # make pi into grid for mu r = np.array(list(r) * len(mu_r)).reshape((len(r), len(mu_r))) r = r.transpose() for n in np.arange(0, len(mu_r[:, 0])): for m in np.arange(0, len(mu_r[0])): L_m, dL = lpmn(l, sab, mu_r[n, m]) # make associated Legendre polynomial grid L[n, m] = L_m[-1, -1] # grid ranges from 0 to sab and 0 to l, so last element is what we seek dmur = (self.mu_r_bins[1:] - self.mu_r_bins[:-1]) dmu_r_array = np.array(list(dmur) * len(r)).reshape((len(r), len(dmur))) multipoles = ( (2 * l + 1) / 2.0 * math.factorial(l - sab) / math.factorial(l + sab) * L * correlation_data * dmu_r_array ) multipoles = np.sum(multipoles, axis=1) dsep = (self.r_bins[1:] - self.r_bins[:-1]) / 2.0 separation = self.r_bins[:-1] + abs(dsep) # middle of bins if return_output: correlation_data_file.close() np.array([separation, multipoles]).transpose() else: group_out = create_group_hdf5( correlation_data_file, f\"{self.snap_group}/multipoles_{corr_type_i}/{jk_group_name}\" ) write_dataset_hdf5(group_out, dataset_name + \"_r\", data=separation) write_dataset_hdf5(group_out, dataset_name, data=multipoles) correlation_data_file.close() return def _obs_estimator(self, corr_type, IA_estimator, dataset_name, dataset_name_randoms, num_samples, jk_group_name=\"\", jk_group_name_randoms=\"\"): \"\"\"Reads various components of xi and combines into correct estimator for cluster or galaxy lightcone alignment correlations. It then writes the xi_gg or xi_g+ in the correct place in the output file. Parameters ---------- corr_type : list of 2 str elements First element: ['gg', 'g+', 'both'], second: 'w' or 'multipoles' IA_estimator : str Chooser from 'clusters' or 'galaxies' for different estimator definition. dataset_name : str Name of the dataset dataset_name_randoms : str Name of the dataset for data with randoms as positions num_samples : dict Dictionary of samples sizes for position, shape and random samples. Keywords: D, S, R_D, R_S jk_group_name : str Name of subgroup in hdf5 file where jackknife realisations are stored. Default value = \"\" Returns ------- \"\"\" output_file = h5py.File(self.output_file_name, \"a\") if corr_type[0] == \"g+\" or corr_type[0] == \"both\": group_gp = output_file[ f\"{self.snap_group}/{corr_type[1]}/xi_g_plus/{jk_group_name}\"] group_gp_r = output_file[ f\"{self.snap_group}/{corr_type[1]}/xi_g_plus/{jk_group_name_randoms}\"] SpD = group_gp[f\"{dataset_name}_SplusD\"][:] SpR = group_gp_r[f\"{dataset_name_randoms}_SplusD\"][:] group_gg = output_file[f\"{self.snap_group}/{corr_type[1]}/xi_gg/{jk_group_name}\"] group_gg_r = output_file[f\"{self.snap_group}/{corr_type[1]}/xi_gg/{jk_group_name_randoms}\"] DD = group_gg[f\"{dataset_name}_DD\"][:] if IA_estimator == \"clusters\": if corr_type[0] == \"gg\": SR = group_gg[f\"{dataset_name}_SR\"][:] else: SR = group_gg_r[f\"{dataset_name_randoms}_DD\"][:] SR *= num_samples[\"D\"] / num_samples[\"R_D\"] if corr_type[0] == \"g+\" or corr_type[0] == \"both\": SpR *= num_samples[\"D\"] / num_samples[\"R_D\"] correlation_gp = SpD / DD - SpR / SR write_dataset_hdf5(group_gp, dataset_name, correlation_gp) if corr_type[0] == \"gg\" or corr_type[0] == \"both\": RD = group_gg[f\"{dataset_name}_RD\"][:] RR = group_gg[f\"{dataset_name}_RR\"][:] RD *= num_samples[\"S\"] / num_samples[\"R_S\"] RR *= (num_samples[\"S\"] / num_samples[\"R_S\"]) * (num_samples[\"D\"] / num_samples[\"R_D\"]) correlation_gg = (DD - RD - SR) / RR - 1 write_dataset_hdf5(group_gg, dataset_name, correlation_gg) elif IA_estimator == \"galaxies\": RR = group_gg[f\"{dataset_name}_RR\"][:] RR *= (num_samples[\"S\"] / num_samples[\"R_S\"]) * (num_samples[\"D\"] / num_samples[\"R_D\"]) if corr_type[0] == \"g+\" or corr_type[0] == \"both\": SpR *= num_samples[\"D\"] / num_samples[\"R_D\"] correlation_gp = (SpD - SpR) / RR write_dataset_hdf5(group_gp, dataset_name, correlation_gp) if corr_type[0] == \"gg\" or corr_type[0] == \"both\": RD = group_gg[f\"{dataset_name}_RD\"][:] if corr_type[0] == \"gg\": SR = group_gg[f\"{dataset_name}_SR\"][:] else: SR = group_gg_r[f\"{dataset_name_randoms}_DD\"][:] RD *= num_samples[\"S\"] / num_samples[\"R_S\"] SR *= num_samples[\"D\"] / num_samples[\"R_D\"] correlation_gg = (DD - RD - SR) / RR - 1 write_dataset_hdf5(group_gg, dataset_name, correlation_gg) else: raise ValueError(\"Unknown input for IA_estimator, choose from [clusters, galaxies].\") output_file.close() return def assign_jackknife_patches(self, data, randoms_data, num_jk): \"\"\"Assigns jackknife patches to data and randoms given a number of patches. Based on https://github.com/esheldon/kmeans_radec Parameters ---------- data : dict Dictionary containing position and shape sample data. Keywords: \"RA\", \"DEC\", \"RA_shape_sample\", \"DEC_shape_sample\" randoms_data : dict Dictionary containing position and shape sample data of randoms. Keywords: \"RA\", \"DEC\", \"RA_shape_sample\", \"DEC_shape_sample\" num_jk : int Number of jackknife patches Returns ------- dict Dictionary with patch numbers for each sample. Keywords: 'position', 'shape', 'randoms_position', 'randoms_shape' \"\"\" jk_patches = {} # Read the randoms file from which the jackknife regions will be created RA = randoms_data['RA'] DEC = randoms_data['DEC'] # Define a number of jaccknife regions and find their centres using kmans X = np.column_stack((RA, DEC)) km = kmeans_sample(X, num_jk, maxiter=100, tol=1.0e-5) jk_labels = km.labels jk_patches['randoms_position'] = jk_labels RA = randoms_data['RA_shape_sample'] DEC = randoms_data['DEC_shape_sample'] X2 = np.column_stack((RA, DEC)) jk_labels = km.find_nearest(X2) jk_patches['randoms_shape'] = jk_labels RA = data['RA'] DEC = data['DEC'] X2 = np.column_stack((RA, DEC)) jk_labels = km.find_nearest(X2) jk_patches['position'] = jk_labels RA = data['RA_shape_sample'] DEC = data['DEC_shape_sample'] X2 = np.column_stack((RA, DEC)) jk_labels = km.find_nearest(X2) jk_patches['shape'] = jk_labels return jk_patches __init__(data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True) The init method of the MeasureIABase class. Parameters: data ( dict or NoneType ) \u2013 Dictionary with data needed for calculations. For cartesian coordinates, the keywords are: 'Position' and 'Position_shape_sample': (N_p,3), (N_s,3) ndarrays with the x, y, z coordinates of the N_p, N_s objects in the position and shape samples, respectively. 'Axis_Direction': (N_s,2) ndarray with the two elements of the unit vectors describing the axis direction of the projected axis of the object shape. 'LOS': index referring back to the column number in the 'Position' samples that contains the line-of-sight coordinate. (e.g. if the shapes are projected over the z-axis, LOS=2) 'q': (N_s) array containing the axis ratio q=b/a for each object in the shape sample. For lightcone coordinates, the keywords are: 'Redshift' and 'Redshift_shape_sample': (N_p) and (N_s) ndarray with redshifts of position and shape samples. 'RA' and 'RA_shape_sample': (N_p) and (N_s) ndarray with RA coordinate of position and shape samples. 'DEC' and 'DEC_shape_sample': (N_p) and (N_s) ndarray with DEC coordinate of position and shape samples. 'e1' and 'e2': (N_s) arrays with the two ellipticity components e1 and e2 of the shape sample objects. output_file_name ( str ) \u2013 Name and filepath of the file where the output should be stored. Needs to be hdf5-type. simulation ( str or NoneType , default: None ) \u2013 Indicator of simulation, obtaining correct boxsize in cMpc/h automatically. Choose from [TNG100, TNG100_2, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1, FLAMINGO_L2p8]. Default is None, in which case boxsize needs to be added manually; or in the case of observational data, the pi_max. snapshot ( int or str or NoneType , default: None ) \u2013 Number of the snapshot, which, if given, will ensure that the output file to contains a group 'Snapshot_[snapshot]'. If None, the group is omitted from the output file structure. Default is None. separation_limits ( iterable of 2 entries , default: [0.1, 20.0] ) \u2013 Bounds of the (projected) separation vector length bins in cMpc/h (so, r or r_p). Default is [0.1,20]. num_bins_r ( int , default: 8 ) \u2013 Number of bins for (projected) separation vector. Default is 8. num_bins_pi ( int , default: 20 ) \u2013 Number of bins for line of sight (LOS) vector, pi or mu_r when multipoles are measured. Default is 20. pi_max ( int or float , default: None ) \u2013 Bound for line of sight bins. Bounds will be [-pi_max, pi_max]. Default is None, in which case half the boxsize will be used. boxsize ( int or float or NoneType , default: None ) \u2013 If simulation is not included in SimInfo, a manual boxsize can be added here. Make sure simulation=None and the boxsize units are equal to those in the data dictionary. Default is None. periodicity ( bool , default: True ) \u2013 If True, the periodic boundary conditions of the simulation box are taken into account. If False, they are ignored. Note that because this code used analytical randoms for the simulations, the correlations will not be correct in this case and only DD and S+D terms should be studied. Non-periodic randoms can be measured by providing random data to the code and considering the DD term that is measured. Correlations and covariance matrix will need to be reconstructed from parts. [Please add a request for teh integration of this method of this if you would like to use this option often.] Default is True. Source code in src/measureia/measure_IA_base.py def __init__( self, data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True, ): \"\"\" The __init__ method of the MeasureIABase class. Parameters ---------- data : dict or NoneType Dictionary with data needed for calculations. For cartesian coordinates, the keywords are: 'Position' and 'Position_shape_sample': (N_p,3), (N_s,3) ndarrays with the x, y, z coordinates of the N_p, N_s objects in the position and shape samples, respectively. 'Axis_Direction': (N_s,2) ndarray with the two elements of the unit vectors describing the axis direction of the projected axis of the object shape. 'LOS': index referring back to the column number in the 'Position' samples that contains the line-of-sight coordinate. (e.g. if the shapes are projected over the z-axis, LOS=2) 'q': (N_s) array containing the axis ratio q=b/a for each object in the shape sample. For lightcone coordinates, the keywords are: 'Redshift' and 'Redshift_shape_sample': (N_p) and (N_s) ndarray with redshifts of position and shape samples. 'RA' and 'RA_shape_sample': (N_p) and (N_s) ndarray with RA coordinate of position and shape samples. 'DEC' and 'DEC_shape_sample': (N_p) and (N_s) ndarray with DEC coordinate of position and shape samples. 'e1' and 'e2': (N_s) arrays with the two ellipticity components e1 and e2 of the shape sample objects. output_file_name : str Name and filepath of the file where the output should be stored. Needs to be hdf5-type. simulation : str or NoneType, optional Indicator of simulation, obtaining correct boxsize in cMpc/h automatically. Choose from [TNG100, TNG100_2, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1, FLAMINGO_L2p8]. Default is None, in which case boxsize needs to be added manually; or in the case of observational data, the pi_max. snapshot : int or str or NoneType, optional Number of the snapshot, which, if given, will ensure that the output file to contains a group 'Snapshot_[snapshot]'. If None, the group is omitted from the output file structure. Default is None. separation_limits : iterable of 2 entries, optional Bounds of the (projected) separation vector length bins in cMpc/h (so, r or r_p). Default is [0.1,20]. num_bins_r : int, optional Number of bins for (projected) separation vector. Default is 8. num_bins_pi : int, optional Number of bins for line of sight (LOS) vector, pi or mu_r when multipoles are measured. Default is 20. pi_max : int or float, optional Bound for line of sight bins. Bounds will be [-pi_max, pi_max]. Default is None, in which case half the boxsize will be used. boxsize : int or float or NoneType, optional If simulation is not included in SimInfo, a manual boxsize can be added here. Make sure simulation=None and the boxsize units are equal to those in the data dictionary. Default is None. periodicity : bool, optional If True, the periodic boundary conditions of the simulation box are taken into account. If False, they are ignored. Note that because this code used analytical randoms for the simulations, the correlations will not be correct in this case and only DD and S+D terms should be studied. Non-periodic randoms can be measured by providing random data to the code and considering the DD term that is measured. Correlations and covariance matrix will need to be reconstructed from parts. [Please add a request for teh integration of this method of this if you would like to use this option often.] Default is True. \"\"\" SimInfo.__init__(self, simulation, snapshot, boxsize) self.data = data self.output_file_name = output_file_name self.periodicity = periodicity if periodicity: periodic = \"periodic \" else: periodic = \"\" try: self.Num_position = len(data[\"Position\"]) # number of halos in position sample self.Num_shape = len(data[\"Position_shape_sample\"]) # number of halos in shape sample except: try: self.Num_position = len(data[\"RA\"]) self.Num_shape = len(data[\"RA_shape_sample\"]) except: self.Num_position = 0 self.Num_shape = 0 print(\"Warning: no Postion or Position_shape_sample given.\") if self.Num_position > 0: try: weight = self.data[\"weight\"] except: self.data[\"weight\"] = np.ones(self.Num_position) try: weight = self.data[\"weight_shape_sample\"] except: self.data[\"weight_shape_sample\"] = np.ones(self.Num_shape) self.r_min = separation_limits[0] # cMpc/h self.r_max = separation_limits[1] # cMpc/h self.num_bins_r = num_bins_r self.num_bins_pi = num_bins_pi self.r_bins = np.logspace(np.log10(self.r_min), np.log10(self.r_max), self.num_bins_r + 1) if pi_max == None: if self.L_0p5 is None: raise ValueError( \"Both pi_max and boxsize are None. Provide input on one of them to determine the integration limit pi_max.\") else: pi_max = self.L_0p5 self.pi_bins = np.linspace(-pi_max, pi_max, self.num_bins_pi + 1) self.mu_r_bins = np.linspace(-1, 1, self.num_bins_pi + 1) if simulation == False: print(f\"MeasureIA object initialised with:\\n \\ observational data.\\n \\ There are {self.Num_shape} galaxies in the shape sample and {self.Num_position} galaxies in the position sample.\\n\\ The separation bin edges are given by {self.r_bins} Mpc.\\n \\ There are {num_bins_r} r or r_p bins and {num_bins_pi} pi bins.\\n \\ The maximum pi used for binning is {pi_max}.\\n \\ The data will be written to {self.output_file_name}\") else: print(f\"MeasureIA object initialised with:\\n \\ simulation {simulation} that has a {periodic}boxsize of {self.boxsize} cMpc/h.\\n \\ There are {self.Num_shape} galaxies in the shape sample and {self.Num_position} galaxies in the position sample.\\n\\ The separation bin edges are given by {self.r_bins} cMpc/h.\\n \\ There are {num_bins_r} r or r_p bins and {num_bins_pi} pi bins.\\n \\ The maximum pi used for binning is {pi_max}.\\n \\ The data will be written to {self.output_file_name}\") return calculate_dot_product_arrays(a1, a2) staticmethod Calculates the dot product over 2 2D arrays across axis 1 so that dot_product[i] = np.dot(a1[i],a2[i]) Parameters: a1 ( ndarray ) \u2013 First array a2 ( ndarray ) \u2013 Second array Returns: ndarray \u2013 Dot product of columns of arrays Source code in src/measureia/measure_IA_base.py @staticmethod def calculate_dot_product_arrays(a1, a2): \"\"\"Calculates the dot product over 2 2D arrays across axis 1 so that dot_product[i] = np.dot(a1[i],a2[i]) Parameters ---------- a1 : ndarray First array a2 : ndarray Second array Returns ------- ndarray Dot product of columns of arrays \"\"\" dot_product = np.zeros(np.shape(a1)[0]) for i in np.arange(0, np.shape(a1)[1]): dot_product += a1[:, i] * a2[:, i] return dot_product get_ellipticity(e, phi) staticmethod Calculates the radial and tangential components of the ellipticity, given the size of the ellipticty vector and the angle between the semimajor or semiminor axis and the separation vector. Parameters: e ( ndarray ) \u2013 size of the ellipticity vector phi ( ndarray ) \u2013 angle between semimajor/semiminor axis and separation vector Returns: ndarray \u2013 e_+ and e_x Source code in src/measureia/measure_IA_base.py @staticmethod def get_ellipticity(e, phi): \"\"\"Calculates the radial and tangential components of the ellipticity, given the size of the ellipticty vector and the angle between the semimajor or semiminor axis and the separation vector. Parameters ---------- e : ndarray size of the ellipticity vector phi : ndarray angle between semimajor/semiminor axis and separation vector Returns ------- ndarray e_+ and e_x \"\"\" e_plus, e_cross = e * np.cos(2 * phi), e * np.sin(2 * phi) return e_plus, e_cross get_random_pairs(rp_max, rp_min, pi_max, pi_min, L3, corrtype, Num_position, Num_shape) staticmethod Returns analytical value of the number of pairs expected in an r_p, pi bin for a random uniform distribution. (Singh et al. 2023) Parameters: rp_max ( float ) \u2013 Upper bound of projected separation vector bin rp_min ( float ) \u2013 Lower bound of projected separation vector bin pi_max ( float ) \u2013 Upper bound of line of sight vector bin pi_min ( float ) \u2013 Lower bound of line of sight vector bin L3 ( float or int ) \u2013 Volume of the simulation box corrtype ( str ) \u2013 Correlation type, auto or cross. RR for auto is RR_cross/2. Num_position ( int ) \u2013 Number of objects in the position sample. Num_shape ( int ) \u2013 Number of objects in the shape sample. Returns: float \u2013 number of pairs in r_p, pi bin Source code in src/measureia/measure_IA_base.py @staticmethod def get_random_pairs(rp_max, rp_min, pi_max, pi_min, L3, corrtype, Num_position, Num_shape): \"\"\"Returns analytical value of the number of pairs expected in an r_p, pi bin for a random uniform distribution. (Singh et al. 2023) Parameters ---------- rp_max : float Upper bound of projected separation vector bin rp_min : float Lower bound of projected separation vector bin pi_max : float Upper bound of line of sight vector bin pi_min : float Lower bound of line of sight vector bin L3 : float or int Volume of the simulation box corrtype : str Correlation type, auto or cross. RR for auto is RR_cross/2. Num_position : int Number of objects in the position sample. Num_shape : int Number of objects in the shape sample. Returns ------- float number of pairs in r_p, pi bin \"\"\" if corrtype == \"auto\": RR = ( (Num_position - 1.0) * Num_shape / 2.0 * np.pi * (rp_max ** 2 - rp_min ** 2) * abs(pi_max - pi_min) / L3 ) # volume is cylindrical pi*dr^2 * height elif corrtype == \"cross\": RR = Num_position * Num_shape * np.pi * (rp_max ** 2 - rp_min ** 2) * abs(pi_max - pi_min) / L3 else: raise ValueError(\"Unknown input for corrtype, choose from auto or cross.\") return RR get_volume_spherical_cap(mur, r) staticmethod Calculate the volume of a spherical cap. Parameters: mur ( float ) \u2013 cos(theta), where theta is the polar angle between the apex and disk of the cap. r ( float ) \u2013 radius Returns: float \u2013 Volume of the spherical cap. Source code in src/measureia/measure_IA_base.py @staticmethod def get_volume_spherical_cap(mur, r): \"\"\"Calculate the volume of a spherical cap. Parameters ---------- mur : float cos(theta), where theta is the polar angle between the apex and disk of the cap. r : float radius Returns ------- float Volume of the spherical cap. \"\"\" return np.pi / 3.0 * r ** 3 * (2 + mur) * (1 - mur) ** 2 get_random_pairs_r_mur(r_max, r_min, mur_max, mur_min, L3, corrtype, Num_position, Num_shape) Returns analytical value of the number of pairs expected in an r, mu_r bin for a random uniform distribution. Parameters: r_max ( float ) \u2013 Upper bound of separation vector bin r_min ( float ) \u2013 Lower bound of separation vector bin mur_max ( float ) \u2013 Upper bound of mu_r bin mur_min ( float ) \u2013 Lower bound of mu_r bin L3 ( float ) \u2013 Volume of the simulation box corrtype ( str ) \u2013 Correlation type, auto or cross. RR for auto is RR_cross/2. Num_position ( int ) \u2013 Number of objects in the position sample. Num_shape ( int ) \u2013 Number of objects in the shape sample. Returns: float \u2013 number of pairs in r, mu_r bin Source code in src/measureia/measure_IA_base.py def get_random_pairs_r_mur(self, r_max, r_min, mur_max, mur_min, L3, corrtype, Num_position, Num_shape): \"\"\"Returns analytical value of the number of pairs expected in an r, mu_r bin for a random uniform distribution. Parameters ---------- r_max : float Upper bound of separation vector bin r_min : float Lower bound of separation vector bin mur_max : float Upper bound of mu_r bin mur_min : float Lower bound of mu_r bin L3 : float Volume of the simulation box corrtype : str Correlation type, auto or cross. RR for auto is RR_cross/2. Num_position : int Number of objects in the position sample. Num_shape : int Number of objects in the shape sample. Returns ------- float number of pairs in r, mu_r bin \"\"\" if corrtype == \"auto\": RR = ( (Num_position - 1.0) / 2.0 * Num_shape * ( self.get_volume_spherical_cap(mur_min, r_max) - self.get_volume_spherical_cap(mur_max, r_max) - (self.get_volume_spherical_cap(mur_min, r_min) - self.get_volume_spherical_cap(mur_max, r_min)) ) / L3 ) # volume is big cap - small cap for large - small radius elif corrtype == \"cross\": RR = ( (Num_position - 1.0) * Num_shape * ( self.get_volume_spherical_cap(mur_min, r_max) - self.get_volume_spherical_cap(mur_max, r_max) - (self.get_volume_spherical_cap(mur_min, r_min) - self.get_volume_spherical_cap(mur_max, r_min)) ) / L3 ) else: raise ValueError(\"Unknown input for corrtype, choose from auto or cross.\") return abs(RR) setdiff2D(a1, a2) staticmethod Compares each row of a1 and a2 and returns the elements that do not overlap Parameters: a1 ( nested list ) \u2013 List containing lists of elements to compare to a2 a2 ( nested list ) \u2013 List containing lists of elements to compare to a1 Returns: nested list \u2013 For each row, the not-overlapping elements between a1 and a2 Source code in src/measureia/measure_IA_base.py @staticmethod def setdiff2D(a1, a2): \"\"\"Compares each row of a1 and a2 and returns the elements that do not overlap Parameters ---------- a1 : nested list List containing lists of elements to compare to a2 a2 : nested list List containing lists of elements to compare to a1 Returns ------- nested list For each row, the not-overlapping elements between a1 and a2 \"\"\" assert len(a1) == len(a2), \"Lengths of lists where each row is to be compared, are not the same.\" diff = [] for i in np.arange(0, len(a1)): setdiff = np.setdiff1d(a1[i], a2[i]) diff.append(setdiff) del setdiff return diff setdiff_omit(a1, a2, incl_ind) staticmethod For rows in nested list a1, whose index is included in incl_ind, returns elements that do not overlap between the row in a1 and a2. Parameters: a1 ( nested list ) \u2013 List of lists or arrays where indicated rows need to be compared to a2 a2 ( list or array ) \u2013 Elements to be compared to the row in a1 [and not included in return values]. incl_ind ( list or array ) \u2013 Indices of rows in a1 to be compared to a2. Returns: nested list \u2013 For each included row in a1, the not-overlapping elements between a1 and a2 Source code in src/measureia/measure_IA_base.py @staticmethod def setdiff_omit(a1, a2, incl_ind): \"\"\"For rows in nested list a1, whose index is included in incl_ind, returns elements that do not overlap between the row in a1 and a2. Parameters ---------- a1 : nested list List of lists or arrays where indicated rows need to be compared to a2 a2 : list or array Elements to be compared to the row in a1 [and not included in return values]. incl_ind : list or array Indices of rows in a1 to be compared to a2. Returns ------- nested list For each included row in a1, the not-overlapping elements between a1 and a2 \"\"\" diff = [] for i in np.arange(0, len(a1)): if np.isin(i, incl_ind): setdiff = np.setdiff1d(a1[i], a2) diff.append(setdiff) del setdiff return diff assign_jackknife_patches(data, randoms_data, num_jk) Assigns jackknife patches to data and randoms given a number of patches. Based on https://github.com/esheldon/kmeans_radec Parameters: data ( dict ) \u2013 Dictionary containing position and shape sample data. Keywords: \"RA\", \"DEC\", \"RA_shape_sample\", \"DEC_shape_sample\" randoms_data ( dict ) \u2013 Dictionary containing position and shape sample data of randoms. Keywords: \"RA\", \"DEC\", \"RA_shape_sample\", \"DEC_shape_sample\" num_jk ( int ) \u2013 Number of jackknife patches Returns: dict \u2013 Dictionary with patch numbers for each sample. Keywords: 'position', 'shape', 'randoms_position', 'randoms_shape' Source code in src/measureia/measure_IA_base.py def assign_jackknife_patches(self, data, randoms_data, num_jk): \"\"\"Assigns jackknife patches to data and randoms given a number of patches. Based on https://github.com/esheldon/kmeans_radec Parameters ---------- data : dict Dictionary containing position and shape sample data. Keywords: \"RA\", \"DEC\", \"RA_shape_sample\", \"DEC_shape_sample\" randoms_data : dict Dictionary containing position and shape sample data of randoms. Keywords: \"RA\", \"DEC\", \"RA_shape_sample\", \"DEC_shape_sample\" num_jk : int Number of jackknife patches Returns ------- dict Dictionary with patch numbers for each sample. Keywords: 'position', 'shape', 'randoms_position', 'randoms_shape' \"\"\" jk_patches = {} # Read the randoms file from which the jackknife regions will be created RA = randoms_data['RA'] DEC = randoms_data['DEC'] # Define a number of jaccknife regions and find their centres using kmans X = np.column_stack((RA, DEC)) km = kmeans_sample(X, num_jk, maxiter=100, tol=1.0e-5) jk_labels = km.labels jk_patches['randoms_position'] = jk_labels RA = randoms_data['RA_shape_sample'] DEC = randoms_data['DEC_shape_sample'] X2 = np.column_stack((RA, DEC)) jk_labels = km.find_nearest(X2) jk_patches['randoms_shape'] = jk_labels RA = data['RA'] DEC = data['DEC'] X2 = np.column_stack((RA, DEC)) jk_labels = km.find_nearest(X2) jk_patches['position'] = jk_labels RA = data['RA_shape_sample'] DEC = data['DEC_shape_sample'] X2 = np.column_stack((RA, DEC)) jk_labels = km.find_nearest(X2) jk_patches['shape'] = jk_labels return jk_patches","title":"MeasureIABase"},{"location":"api/MeasureIABase/#measureiabase","text":"","title":"MeasureIABase"},{"location":"api/MeasureIABase/#measureia.MeasureIABase","text":"Bases: SimInfo Base class for MeasureIA package that includes some general methods used throughout the package. Attributes: Num_position ( int ) \u2013 Number of objects in the position sample. This value is updated in jackknife realisations. Num_shape ( int ) \u2013 Number of objects in the shape sample. This value is updated in jackknife realisations. r_min ( float ) \u2013 Minimum bound of (projected) separation length; bin edge. Default is 0.1. r_max ( float ) \u2013 Maximum bound of (projected) separation length; bin edge. Default is 20. r_bins ( ndarray ) \u2013 Bin edges of the (projected) separation length (r_p or r). pi_bins ( ndarray ) \u2013 Bin edges of the line of sight (pi). mu_r_bins ( ndarray ) \u2013 Bin edges of the mu_r. Methods: Name Description calculate_dot_product_arrays Calculates dot product of elements of two arrays get_ellipticity Given e and phi, e_+ and e_x components of ellipticity are returned. get_random_pairs Analytical RR for a (rp,pi) bin. get_volume_spherical_cap Volume of an (r,mu_r) bin. get_random_pairs_r_mur Analytical RR for a (r,mu_r) bin. setdiff2D Compares each row of a1 and a2 and returns the elements that do not overlap. setdiff_omit For rows in nested list a1, whose index is included in incl_ind, returns elements that do not overlap between the row in a1 and a2. _measure_w_g_i Measure wgg or wg+ from xi grid provided by MeasureWBox or MeasureWLightcone class methods. _measure_multipoles Measure multipoles (gg or g+) from xi grid provided by MeasureMultipolesBox or MeasureMultipolesLightcone class methods. _obs_estimator Combines elements (DD, RR, etc) of xi estimators into xi_gg or xi_g+ for MeasureIALightcone. assign_jackknife_patches Given positions of multiple samples, defines jackknife patches and returns index of every object in the sample. Notes Inherits attributes from 'SimInfo', where 'boxsize', 'L_0p5' and 'snap_group' are used in this class. Source code in src/measureia/measure_IA_base.py class MeasureIABase(SimInfo): \"\"\"Base class for MeasureIA package that includes some general methods used throughout the package. Attributes ---------- Num_position : int Number of objects in the position sample. This value is updated in jackknife realisations. Num_shape : int Number of objects in the shape sample. This value is updated in jackknife realisations. r_min : float Minimum bound of (projected) separation length; bin edge. Default is 0.1. r_max : float Maximum bound of (projected) separation length; bin edge. Default is 20. r_bins : ndarray Bin edges of the (projected) separation length (r_p or r). pi_bins : ndarray Bin edges of the line of sight (pi). mu_r_bins : ndarray Bin edges of the mu_r. Methods ------- calculate_dot_product_arrays() Calculates dot product of elements of two arrays get_ellipticity() Given e and phi, e_+ and e_x components of ellipticity are returned. get_random_pairs() Analytical RR for a (rp,pi) bin. get_volume_spherical_cap() Volume of an (r,mu_r) bin. get_random_pairs_r_mur() Analytical RR for a (r,mu_r) bin. setdiff2D() Compares each row of a1 and a2 and returns the elements that do not overlap. setdiff_omit() For rows in nested list a1, whose index is included in incl_ind, returns elements that do not overlap between the row in a1 and a2. _measure_w_g_i() Measure wgg or wg+ from xi grid provided by MeasureWBox or MeasureWLightcone class methods. _measure_multipoles() Measure multipoles (gg or g+) from xi grid provided by MeasureMultipolesBox or MeasureMultipolesLightcone class methods. _obs_estimator() Combines elements (DD, RR, etc) of xi estimators into xi_gg or xi_g+ for MeasureIALightcone. assign_jackknife_patches() Given positions of multiple samples, defines jackknife patches and returns index of every object in the sample. Notes ----- Inherits attributes from 'SimInfo', where 'boxsize', 'L_0p5' and 'snap_group' are used in this class. \"\"\" def __init__( self, data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True, ): \"\"\" The __init__ method of the MeasureIABase class. Parameters ---------- data : dict or NoneType Dictionary with data needed for calculations. For cartesian coordinates, the keywords are: 'Position' and 'Position_shape_sample': (N_p,3), (N_s,3) ndarrays with the x, y, z coordinates of the N_p, N_s objects in the position and shape samples, respectively. 'Axis_Direction': (N_s,2) ndarray with the two elements of the unit vectors describing the axis direction of the projected axis of the object shape. 'LOS': index referring back to the column number in the 'Position' samples that contains the line-of-sight coordinate. (e.g. if the shapes are projected over the z-axis, LOS=2) 'q': (N_s) array containing the axis ratio q=b/a for each object in the shape sample. For lightcone coordinates, the keywords are: 'Redshift' and 'Redshift_shape_sample': (N_p) and (N_s) ndarray with redshifts of position and shape samples. 'RA' and 'RA_shape_sample': (N_p) and (N_s) ndarray with RA coordinate of position and shape samples. 'DEC' and 'DEC_shape_sample': (N_p) and (N_s) ndarray with DEC coordinate of position and shape samples. 'e1' and 'e2': (N_s) arrays with the two ellipticity components e1 and e2 of the shape sample objects. output_file_name : str Name and filepath of the file where the output should be stored. Needs to be hdf5-type. simulation : str or NoneType, optional Indicator of simulation, obtaining correct boxsize in cMpc/h automatically. Choose from [TNG100, TNG100_2, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1, FLAMINGO_L2p8]. Default is None, in which case boxsize needs to be added manually; or in the case of observational data, the pi_max. snapshot : int or str or NoneType, optional Number of the snapshot, which, if given, will ensure that the output file to contains a group 'Snapshot_[snapshot]'. If None, the group is omitted from the output file structure. Default is None. separation_limits : iterable of 2 entries, optional Bounds of the (projected) separation vector length bins in cMpc/h (so, r or r_p). Default is [0.1,20]. num_bins_r : int, optional Number of bins for (projected) separation vector. Default is 8. num_bins_pi : int, optional Number of bins for line of sight (LOS) vector, pi or mu_r when multipoles are measured. Default is 20. pi_max : int or float, optional Bound for line of sight bins. Bounds will be [-pi_max, pi_max]. Default is None, in which case half the boxsize will be used. boxsize : int or float or NoneType, optional If simulation is not included in SimInfo, a manual boxsize can be added here. Make sure simulation=None and the boxsize units are equal to those in the data dictionary. Default is None. periodicity : bool, optional If True, the periodic boundary conditions of the simulation box are taken into account. If False, they are ignored. Note that because this code used analytical randoms for the simulations, the correlations will not be correct in this case and only DD and S+D terms should be studied. Non-periodic randoms can be measured by providing random data to the code and considering the DD term that is measured. Correlations and covariance matrix will need to be reconstructed from parts. [Please add a request for teh integration of this method of this if you would like to use this option often.] Default is True. \"\"\" SimInfo.__init__(self, simulation, snapshot, boxsize) self.data = data self.output_file_name = output_file_name self.periodicity = periodicity if periodicity: periodic = \"periodic \" else: periodic = \"\" try: self.Num_position = len(data[\"Position\"]) # number of halos in position sample self.Num_shape = len(data[\"Position_shape_sample\"]) # number of halos in shape sample except: try: self.Num_position = len(data[\"RA\"]) self.Num_shape = len(data[\"RA_shape_sample\"]) except: self.Num_position = 0 self.Num_shape = 0 print(\"Warning: no Postion or Position_shape_sample given.\") if self.Num_position > 0: try: weight = self.data[\"weight\"] except: self.data[\"weight\"] = np.ones(self.Num_position) try: weight = self.data[\"weight_shape_sample\"] except: self.data[\"weight_shape_sample\"] = np.ones(self.Num_shape) self.r_min = separation_limits[0] # cMpc/h self.r_max = separation_limits[1] # cMpc/h self.num_bins_r = num_bins_r self.num_bins_pi = num_bins_pi self.r_bins = np.logspace(np.log10(self.r_min), np.log10(self.r_max), self.num_bins_r + 1) if pi_max == None: if self.L_0p5 is None: raise ValueError( \"Both pi_max and boxsize are None. Provide input on one of them to determine the integration limit pi_max.\") else: pi_max = self.L_0p5 self.pi_bins = np.linspace(-pi_max, pi_max, self.num_bins_pi + 1) self.mu_r_bins = np.linspace(-1, 1, self.num_bins_pi + 1) if simulation == False: print(f\"MeasureIA object initialised with:\\n \\ observational data.\\n \\ There are {self.Num_shape} galaxies in the shape sample and {self.Num_position} galaxies in the position sample.\\n\\ The separation bin edges are given by {self.r_bins} Mpc.\\n \\ There are {num_bins_r} r or r_p bins and {num_bins_pi} pi bins.\\n \\ The maximum pi used for binning is {pi_max}.\\n \\ The data will be written to {self.output_file_name}\") else: print(f\"MeasureIA object initialised with:\\n \\ simulation {simulation} that has a {periodic}boxsize of {self.boxsize} cMpc/h.\\n \\ There are {self.Num_shape} galaxies in the shape sample and {self.Num_position} galaxies in the position sample.\\n\\ The separation bin edges are given by {self.r_bins} cMpc/h.\\n \\ There are {num_bins_r} r or r_p bins and {num_bins_pi} pi bins.\\n \\ The maximum pi used for binning is {pi_max}.\\n \\ The data will be written to {self.output_file_name}\") return @staticmethod def calculate_dot_product_arrays(a1, a2): \"\"\"Calculates the dot product over 2 2D arrays across axis 1 so that dot_product[i] = np.dot(a1[i],a2[i]) Parameters ---------- a1 : ndarray First array a2 : ndarray Second array Returns ------- ndarray Dot product of columns of arrays \"\"\" dot_product = np.zeros(np.shape(a1)[0]) for i in np.arange(0, np.shape(a1)[1]): dot_product += a1[:, i] * a2[:, i] return dot_product @staticmethod def get_ellipticity(e, phi): \"\"\"Calculates the radial and tangential components of the ellipticity, given the size of the ellipticty vector and the angle between the semimajor or semiminor axis and the separation vector. Parameters ---------- e : ndarray size of the ellipticity vector phi : ndarray angle between semimajor/semiminor axis and separation vector Returns ------- ndarray e_+ and e_x \"\"\" e_plus, e_cross = e * np.cos(2 * phi), e * np.sin(2 * phi) return e_plus, e_cross @staticmethod def get_random_pairs(rp_max, rp_min, pi_max, pi_min, L3, corrtype, Num_position, Num_shape): \"\"\"Returns analytical value of the number of pairs expected in an r_p, pi bin for a random uniform distribution. (Singh et al. 2023) Parameters ---------- rp_max : float Upper bound of projected separation vector bin rp_min : float Lower bound of projected separation vector bin pi_max : float Upper bound of line of sight vector bin pi_min : float Lower bound of line of sight vector bin L3 : float or int Volume of the simulation box corrtype : str Correlation type, auto or cross. RR for auto is RR_cross/2. Num_position : int Number of objects in the position sample. Num_shape : int Number of objects in the shape sample. Returns ------- float number of pairs in r_p, pi bin \"\"\" if corrtype == \"auto\": RR = ( (Num_position - 1.0) * Num_shape / 2.0 * np.pi * (rp_max ** 2 - rp_min ** 2) * abs(pi_max - pi_min) / L3 ) # volume is cylindrical pi*dr^2 * height elif corrtype == \"cross\": RR = Num_position * Num_shape * np.pi * (rp_max ** 2 - rp_min ** 2) * abs(pi_max - pi_min) / L3 else: raise ValueError(\"Unknown input for corrtype, choose from auto or cross.\") return RR @staticmethod def get_volume_spherical_cap(mur, r): \"\"\"Calculate the volume of a spherical cap. Parameters ---------- mur : float cos(theta), where theta is the polar angle between the apex and disk of the cap. r : float radius Returns ------- float Volume of the spherical cap. \"\"\" return np.pi / 3.0 * r ** 3 * (2 + mur) * (1 - mur) ** 2 def get_random_pairs_r_mur(self, r_max, r_min, mur_max, mur_min, L3, corrtype, Num_position, Num_shape): \"\"\"Returns analytical value of the number of pairs expected in an r, mu_r bin for a random uniform distribution. Parameters ---------- r_max : float Upper bound of separation vector bin r_min : float Lower bound of separation vector bin mur_max : float Upper bound of mu_r bin mur_min : float Lower bound of mu_r bin L3 : float Volume of the simulation box corrtype : str Correlation type, auto or cross. RR for auto is RR_cross/2. Num_position : int Number of objects in the position sample. Num_shape : int Number of objects in the shape sample. Returns ------- float number of pairs in r, mu_r bin \"\"\" if corrtype == \"auto\": RR = ( (Num_position - 1.0) / 2.0 * Num_shape * ( self.get_volume_spherical_cap(mur_min, r_max) - self.get_volume_spherical_cap(mur_max, r_max) - (self.get_volume_spherical_cap(mur_min, r_min) - self.get_volume_spherical_cap(mur_max, r_min)) ) / L3 ) # volume is big cap - small cap for large - small radius elif corrtype == \"cross\": RR = ( (Num_position - 1.0) * Num_shape * ( self.get_volume_spherical_cap(mur_min, r_max) - self.get_volume_spherical_cap(mur_max, r_max) - (self.get_volume_spherical_cap(mur_min, r_min) - self.get_volume_spherical_cap(mur_max, r_min)) ) / L3 ) else: raise ValueError(\"Unknown input for corrtype, choose from auto or cross.\") return abs(RR) @staticmethod def setdiff2D(a1, a2): \"\"\"Compares each row of a1 and a2 and returns the elements that do not overlap Parameters ---------- a1 : nested list List containing lists of elements to compare to a2 a2 : nested list List containing lists of elements to compare to a1 Returns ------- nested list For each row, the not-overlapping elements between a1 and a2 \"\"\" assert len(a1) == len(a2), \"Lengths of lists where each row is to be compared, are not the same.\" diff = [] for i in np.arange(0, len(a1)): setdiff = np.setdiff1d(a1[i], a2[i]) diff.append(setdiff) del setdiff return diff @staticmethod def setdiff_omit(a1, a2, incl_ind): \"\"\"For rows in nested list a1, whose index is included in incl_ind, returns elements that do not overlap between the row in a1 and a2. Parameters ---------- a1 : nested list List of lists or arrays where indicated rows need to be compared to a2 a2 : list or array Elements to be compared to the row in a1 [and not included in return values]. incl_ind : list or array Indices of rows in a1 to be compared to a2. Returns ------- nested list For each included row in a1, the not-overlapping elements between a1 and a2 \"\"\" diff = [] for i in np.arange(0, len(a1)): if np.isin(i, incl_ind): setdiff = np.setdiff1d(a1[i], a2) diff.append(setdiff) del setdiff return diff def _get_jackknife_region_indices(self, masks, L_subboxes): \"\"\" Split the box in L_subboxes^3 subboxes and return indices of which subbox objects are in for position and shape sample. Parameters ---------- masks: dict or NoneType Input in methods in MeasureIABox that masks the input data dictionary. L_subboxes: int Number of subboxes on one side of the box. L_subboxes^3 is the total number of jackknife realisations. Returns ------- ndarrays indices of jackknife region of position sample and indices of jackknife region of shape sample \"\"\" if masks == None: positions = self.data[\"Position\"] positions_shape_sample = self.data[\"Position_shape_sample\"] else: positions = self.data[\"Position\"][masks[\"Position\"]] positions_shape_sample = self.data[\"Position_shape_sample\"][masks[\"Position_shape_sample\"]] L_sub = self.L_0p5 * 2.0 / L_subboxes jackknife_region_indices_pos = np.zeros(len(positions)) jackknife_region_indices_shape = np.zeros(len(positions_shape_sample)) num_box = 0 for i in np.arange(0, L_subboxes): for j in np.arange(0, L_subboxes): for k in np.arange(0, L_subboxes): x_bounds = [i * L_sub, (i + 1) * L_sub] y_bounds = [j * L_sub, (j + 1) * L_sub] z_bounds = [k * L_sub, (k + 1) * L_sub] x_mask = (positions[:, 0] > x_bounds[0]) * (positions[:, 0] < x_bounds[1]) y_mask = (positions[:, 1] > y_bounds[0]) * (positions[:, 1] < y_bounds[1]) z_mask = (positions[:, 2] > z_bounds[0]) * (positions[:, 2] < z_bounds[1]) x_mask_shape = (positions_shape_sample[:, 0] > x_bounds[0]) * ( positions_shape_sample[:, 0] < x_bounds[1]) y_mask_shape = (positions_shape_sample[:, 1] > y_bounds[0]) * ( positions_shape_sample[:, 1] < y_bounds[1]) z_mask_shape = (positions_shape_sample[:, 2] > z_bounds[0]) * ( positions_shape_sample[:, 2] < z_bounds[1]) mask_position = x_mask * y_mask * z_mask # mask that is True for all positions in the subbox mask_shape = x_mask_shape * y_mask_shape * z_mask_shape # mask that is True for all positions not in the subbox jackknife_region_indices_pos[mask_position] = num_box jackknife_region_indices_shape[mask_shape] = num_box num_box += 1 return np.array(jackknife_region_indices_pos, dtype=int), np.array(jackknife_region_indices_shape, dtype=int) def _combine_jackknife_information(self, dataset_name, jk_group_name, corr_group, num_box, return_output=False): \"\"\" Combine jackknife realisations into a covariance matrix. Parameters ---------- dataset_name: str Name of the dataset in the output file. jk_group_name: str Name of the subgroup in the output file where the jackknife realisations are saved. corr_group: list of str Name of the subgroups in the output file denoting the correlation (e.g. w_g_plus, multipoles_gg etc). num_box: int Number of jackknife realisations. return_output: bool, optional When True, returns output, otherwise saves to output file. Returns ------- list of ndarrays list of covariances for each entry in corr_group and list of standard deviations for each entry in corr_group \"\"\" covs, stds = [], [] for d in np.arange(0, len(corr_group)): data_file = h5py.File(self.output_file_name, \"a\") group_multipoles = data_file[f\"{self.snap_group}/{corr_group[d]}/{jk_group_name}/\"] # calculating mean of the datavectors mean_multipoles = np.zeros(self.num_bins_r) for b in np.arange(0, num_box): mean_multipoles += group_multipoles[dataset_name + \"_\" + str(b)][:] mean_multipoles /= num_box # calculation the covariance matrix (multipoles) and the standard deviation (sqrt of diag of cov) cov = np.zeros((self.num_bins_r, self.num_bins_r)) std = np.zeros(self.num_bins_r) for b in np.arange(0, num_box): std += (group_multipoles[dataset_name + \"_\" + str(b)][:] - mean_multipoles) ** 2 for i in np.arange(self.num_bins_r): cov[:, i] += (group_multipoles[dataset_name + \"_\" + str(b)][:] - mean_multipoles) * ( group_multipoles[dataset_name + \"_\" + str(b)][i] - mean_multipoles[i] ) std *= (num_box - 1) / num_box # see Singh 2023 std = np.sqrt(std) # size of errorbars cov *= (num_box - 1) / num_box # cov not sqrt so to get std, sqrt of diag would need to be taken data_file.close() if return_output: covs.append(cov) stds.append(std) else: output_file = h5py.File(self.output_file_name, \"a\") group_multipoles = create_group_hdf5(output_file, f\"{self.snap_group}/\" + corr_group[d]) write_dataset_hdf5(group_multipoles, dataset_name + \"_mean_\" + str(num_box), data=mean_multipoles) write_dataset_hdf5(group_multipoles, dataset_name + \"_jackknife_\" + str(num_box), data=std) write_dataset_hdf5(group_multipoles, dataset_name + \"_jackknife_cov_\" + str(num_box), data=cov) output_file.close() if return_output: return covs, stds else: return def _measure_w_g_i(self, dataset_name, corr_type=\"both\", return_output=False, jk_group_name=\"\"): \"\"\"Measures w_gg or w_g+ for a given xi_gi dataset that has been calculated with the _measure_xi_rp_pi_sims methods. Integrates over pi bins via sum * dpi. Stores rp, and w_gg or w_g+. Parameters ---------- dataset_name : str Name of xi_gg or xi_g+ dataset and name given to w_gg or w_g+ dataset when stored. return_output : bool, optional Output is returned if True, saved to file if False. Default value = False corr_type : str, optional Type of correlation function. Choose from [g+,gg,both]. Default value = \"both\" jk_group_name : str, optional Name of subgroup in hdf5 file where jackknife realisations are stored. Default value = \"\" Returns ------- ndarray [rp, wgg] or [rp, wg+] if return_output is True \"\"\" if corr_type == \"both\": xi_data = [\"xi_g_plus\", \"xi_gg\"] wg_data = [\"w_g_plus\", \"w_gg\"] elif corr_type == \"g+\": xi_data = [\"xi_g_plus\"] wg_data = [\"w_g_plus\"] elif corr_type == \"gg\": xi_data = [\"xi_gg\"] wg_data = [\"w_gg\"] else: raise KeyError(\"Unknown value for corr_type. Choose from [g+, gg, both]\") for i in np.arange(0, len(xi_data)): correlation_data_file = h5py.File(self.output_file_name, \"a\") group = correlation_data_file[f\"{self.snap_group}/w/{xi_data[i]}/{jk_group_name}\"] correlation_data = group[dataset_name][:] pi = group[dataset_name + \"_pi\"] rp = group[dataset_name + \"_rp\"] dpi = (self.pi_bins[1:] - self.pi_bins[:-1]) pi_bins = self.pi_bins[:-1] + abs(dpi) / 2.0 # middle of bins # variance = group[dataset_name + \"_sigmasq\"][:] if sum(np.isin(pi, pi_bins)) == len(pi): dpi = np.array([dpi] * len(correlation_data[:, 0])) correlation_data = correlation_data * abs(dpi) # sigsq_el = variance * dpi ** 2 else: raise ValueError(\"Update pi bins in initialisation of object to match xi_g_plus dataset.\") w_g_i = np.sum(correlation_data, axis=1) # sum over pi values # sigsq = np.sum(sigsq_el, axis=1) if return_output: output_data = np.array([rp, w_g_i]).transpose() correlation_data_file.close() return output_data else: group_out = create_group_hdf5(correlation_data_file, f\"{self.snap_group}/{wg_data[i]}/{jk_group_name}\") write_dataset_hdf5(group_out, dataset_name + \"_rp\", data=rp) write_dataset_hdf5(group_out, dataset_name, data=w_g_i) # write_dataset_hdf5(group_out, dataset_name + \"_sigma\", data=np.sqrt(sigsq)) correlation_data_file.close() return def _measure_multipoles(self, dataset_name, corr_type=\"both\", return_output=False, jk_group_name=\"\"): \"\"\"Measures multipoles for a given xi_g+ or xi_gg measured by _measure_xi_r_pi_sims methods. The data assumes xi_g+ and xi_gg to be measured in bins of r and mu_r. Parameters ---------- dataset_name : str Name of xi_gg or xi_g+ dataset and name given to multipoles dataset when stored. corr_type : str, optional Type of correlation function. Choose from [g+,gg,both]. Default value = \"both\" return_output : bool, optional Output is returned if True, saved to file if False. Default value = False. jk_group_name : str, optional Name of subgroup in hdf5 file where jackknife realisations are stored. Default value = \"\" Returns ------- ndarray [r, multipoles_gg] or [r, multipoles_g+] if return_output is True \"\"\" correlation_data_file = h5py.File(self.output_file_name, \"a\") if corr_type == \"g+\": # todo: expand to include ++ option group = correlation_data_file[f\"{self.snap_group}/multipoles/xi_g_plus/{jk_group_name}\"] correlation_data_list = [group[dataset_name][:]] # xi_g+ in grid of r,mur r_list = [group[dataset_name + \"_r\"][:]] mu_r_list = [group[dataset_name + \"_mu_r\"][:]] sab_list = [2] l_list = sab_list corr_type_list = [\"g_plus\"] elif corr_type == \"gg\": group = correlation_data_file[f\"{self.snap_group}/multipoles/xi_gg/{jk_group_name}\"] correlation_data_list = [group[dataset_name][:]] # xi_g+ in grid of rp,pi r_list = [group[dataset_name + \"_r\"][:]] mu_r_list = [group[dataset_name + \"_mu_r\"][:]] sab_list = [0] l_list = sab_list corr_type_list = [\"gg\"] elif corr_type == \"both\": group = correlation_data_file[f\"{self.snap_group}/multipoles/xi_g_plus/{jk_group_name}\"] correlation_data_list = [group[dataset_name][:]] # xi_g+ in grid of rp,pi r_list = [group[dataset_name + \"_r\"][:]] mu_r_list = [group[dataset_name + \"_mu_r\"][:]] group = correlation_data_file[f\"{self.snap_group}/multipoles/xi_gg/{jk_group_name}\"] correlation_data_list.append(group[dataset_name][:]) # xi_g+ in grid of rp,pi r_list.append(group[dataset_name + \"_r\"][:]) mu_r_list.append(group[dataset_name + \"_mu_r\"][:]) sab_list = [2, 0] l_list = sab_list corr_type_list = [\"g_plus\", \"gg\"] else: raise KeyError(\"Unknown value for corr_type. Choose from [g+, gg, both]\") for i in np.arange(0, len(sab_list)): corr_type_i = corr_type_list[i] correlation_data = correlation_data_list[i] r = r_list[i] mu_r = mu_r_list[i] sab = sab_list[i] l = l_list[i] L = np.zeros((len(r), len(mu_r))) mu_r = np.array(list(mu_r) * len(r)).reshape((len(r), len(mu_r))) # make pi into grid for mu r = np.array(list(r) * len(mu_r)).reshape((len(r), len(mu_r))) r = r.transpose() for n in np.arange(0, len(mu_r[:, 0])): for m in np.arange(0, len(mu_r[0])): L_m, dL = lpmn(l, sab, mu_r[n, m]) # make associated Legendre polynomial grid L[n, m] = L_m[-1, -1] # grid ranges from 0 to sab and 0 to l, so last element is what we seek dmur = (self.mu_r_bins[1:] - self.mu_r_bins[:-1]) dmu_r_array = np.array(list(dmur) * len(r)).reshape((len(r), len(dmur))) multipoles = ( (2 * l + 1) / 2.0 * math.factorial(l - sab) / math.factorial(l + sab) * L * correlation_data * dmu_r_array ) multipoles = np.sum(multipoles, axis=1) dsep = (self.r_bins[1:] - self.r_bins[:-1]) / 2.0 separation = self.r_bins[:-1] + abs(dsep) # middle of bins if return_output: correlation_data_file.close() np.array([separation, multipoles]).transpose() else: group_out = create_group_hdf5( correlation_data_file, f\"{self.snap_group}/multipoles_{corr_type_i}/{jk_group_name}\" ) write_dataset_hdf5(group_out, dataset_name + \"_r\", data=separation) write_dataset_hdf5(group_out, dataset_name, data=multipoles) correlation_data_file.close() return def _obs_estimator(self, corr_type, IA_estimator, dataset_name, dataset_name_randoms, num_samples, jk_group_name=\"\", jk_group_name_randoms=\"\"): \"\"\"Reads various components of xi and combines into correct estimator for cluster or galaxy lightcone alignment correlations. It then writes the xi_gg or xi_g+ in the correct place in the output file. Parameters ---------- corr_type : list of 2 str elements First element: ['gg', 'g+', 'both'], second: 'w' or 'multipoles' IA_estimator : str Chooser from 'clusters' or 'galaxies' for different estimator definition. dataset_name : str Name of the dataset dataset_name_randoms : str Name of the dataset for data with randoms as positions num_samples : dict Dictionary of samples sizes for position, shape and random samples. Keywords: D, S, R_D, R_S jk_group_name : str Name of subgroup in hdf5 file where jackknife realisations are stored. Default value = \"\" Returns ------- \"\"\" output_file = h5py.File(self.output_file_name, \"a\") if corr_type[0] == \"g+\" or corr_type[0] == \"both\": group_gp = output_file[ f\"{self.snap_group}/{corr_type[1]}/xi_g_plus/{jk_group_name}\"] group_gp_r = output_file[ f\"{self.snap_group}/{corr_type[1]}/xi_g_plus/{jk_group_name_randoms}\"] SpD = group_gp[f\"{dataset_name}_SplusD\"][:] SpR = group_gp_r[f\"{dataset_name_randoms}_SplusD\"][:] group_gg = output_file[f\"{self.snap_group}/{corr_type[1]}/xi_gg/{jk_group_name}\"] group_gg_r = output_file[f\"{self.snap_group}/{corr_type[1]}/xi_gg/{jk_group_name_randoms}\"] DD = group_gg[f\"{dataset_name}_DD\"][:] if IA_estimator == \"clusters\": if corr_type[0] == \"gg\": SR = group_gg[f\"{dataset_name}_SR\"][:] else: SR = group_gg_r[f\"{dataset_name_randoms}_DD\"][:] SR *= num_samples[\"D\"] / num_samples[\"R_D\"] if corr_type[0] == \"g+\" or corr_type[0] == \"both\": SpR *= num_samples[\"D\"] / num_samples[\"R_D\"] correlation_gp = SpD / DD - SpR / SR write_dataset_hdf5(group_gp, dataset_name, correlation_gp) if corr_type[0] == \"gg\" or corr_type[0] == \"both\": RD = group_gg[f\"{dataset_name}_RD\"][:] RR = group_gg[f\"{dataset_name}_RR\"][:] RD *= num_samples[\"S\"] / num_samples[\"R_S\"] RR *= (num_samples[\"S\"] / num_samples[\"R_S\"]) * (num_samples[\"D\"] / num_samples[\"R_D\"]) correlation_gg = (DD - RD - SR) / RR - 1 write_dataset_hdf5(group_gg, dataset_name, correlation_gg) elif IA_estimator == \"galaxies\": RR = group_gg[f\"{dataset_name}_RR\"][:] RR *= (num_samples[\"S\"] / num_samples[\"R_S\"]) * (num_samples[\"D\"] / num_samples[\"R_D\"]) if corr_type[0] == \"g+\" or corr_type[0] == \"both\": SpR *= num_samples[\"D\"] / num_samples[\"R_D\"] correlation_gp = (SpD - SpR) / RR write_dataset_hdf5(group_gp, dataset_name, correlation_gp) if corr_type[0] == \"gg\" or corr_type[0] == \"both\": RD = group_gg[f\"{dataset_name}_RD\"][:] if corr_type[0] == \"gg\": SR = group_gg[f\"{dataset_name}_SR\"][:] else: SR = group_gg_r[f\"{dataset_name_randoms}_DD\"][:] RD *= num_samples[\"S\"] / num_samples[\"R_S\"] SR *= num_samples[\"D\"] / num_samples[\"R_D\"] correlation_gg = (DD - RD - SR) / RR - 1 write_dataset_hdf5(group_gg, dataset_name, correlation_gg) else: raise ValueError(\"Unknown input for IA_estimator, choose from [clusters, galaxies].\") output_file.close() return def assign_jackknife_patches(self, data, randoms_data, num_jk): \"\"\"Assigns jackknife patches to data and randoms given a number of patches. Based on https://github.com/esheldon/kmeans_radec Parameters ---------- data : dict Dictionary containing position and shape sample data. Keywords: \"RA\", \"DEC\", \"RA_shape_sample\", \"DEC_shape_sample\" randoms_data : dict Dictionary containing position and shape sample data of randoms. Keywords: \"RA\", \"DEC\", \"RA_shape_sample\", \"DEC_shape_sample\" num_jk : int Number of jackknife patches Returns ------- dict Dictionary with patch numbers for each sample. Keywords: 'position', 'shape', 'randoms_position', 'randoms_shape' \"\"\" jk_patches = {} # Read the randoms file from which the jackknife regions will be created RA = randoms_data['RA'] DEC = randoms_data['DEC'] # Define a number of jaccknife regions and find their centres using kmans X = np.column_stack((RA, DEC)) km = kmeans_sample(X, num_jk, maxiter=100, tol=1.0e-5) jk_labels = km.labels jk_patches['randoms_position'] = jk_labels RA = randoms_data['RA_shape_sample'] DEC = randoms_data['DEC_shape_sample'] X2 = np.column_stack((RA, DEC)) jk_labels = km.find_nearest(X2) jk_patches['randoms_shape'] = jk_labels RA = data['RA'] DEC = data['DEC'] X2 = np.column_stack((RA, DEC)) jk_labels = km.find_nearest(X2) jk_patches['position'] = jk_labels RA = data['RA_shape_sample'] DEC = data['DEC_shape_sample'] X2 = np.column_stack((RA, DEC)) jk_labels = km.find_nearest(X2) jk_patches['shape'] = jk_labels return jk_patches","title":"MeasureIABase"},{"location":"api/MeasureIABase/#measureia.MeasureIABase.__init__","text":"The init method of the MeasureIABase class. Parameters: data ( dict or NoneType ) \u2013 Dictionary with data needed for calculations. For cartesian coordinates, the keywords are: 'Position' and 'Position_shape_sample': (N_p,3), (N_s,3) ndarrays with the x, y, z coordinates of the N_p, N_s objects in the position and shape samples, respectively. 'Axis_Direction': (N_s,2) ndarray with the two elements of the unit vectors describing the axis direction of the projected axis of the object shape. 'LOS': index referring back to the column number in the 'Position' samples that contains the line-of-sight coordinate. (e.g. if the shapes are projected over the z-axis, LOS=2) 'q': (N_s) array containing the axis ratio q=b/a for each object in the shape sample. For lightcone coordinates, the keywords are: 'Redshift' and 'Redshift_shape_sample': (N_p) and (N_s) ndarray with redshifts of position and shape samples. 'RA' and 'RA_shape_sample': (N_p) and (N_s) ndarray with RA coordinate of position and shape samples. 'DEC' and 'DEC_shape_sample': (N_p) and (N_s) ndarray with DEC coordinate of position and shape samples. 'e1' and 'e2': (N_s) arrays with the two ellipticity components e1 and e2 of the shape sample objects. output_file_name ( str ) \u2013 Name and filepath of the file where the output should be stored. Needs to be hdf5-type. simulation ( str or NoneType , default: None ) \u2013 Indicator of simulation, obtaining correct boxsize in cMpc/h automatically. Choose from [TNG100, TNG100_2, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1, FLAMINGO_L2p8]. Default is None, in which case boxsize needs to be added manually; or in the case of observational data, the pi_max. snapshot ( int or str or NoneType , default: None ) \u2013 Number of the snapshot, which, if given, will ensure that the output file to contains a group 'Snapshot_[snapshot]'. If None, the group is omitted from the output file structure. Default is None. separation_limits ( iterable of 2 entries , default: [0.1, 20.0] ) \u2013 Bounds of the (projected) separation vector length bins in cMpc/h (so, r or r_p). Default is [0.1,20]. num_bins_r ( int , default: 8 ) \u2013 Number of bins for (projected) separation vector. Default is 8. num_bins_pi ( int , default: 20 ) \u2013 Number of bins for line of sight (LOS) vector, pi or mu_r when multipoles are measured. Default is 20. pi_max ( int or float , default: None ) \u2013 Bound for line of sight bins. Bounds will be [-pi_max, pi_max]. Default is None, in which case half the boxsize will be used. boxsize ( int or float or NoneType , default: None ) \u2013 If simulation is not included in SimInfo, a manual boxsize can be added here. Make sure simulation=None and the boxsize units are equal to those in the data dictionary. Default is None. periodicity ( bool , default: True ) \u2013 If True, the periodic boundary conditions of the simulation box are taken into account. If False, they are ignored. Note that because this code used analytical randoms for the simulations, the correlations will not be correct in this case and only DD and S+D terms should be studied. Non-periodic randoms can be measured by providing random data to the code and considering the DD term that is measured. Correlations and covariance matrix will need to be reconstructed from parts. [Please add a request for teh integration of this method of this if you would like to use this option often.] Default is True. Source code in src/measureia/measure_IA_base.py def __init__( self, data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True, ): \"\"\" The __init__ method of the MeasureIABase class. Parameters ---------- data : dict or NoneType Dictionary with data needed for calculations. For cartesian coordinates, the keywords are: 'Position' and 'Position_shape_sample': (N_p,3), (N_s,3) ndarrays with the x, y, z coordinates of the N_p, N_s objects in the position and shape samples, respectively. 'Axis_Direction': (N_s,2) ndarray with the two elements of the unit vectors describing the axis direction of the projected axis of the object shape. 'LOS': index referring back to the column number in the 'Position' samples that contains the line-of-sight coordinate. (e.g. if the shapes are projected over the z-axis, LOS=2) 'q': (N_s) array containing the axis ratio q=b/a for each object in the shape sample. For lightcone coordinates, the keywords are: 'Redshift' and 'Redshift_shape_sample': (N_p) and (N_s) ndarray with redshifts of position and shape samples. 'RA' and 'RA_shape_sample': (N_p) and (N_s) ndarray with RA coordinate of position and shape samples. 'DEC' and 'DEC_shape_sample': (N_p) and (N_s) ndarray with DEC coordinate of position and shape samples. 'e1' and 'e2': (N_s) arrays with the two ellipticity components e1 and e2 of the shape sample objects. output_file_name : str Name and filepath of the file where the output should be stored. Needs to be hdf5-type. simulation : str or NoneType, optional Indicator of simulation, obtaining correct boxsize in cMpc/h automatically. Choose from [TNG100, TNG100_2, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1, FLAMINGO_L2p8]. Default is None, in which case boxsize needs to be added manually; or in the case of observational data, the pi_max. snapshot : int or str or NoneType, optional Number of the snapshot, which, if given, will ensure that the output file to contains a group 'Snapshot_[snapshot]'. If None, the group is omitted from the output file structure. Default is None. separation_limits : iterable of 2 entries, optional Bounds of the (projected) separation vector length bins in cMpc/h (so, r or r_p). Default is [0.1,20]. num_bins_r : int, optional Number of bins for (projected) separation vector. Default is 8. num_bins_pi : int, optional Number of bins for line of sight (LOS) vector, pi or mu_r when multipoles are measured. Default is 20. pi_max : int or float, optional Bound for line of sight bins. Bounds will be [-pi_max, pi_max]. Default is None, in which case half the boxsize will be used. boxsize : int or float or NoneType, optional If simulation is not included in SimInfo, a manual boxsize can be added here. Make sure simulation=None and the boxsize units are equal to those in the data dictionary. Default is None. periodicity : bool, optional If True, the periodic boundary conditions of the simulation box are taken into account. If False, they are ignored. Note that because this code used analytical randoms for the simulations, the correlations will not be correct in this case and only DD and S+D terms should be studied. Non-periodic randoms can be measured by providing random data to the code and considering the DD term that is measured. Correlations and covariance matrix will need to be reconstructed from parts. [Please add a request for teh integration of this method of this if you would like to use this option often.] Default is True. \"\"\" SimInfo.__init__(self, simulation, snapshot, boxsize) self.data = data self.output_file_name = output_file_name self.periodicity = periodicity if periodicity: periodic = \"periodic \" else: periodic = \"\" try: self.Num_position = len(data[\"Position\"]) # number of halos in position sample self.Num_shape = len(data[\"Position_shape_sample\"]) # number of halos in shape sample except: try: self.Num_position = len(data[\"RA\"]) self.Num_shape = len(data[\"RA_shape_sample\"]) except: self.Num_position = 0 self.Num_shape = 0 print(\"Warning: no Postion or Position_shape_sample given.\") if self.Num_position > 0: try: weight = self.data[\"weight\"] except: self.data[\"weight\"] = np.ones(self.Num_position) try: weight = self.data[\"weight_shape_sample\"] except: self.data[\"weight_shape_sample\"] = np.ones(self.Num_shape) self.r_min = separation_limits[0] # cMpc/h self.r_max = separation_limits[1] # cMpc/h self.num_bins_r = num_bins_r self.num_bins_pi = num_bins_pi self.r_bins = np.logspace(np.log10(self.r_min), np.log10(self.r_max), self.num_bins_r + 1) if pi_max == None: if self.L_0p5 is None: raise ValueError( \"Both pi_max and boxsize are None. Provide input on one of them to determine the integration limit pi_max.\") else: pi_max = self.L_0p5 self.pi_bins = np.linspace(-pi_max, pi_max, self.num_bins_pi + 1) self.mu_r_bins = np.linspace(-1, 1, self.num_bins_pi + 1) if simulation == False: print(f\"MeasureIA object initialised with:\\n \\ observational data.\\n \\ There are {self.Num_shape} galaxies in the shape sample and {self.Num_position} galaxies in the position sample.\\n\\ The separation bin edges are given by {self.r_bins} Mpc.\\n \\ There are {num_bins_r} r or r_p bins and {num_bins_pi} pi bins.\\n \\ The maximum pi used for binning is {pi_max}.\\n \\ The data will be written to {self.output_file_name}\") else: print(f\"MeasureIA object initialised with:\\n \\ simulation {simulation} that has a {periodic}boxsize of {self.boxsize} cMpc/h.\\n \\ There are {self.Num_shape} galaxies in the shape sample and {self.Num_position} galaxies in the position sample.\\n\\ The separation bin edges are given by {self.r_bins} cMpc/h.\\n \\ There are {num_bins_r} r or r_p bins and {num_bins_pi} pi bins.\\n \\ The maximum pi used for binning is {pi_max}.\\n \\ The data will be written to {self.output_file_name}\") return","title":"__init__"},{"location":"api/MeasureIABase/#measureia.MeasureIABase.calculate_dot_product_arrays","text":"Calculates the dot product over 2 2D arrays across axis 1 so that dot_product[i] = np.dot(a1[i],a2[i]) Parameters: a1 ( ndarray ) \u2013 First array a2 ( ndarray ) \u2013 Second array Returns: ndarray \u2013 Dot product of columns of arrays Source code in src/measureia/measure_IA_base.py @staticmethod def calculate_dot_product_arrays(a1, a2): \"\"\"Calculates the dot product over 2 2D arrays across axis 1 so that dot_product[i] = np.dot(a1[i],a2[i]) Parameters ---------- a1 : ndarray First array a2 : ndarray Second array Returns ------- ndarray Dot product of columns of arrays \"\"\" dot_product = np.zeros(np.shape(a1)[0]) for i in np.arange(0, np.shape(a1)[1]): dot_product += a1[:, i] * a2[:, i] return dot_product","title":"calculate_dot_product_arrays"},{"location":"api/MeasureIABase/#measureia.MeasureIABase.get_ellipticity","text":"Calculates the radial and tangential components of the ellipticity, given the size of the ellipticty vector and the angle between the semimajor or semiminor axis and the separation vector. Parameters: e ( ndarray ) \u2013 size of the ellipticity vector phi ( ndarray ) \u2013 angle between semimajor/semiminor axis and separation vector Returns: ndarray \u2013 e_+ and e_x Source code in src/measureia/measure_IA_base.py @staticmethod def get_ellipticity(e, phi): \"\"\"Calculates the radial and tangential components of the ellipticity, given the size of the ellipticty vector and the angle between the semimajor or semiminor axis and the separation vector. Parameters ---------- e : ndarray size of the ellipticity vector phi : ndarray angle between semimajor/semiminor axis and separation vector Returns ------- ndarray e_+ and e_x \"\"\" e_plus, e_cross = e * np.cos(2 * phi), e * np.sin(2 * phi) return e_plus, e_cross","title":"get_ellipticity"},{"location":"api/MeasureIABase/#measureia.MeasureIABase.get_random_pairs","text":"Returns analytical value of the number of pairs expected in an r_p, pi bin for a random uniform distribution. (Singh et al. 2023) Parameters: rp_max ( float ) \u2013 Upper bound of projected separation vector bin rp_min ( float ) \u2013 Lower bound of projected separation vector bin pi_max ( float ) \u2013 Upper bound of line of sight vector bin pi_min ( float ) \u2013 Lower bound of line of sight vector bin L3 ( float or int ) \u2013 Volume of the simulation box corrtype ( str ) \u2013 Correlation type, auto or cross. RR for auto is RR_cross/2. Num_position ( int ) \u2013 Number of objects in the position sample. Num_shape ( int ) \u2013 Number of objects in the shape sample. Returns: float \u2013 number of pairs in r_p, pi bin Source code in src/measureia/measure_IA_base.py @staticmethod def get_random_pairs(rp_max, rp_min, pi_max, pi_min, L3, corrtype, Num_position, Num_shape): \"\"\"Returns analytical value of the number of pairs expected in an r_p, pi bin for a random uniform distribution. (Singh et al. 2023) Parameters ---------- rp_max : float Upper bound of projected separation vector bin rp_min : float Lower bound of projected separation vector bin pi_max : float Upper bound of line of sight vector bin pi_min : float Lower bound of line of sight vector bin L3 : float or int Volume of the simulation box corrtype : str Correlation type, auto or cross. RR for auto is RR_cross/2. Num_position : int Number of objects in the position sample. Num_shape : int Number of objects in the shape sample. Returns ------- float number of pairs in r_p, pi bin \"\"\" if corrtype == \"auto\": RR = ( (Num_position - 1.0) * Num_shape / 2.0 * np.pi * (rp_max ** 2 - rp_min ** 2) * abs(pi_max - pi_min) / L3 ) # volume is cylindrical pi*dr^2 * height elif corrtype == \"cross\": RR = Num_position * Num_shape * np.pi * (rp_max ** 2 - rp_min ** 2) * abs(pi_max - pi_min) / L3 else: raise ValueError(\"Unknown input for corrtype, choose from auto or cross.\") return RR","title":"get_random_pairs"},{"location":"api/MeasureIABase/#measureia.MeasureIABase.get_volume_spherical_cap","text":"Calculate the volume of a spherical cap. Parameters: mur ( float ) \u2013 cos(theta), where theta is the polar angle between the apex and disk of the cap. r ( float ) \u2013 radius Returns: float \u2013 Volume of the spherical cap. Source code in src/measureia/measure_IA_base.py @staticmethod def get_volume_spherical_cap(mur, r): \"\"\"Calculate the volume of a spherical cap. Parameters ---------- mur : float cos(theta), where theta is the polar angle between the apex and disk of the cap. r : float radius Returns ------- float Volume of the spherical cap. \"\"\" return np.pi / 3.0 * r ** 3 * (2 + mur) * (1 - mur) ** 2","title":"get_volume_spherical_cap"},{"location":"api/MeasureIABase/#measureia.MeasureIABase.get_random_pairs_r_mur","text":"Returns analytical value of the number of pairs expected in an r, mu_r bin for a random uniform distribution. Parameters: r_max ( float ) \u2013 Upper bound of separation vector bin r_min ( float ) \u2013 Lower bound of separation vector bin mur_max ( float ) \u2013 Upper bound of mu_r bin mur_min ( float ) \u2013 Lower bound of mu_r bin L3 ( float ) \u2013 Volume of the simulation box corrtype ( str ) \u2013 Correlation type, auto or cross. RR for auto is RR_cross/2. Num_position ( int ) \u2013 Number of objects in the position sample. Num_shape ( int ) \u2013 Number of objects in the shape sample. Returns: float \u2013 number of pairs in r, mu_r bin Source code in src/measureia/measure_IA_base.py def get_random_pairs_r_mur(self, r_max, r_min, mur_max, mur_min, L3, corrtype, Num_position, Num_shape): \"\"\"Returns analytical value of the number of pairs expected in an r, mu_r bin for a random uniform distribution. Parameters ---------- r_max : float Upper bound of separation vector bin r_min : float Lower bound of separation vector bin mur_max : float Upper bound of mu_r bin mur_min : float Lower bound of mu_r bin L3 : float Volume of the simulation box corrtype : str Correlation type, auto or cross. RR for auto is RR_cross/2. Num_position : int Number of objects in the position sample. Num_shape : int Number of objects in the shape sample. Returns ------- float number of pairs in r, mu_r bin \"\"\" if corrtype == \"auto\": RR = ( (Num_position - 1.0) / 2.0 * Num_shape * ( self.get_volume_spherical_cap(mur_min, r_max) - self.get_volume_spherical_cap(mur_max, r_max) - (self.get_volume_spherical_cap(mur_min, r_min) - self.get_volume_spherical_cap(mur_max, r_min)) ) / L3 ) # volume is big cap - small cap for large - small radius elif corrtype == \"cross\": RR = ( (Num_position - 1.0) * Num_shape * ( self.get_volume_spherical_cap(mur_min, r_max) - self.get_volume_spherical_cap(mur_max, r_max) - (self.get_volume_spherical_cap(mur_min, r_min) - self.get_volume_spherical_cap(mur_max, r_min)) ) / L3 ) else: raise ValueError(\"Unknown input for corrtype, choose from auto or cross.\") return abs(RR)","title":"get_random_pairs_r_mur"},{"location":"api/MeasureIABase/#measureia.MeasureIABase.setdiff2D","text":"Compares each row of a1 and a2 and returns the elements that do not overlap Parameters: a1 ( nested list ) \u2013 List containing lists of elements to compare to a2 a2 ( nested list ) \u2013 List containing lists of elements to compare to a1 Returns: nested list \u2013 For each row, the not-overlapping elements between a1 and a2 Source code in src/measureia/measure_IA_base.py @staticmethod def setdiff2D(a1, a2): \"\"\"Compares each row of a1 and a2 and returns the elements that do not overlap Parameters ---------- a1 : nested list List containing lists of elements to compare to a2 a2 : nested list List containing lists of elements to compare to a1 Returns ------- nested list For each row, the not-overlapping elements between a1 and a2 \"\"\" assert len(a1) == len(a2), \"Lengths of lists where each row is to be compared, are not the same.\" diff = [] for i in np.arange(0, len(a1)): setdiff = np.setdiff1d(a1[i], a2[i]) diff.append(setdiff) del setdiff return diff","title":"setdiff2D"},{"location":"api/MeasureIABase/#measureia.MeasureIABase.setdiff_omit","text":"For rows in nested list a1, whose index is included in incl_ind, returns elements that do not overlap between the row in a1 and a2. Parameters: a1 ( nested list ) \u2013 List of lists or arrays where indicated rows need to be compared to a2 a2 ( list or array ) \u2013 Elements to be compared to the row in a1 [and not included in return values]. incl_ind ( list or array ) \u2013 Indices of rows in a1 to be compared to a2. Returns: nested list \u2013 For each included row in a1, the not-overlapping elements between a1 and a2 Source code in src/measureia/measure_IA_base.py @staticmethod def setdiff_omit(a1, a2, incl_ind): \"\"\"For rows in nested list a1, whose index is included in incl_ind, returns elements that do not overlap between the row in a1 and a2. Parameters ---------- a1 : nested list List of lists or arrays where indicated rows need to be compared to a2 a2 : list or array Elements to be compared to the row in a1 [and not included in return values]. incl_ind : list or array Indices of rows in a1 to be compared to a2. Returns ------- nested list For each included row in a1, the not-overlapping elements between a1 and a2 \"\"\" diff = [] for i in np.arange(0, len(a1)): if np.isin(i, incl_ind): setdiff = np.setdiff1d(a1[i], a2) diff.append(setdiff) del setdiff return diff","title":"setdiff_omit"},{"location":"api/MeasureIABase/#measureia.MeasureIABase.assign_jackknife_patches","text":"Assigns jackknife patches to data and randoms given a number of patches. Based on https://github.com/esheldon/kmeans_radec Parameters: data ( dict ) \u2013 Dictionary containing position and shape sample data. Keywords: \"RA\", \"DEC\", \"RA_shape_sample\", \"DEC_shape_sample\" randoms_data ( dict ) \u2013 Dictionary containing position and shape sample data of randoms. Keywords: \"RA\", \"DEC\", \"RA_shape_sample\", \"DEC_shape_sample\" num_jk ( int ) \u2013 Number of jackknife patches Returns: dict \u2013 Dictionary with patch numbers for each sample. Keywords: 'position', 'shape', 'randoms_position', 'randoms_shape' Source code in src/measureia/measure_IA_base.py def assign_jackknife_patches(self, data, randoms_data, num_jk): \"\"\"Assigns jackknife patches to data and randoms given a number of patches. Based on https://github.com/esheldon/kmeans_radec Parameters ---------- data : dict Dictionary containing position and shape sample data. Keywords: \"RA\", \"DEC\", \"RA_shape_sample\", \"DEC_shape_sample\" randoms_data : dict Dictionary containing position and shape sample data of randoms. Keywords: \"RA\", \"DEC\", \"RA_shape_sample\", \"DEC_shape_sample\" num_jk : int Number of jackknife patches Returns ------- dict Dictionary with patch numbers for each sample. Keywords: 'position', 'shape', 'randoms_position', 'randoms_shape' \"\"\" jk_patches = {} # Read the randoms file from which the jackknife regions will be created RA = randoms_data['RA'] DEC = randoms_data['DEC'] # Define a number of jaccknife regions and find their centres using kmans X = np.column_stack((RA, DEC)) km = kmeans_sample(X, num_jk, maxiter=100, tol=1.0e-5) jk_labels = km.labels jk_patches['randoms_position'] = jk_labels RA = randoms_data['RA_shape_sample'] DEC = randoms_data['DEC_shape_sample'] X2 = np.column_stack((RA, DEC)) jk_labels = km.find_nearest(X2) jk_patches['randoms_shape'] = jk_labels RA = data['RA'] DEC = data['DEC'] X2 = np.column_stack((RA, DEC)) jk_labels = km.find_nearest(X2) jk_patches['position'] = jk_labels RA = data['RA_shape_sample'] DEC = data['DEC_shape_sample'] X2 = np.column_stack((RA, DEC)) jk_labels = km.find_nearest(X2) jk_patches['shape'] = jk_labels return jk_patches","title":"assign_jackknife_patches"},{"location":"api/MeasureJackknife/","text":"MeasureJackknife measureia.MeasureJackknife Bases: MeasureWLightcone , MeasureMultipolesLightcone Class that contains all methods for jackknife covariance measurements for IA correlation functions. Methods: Name Description _measure_jackknife_realisations_obs Measures all jackknife realisations for MeasureIALightcone using 1 or more CPUs. _measure_jackknife_covariance_obs Combines jackknife realisations for MeasureIALightcone into covariance. _measure_jackknife_realisations_obs_multiprocessing Measures all jackknife realisations for MeasureIALightcone using >1 CPU. measure_covariance_multiple_datasets Given the jackknife realisations of two datasets, creates the cross covariance. create_full_cov_matrix_projections Creates larger covariance matrix of multiple datasets including cross terms. Notes Inherits attributes from 'SimInfo', where 'boxsize', 'L_0p5' and 'snap_group' are used in this class. Inherits attributes from 'MeasureIABase', where 'data', 'output_file_name', 'periodicity', 'Num_position', 'Num_shape', 'r_min', 'r_max', 'num_bins_r', 'num_bins_pi', 'r_bins', 'pi_bins', 'mu_r_bins' are used. Source code in src/measureia/measure_jackknife.py class MeasureJackknife(MeasureWLightcone, MeasureMultipolesLightcone): \"\"\"Class that contains all methods for jackknife covariance measurements for IA correlation functions. Methods ------- _measure_jackknife_realisations_obs() Measures all jackknife realisations for MeasureIALightcone using 1 or more CPUs. _measure_jackknife_covariance_obs() Combines jackknife realisations for MeasureIALightcone into covariance. _measure_jackknife_realisations_obs_multiprocessing() Measures all jackknife realisations for MeasureIALightcone using >1 CPU. measure_covariance_multiple_datasets() Given the jackknife realisations of two datasets, creates the cross covariance. create_full_cov_matrix_projections() Creates larger covariance matrix of multiple datasets including cross terms. Notes ----- Inherits attributes from 'SimInfo', where 'boxsize', 'L_0p5' and 'snap_group' are used in this class. Inherits attributes from 'MeasureIABase', where 'data', 'output_file_name', 'periodicity', 'Num_position', 'Num_shape', 'r_min', 'r_max', 'num_bins_r', 'num_bins_pi', 'r_bins', 'pi_bins', 'mu_r_bins' are used. \"\"\" def __init__( self, data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True, ): \"\"\" The __init__ method of the MeasureJackknife class. Notes ----- Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. \"\"\" super().__init__(data, output_file_name, simulation, snapshot, separation_limits, num_bins_r, num_bins_pi, pi_max, boxsize, periodicity) return def _measure_jackknife_realisations_lightcone( self, patches_pos, patches_shape, corr_type, dataset_name, masks=None, rp_cut=None, over_h=False, cosmology=None, count_pairs=False, data_suffix=\"\", num_sample_names=[\"S\", \"D\"] ): \"\"\"Measures the jackknife realisations for the projected correlation functions in MeasureIALightcone using the jackknife method. The area is already divided into patches; the correlation function is calculated omitting one patch at a time. This method uses 1 or more CPUs. Parameters ---------- patches_pos : ndarray Array with the patch numbers of each object in the position sample. patches_shape : ndarray Array with the patch numbers of each object in the shape sample. dataset_name : str Name of the dataset in the output file. corr_type : iterable with 2 str entries Array with two entries. For first choose from [gg, g+, both], for second from [w, multipoles]. masks : dict or NoneType, optional See MeasureIALightcone methods. Default is None. rp_cut : float or NoneType, optional See MeasureIALightcone.measure_xi_multipoles. Default is None. over_h : bool, optional See MeasureIALightcone. Default value is False. cosmology : pyccl cosmology object or NoneType, optional See MeasureIALightcone. Default is None. count_pairs : bool, optional If True, only gg is measured, not g+. Default value is False. data_suffix : str, optional Addition to dataset name. Used to distinguish between DR,DD and RR measurements. Default value is \"\". num_sample_names : list with two entries, optional Keywords of the num_samples dictionary to access number of objects in position ond shape samples. Default value is [\"S\", \"D\"]. Returns ------- \"\"\" if count_pairs and data_suffix == \"\": raise ValueError(\"Enter a data suffix (like _DD) for your pair count.\") figname_dataset_name = dataset_name if \"/\" in dataset_name: figname_dataset_name = figname_dataset_name.replace(\"/\", \"_\") if \".\" in dataset_name: figname_dataset_name = figname_dataset_name.replace(\".\", \"p\") min_patch, max_patch = int(min(patches_pos)), int(max(patches_pos)) num_patches = max_patch - min_patch + 1 if min(patches_shape) != min_patch: print( \"Warning! Minimum patch number of shape sample is not equal to minimum patch number of position sample.\") if max(patches_shape) != max_patch: print( \"Warning! Maximum patch number of shape sample is not equal to maximum patch number of position sample.\") print( f\"Calculating jackknife realisations for {num_patches} patches for {dataset_name}.\") for i in np.arange(min_patch, max_patch + 1): mask_position = (patches_pos != i) mask_shape = (patches_shape != i) if masks != None: mask_position = mask_position * masks[\"Redshift\"] mask_shape = mask_shape * masks[\"Redshift_shape_sample\"] self.num_samples[f\"{i}\"][num_sample_names[0]] = sum(mask_shape) self.num_samples[f\"{i}\"][num_sample_names[1]] = sum(mask_position) masks_total = { \"Redshift\": mask_position, \"Redshift_shape_sample\": mask_shape, \"RA\": mask_position, \"RA_shape_sample\": mask_shape, \"DEC\": mask_position, \"DEC_shape_sample\": mask_shape, \"e1\": mask_shape, \"e2\": mask_shape, \"weight\": mask_position, \"weight_shape_sample\": mask_shape, } if corr_type[1] == \"multipoles\": if count_pairs: self._count_pairs_xi_r_mur_lightcone_brute(masks=masks_total, dataset_name=dataset_name + \"_\" + str(i), over_h=over_h, cosmology=cosmology, print_num=False, data_suffix=data_suffix, rp_cut=rp_cut, jk_group_name=f\"{dataset_name}_jk{num_patches}\") else: self._measure_xi_r_mur_lightcone_brute( masks=masks_total, rp_cut=rp_cut, dataset_name=dataset_name + \"_\" + str(i), print_num=False, over_h=over_h, cosmology=cosmology, jk_group_name=f\"{dataset_name}_jk{num_patches}\", ) else: if count_pairs: self._count_pairs_xi_rp_pi_lightcone_brute(masks=masks_total, dataset_name=dataset_name + \"_\" + str(i), over_h=over_h, cosmology=cosmology, print_num=False, data_suffix=data_suffix, jk_group_name=f\"{dataset_name}_jk{num_patches}\") else: self._measure_xi_rp_pi_lightcone_brute( masks=masks_total, dataset_name=dataset_name + \"_\" + str(i), print_num=False, over_h=over_h, cosmology=cosmology, jk_group_name=f\"{dataset_name}_jk{num_patches}\", ) return def _measure_jackknife_covariance_lightcone( self, IA_estimator, corr_type, dataset_name, max_patch, min_patch=1, randoms_suf=\"_randoms\" ): \"\"\"Combines the jackknife realisations measured with _measure_jackknife_realisations_obs or _measure_jackknife_realisations_obs_multiprocessing into a covariance. Parameters ---------- IA_estimator : str Choose which type of xi estimator is used. Choose from \"clusters\" or \"galaxies\". dataset_name : str Name of the dataset in the output file. corr_type : iterable with 2 str entries Array with two entries. For first choose from [gg, g+, both], for second from [w, multipoles]. max_patch : int Maximum patch number used. min_patch : int, optional Minimum patch number used. Default value is 1. randoms_suf : str, optional Suffix used to denote the datasets that have been created using the randoms. Default value is \"_randoms\". Returns ------- \"\"\" if corr_type[0] == \"both\": data = [corr_type[1] + \"_g_plus\", corr_type[1] + \"_gg\"] elif corr_type[0] == \"g+\": data = [corr_type[1] + \"_g_plus\"] elif corr_type[0] == \"gg\": data = [corr_type[1] + \"_gg\"] else: raise KeyError(\"Unknown value for corr_type. Choose from [g+, gg, both]\") figname_dataset_name = dataset_name if \"/\" in dataset_name: figname_dataset_name = figname_dataset_name.replace(\"/\", \"_\") if \".\" in dataset_name: figname_dataset_name = figname_dataset_name.replace(\".\", \"p\") num_patches = max_patch - min_patch + 1 print( f\"Calculating jackknife errors for {num_patches} patches for {dataset_name} with {dataset_name}{randoms_suf} as randoms.\") covs, stds = [], [] for d in np.arange(0, len(data)): for b in np.arange(min_patch, max_patch + 1): self._obs_estimator(corr_type, IA_estimator, f\"{dataset_name}_{b}\", f\"{dataset_name}{randoms_suf}_{b}\", self.num_samples[f\"{b}\"], jk_group_name=f\"{dataset_name}_jk{num_patches}\", jk_group_name_randoms=f\"{dataset_name}{randoms_suf}_jk{num_patches}\") if \"w\" in data[d]: self._measure_w_g_i(corr_type=corr_type[0], dataset_name=f\"{dataset_name}_{b}\", jk_group_name=f\"{dataset_name}_jk{num_patches}\") else: self._measure_multipoles(corr_type=corr_type[0], dataset_name=f\"{dataset_name}_{b}\", jk_group_name=f\"{dataset_name}_jk{num_patches}\") data_file = h5py.File(self.output_file_name, \"a\") group_multipoles = data_file[f\"{self.snap_group}/{data[d]}/{dataset_name}_jk{num_patches}\"] # calculating mean of the datavectors mean_multipoles = np.zeros(self.num_bins_r) for b in np.arange(min_patch, max_patch + 1): mean_multipoles += group_multipoles[f\"{dataset_name}_{b}\"][:] mean_multipoles /= num_patches # calculation the covariance matrix (multipoles) and the standard deviation (sqrt of diag of cov) cov = np.zeros((self.num_bins_r, self.num_bins_r)) std = np.zeros(self.num_bins_r) for b in np.arange(min_patch, max_patch + 1): correlation = group_multipoles[f\"{dataset_name}_{b}\"][:] std += (correlation - mean_multipoles) ** 2 for i in np.arange(self.num_bins_r): cov[:, i] += (correlation - mean_multipoles) * (correlation[i] - mean_multipoles[i]) std *= (num_patches - 1) / num_patches # see Singh 2023 std = np.sqrt(std) # size of errorbars cov *= (num_patches - 1) / num_patches # cov not sqrt so to get std, sqrt of diag would need to be taken data_file.close() if self.output_file_name != None: output_file = h5py.File(self.output_file_name, \"a\") group_multipoles = create_group_hdf5(output_file, f\"{self.snap_group}/\" + data[d]) write_dataset_hdf5(group_multipoles, dataset_name + \"_mean_\" + str(num_patches), data=mean_multipoles) write_dataset_hdf5(group_multipoles, dataset_name + \"_jackknife_\" + str(num_patches), data=std) write_dataset_hdf5(group_multipoles, dataset_name + \"_jackknife_cov_\" + str(num_patches), data=cov) output_file.close() else: covs.append(cov) stds.append(std) if self.output_file_name != None: return else: return covs, stds def _measure_jackknife_realisations_lightcone_multiprocessing( self, patches_pos, patches_shape, corr_type, dataset_name, masks=None, rp_cut=None, over_h=False, num_nodes=4, cosmology=None, count_pairs=False, data_suffix=\"\", num_sample_names=[\"S\", \"D\"] ): \"\"\"Measures the jackknife realisations for the projected correlation functions in MeasureIALightcone using the jackknife method. The area is already divided into patches; the correlation function is calculated omitting one patch at a time. This method uses >1 CPUs. Parameters ---------- patches_pos : ndarray Array with the patch numbers of each object in the position sample. patches_shape : ndarray Array with the patch numbers of each object in the shape sample. dataset_name : str Name of the dataset in the output file. corr_type : iterable with 2 str entries Array with two entries. For first choose from [gg, g+, both], for second from [w, multipoles]. masks : dict or NoneType, optional See MeasureIALightcone methods. Default is None. rp_cut : float or NoneType, optional See MeasureIALightcone.measure_xi_multipoles. Default is None. over_h : bool, optional See MeasureIALightcone. Default value is False. num_nodes : int, optional Number of cores to be used in multiprocessing. Default is 4. cosmology : pyccl cosmology object or NoneType, optional See MeasureIALightcone. Default is None. count_pairs : bool, optional If True, only gg is measured, not g+. Default value is False. data_suffix : str, optional Addition to dataset name. Used to distinguish between DR,DD and RR measurements. Default value is \"\". num_sample_names : list with two entries, optional Keywords of the num_samples dictionary to access number of objects in position ond shape samples. Default value is [\"S\", \"D\"]. Returns ------- \"\"\" if num_nodes == 1: self._measure_jackknife_realisations_lightcone(patches_pos, patches_shape, masks, corr_type, dataset_name, rp_cut, over_h, cosmology, count_pairs, data_suffix, num_sample_names) return if count_pairs == False: corr_type[0] = \"both\" if corr_type[0] == \"both\": data = [corr_type[1] + \"_g_plus\", corr_type[1] + \"_gg\"] corr_type_suff = [\"_g_plus\", \"_gg\"] xi_suff = [\"SplusD\", \"DD\"] elif corr_type[0] == \"g+\": data = [corr_type[1] + \"_g_plus\"] corr_type_suff = [\"_g_plus\"] xi_suff = [\"SplusD\"] elif corr_type[0] == \"gg\": data = [corr_type[1] + \"_gg\"] corr_type_suff = [\"_gg\"] xi_suff = [\"DD\"] else: raise KeyError(\"Unknown value for first entry of corr_type. Choose from [g+, gg, both]\") if corr_type[1] == \"multipoles\": bin_var_names = [\"r\", \"mu_r\"] elif corr_type[1] == \"w\": bin_var_names = [\"rp\", \"pi\"] else: raise KeyError(\"Unknown value for second entry of corr_type. Choose from [multipoles, w_g_plus]\") min_patch, max_patch = int(min(patches_pos)), int(max(patches_pos)) num_patches = max_patch - min_patch + 1 if min(patches_shape) != min_patch: print( \"Warning! Minimum patch number of shape sample is not equal to minimum patch number of position sample.\") if max(patches_shape) != max_patch: print( \"Warning! Maximum patch number of shape sample is not equal to maximum patch number of position sample.\") args_xi_g_plus, args_multipoles, tree_args = [], [], [] for i in np.arange(min_patch, max_patch + 1): mask_position = (patches_pos != i) mask_shape = (patches_shape != i) if masks != None: mask_position = mask_position * masks[\"Redshift\"] mask_shape = mask_shape * masks[\"Redshift_shape_sample\"] self.num_samples[f\"{i}\"][num_sample_names[0]] = sum(mask_shape) self.num_samples[f\"{i}\"][num_sample_names[1]] = sum(mask_position) masks_total = { \"Redshift\": mask_position, \"Redshift_shape_sample\": mask_shape, \"RA\": mask_position, \"RA_shape_sample\": mask_shape, \"DEC\": mask_position, \"DEC_shape_sample\": mask_shape, \"e1\": mask_shape, \"e2\": mask_shape, \"weight\": mask_position, \"weight_shape_sample\": mask_shape, } if corr_type[1] == \"multipoles\": args_xi_g_plus.append( ( dataset_name + \"_\" + str(i), masks_total, True, False, over_h, cosmology, rp_cut, data_suffix ) ) else: args_xi_g_plus.append( ( dataset_name + \"_\" + str(i), masks_total, True, False, over_h, cosmology, data_suffix ) ) args_xi_g_plus = np.array(args_xi_g_plus) multiproc_chuncks = np.array_split(np.arange(num_patches), np.ceil(num_patches / num_nodes)) for chunck in multiproc_chuncks: chunck = np.array(chunck, dtype=int) if corr_type[1] == \"multipoles\": if count_pairs: result = ProcessingPool(nodes=len(chunck)).map( self._count_pairs_xi_r_mur_lightcone_brute, args_xi_g_plus[chunck][:, 0], args_xi_g_plus[chunck][:, 1], args_xi_g_plus[chunck][:, 2], args_xi_g_plus[chunck][:, 3], args_xi_g_plus[chunck][:, 4], args_xi_g_plus[chunck][:, 5], args_xi_g_plus[chunck][:, 6], args_xi_g_plus[chunck][:, 7], ) else: result = ProcessingPool(nodes=len(chunck)).map( self._measure_xi_r_mur_lightcone_brute, args_xi_g_plus[chunck][:, 0], args_xi_g_plus[chunck][:, 1], args_xi_g_plus[chunck][:, 2], args_xi_g_plus[chunck][:, 3], args_xi_g_plus[chunck][:, 4], args_xi_g_plus[chunck][:, 5], args_xi_g_plus[chunck][:, 6], ) else: if count_pairs: result = ProcessingPool(nodes=len(chunck)).map( self._count_pairs_xi_rp_pi_lightcone_brute, args_xi_g_plus[chunck][:, 0], args_xi_g_plus[chunck][:, 1], args_xi_g_plus[chunck][:, 2], args_xi_g_plus[chunck][:, 3], args_xi_g_plus[chunck][:, 4], args_xi_g_plus[chunck][:, 5], args_xi_g_plus[chunck][:, 6], ) else: result = ProcessingPool(nodes=len(chunck)).map( self._measure_xi_rp_pi_lightcone_brute, args_xi_g_plus[chunck][:, 0], args_xi_g_plus[chunck][:, 1], args_xi_g_plus[chunck][:, 2], args_xi_g_plus[chunck][:, 3], args_xi_g_plus[chunck][:, 4], args_xi_g_plus[chunck][:, 5], ) output_file = h5py.File(self.output_file_name, \"a\") if count_pairs: for i in np.arange(0, len(chunck)): for j, data_j in enumerate(data): group_xigplus = create_group_hdf5( output_file, f\"{self.snap_group}/{corr_type[1]}/xi_gg/{dataset_name}_jk{num_patches}\" ) write_dataset_hdf5(group_xigplus, f\"{dataset_name}_{chunck[i] + min_patch}{data_suffix}\", data=result[i][j]) write_dataset_hdf5( group_xigplus, f\"{dataset_name}_{chunck[i] + min_patch}_{bin_var_names[0]}\", data=result[i][1] ) write_dataset_hdf5( group_xigplus, f\"{dataset_name}_{chunck[i] + min_patch}_{bin_var_names[1]}\", data=result[i][2] ) else: for i in np.arange(0, len(chunck)): for j, data_j in enumerate(data): group_xigplus = create_group_hdf5( output_file, f\"{self.snap_group}/{corr_type[1]}/xi{corr_type_suff[j]}/{dataset_name}_jk{num_patches}\" ) write_dataset_hdf5(group_xigplus, f\"{dataset_name}_{chunck[i] + min_patch}_{xi_suff[j]}\", data=result[i][j]) write_dataset_hdf5( group_xigplus, f\"{dataset_name}_{chunck[i] + min_patch}_{bin_var_names[0]}\", data=result[i][2] ) write_dataset_hdf5( group_xigplus, f\"{dataset_name}_{chunck[i] + min_patch}_{bin_var_names[1]}\", data=result[i][3] ) # write_dataset_hdf5(group_xigplus, f\"{dataset_name}_{chunck[i]}_sigmasq\", data=result[i][3]) output_file.close() # for i in np.arange(0, num_patches): # if corr_type[1] == \"multipoles\": # self.measure_multipoles(corr_type=args_multipoles[i, 0], dataset_name=args_multipoles[i, 1]) # else: # self.measure_w_g_i(corr_type=args_multipoles[i, 0], dataset_name=args_multipoles[i, 1]) return def measure_covariance_multiple_datasets(self, corr_type, dataset_names, num_box=27, return_output=False): \"\"\"Combines the jackknife measurements for different datasets into one covariance matrix. Author: Marta Garcia Escobar (starting from measure_jackknife methods); updated Parameters ---------- corr_type : str Which type of correlation is measured. Takes 'w_g_plus', 'w_gg', 'multipoles_g_plus' or 'multipoles_gg'. dataset_names : list of str List of the dataset names. If there is only one value, it calculates the covariance matrix with itself. num_box : int, optional Number of jackknife realisations. Default value is 27. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. Returns ------- ndarray, ndarray covariance, standard deviation \"\"\" # check if corr_type is valid valid_corr_types = [\"w_g_plus\", \"multipoles_g_plus\", \"w_gg\", \"multipoles_gg\"] if corr_type not in valid_corr_types: raise ValueError(\"corr_type must be 'w_g_plus', 'w_gg', 'multipoles_g_plus' or 'multipoles_gg'.\") data_file = h5py.File(self.output_file_name, \"a\") mean_list = [] # list of arrays for dataset_name in dataset_names: group = data_file[f\"{self.snap_group}/{corr_type}/{dataset_name}_jk{num_box}\"] mean_multipoles = np.zeros(self.num_bins_r) for b in np.arange(0, num_box): mean_multipoles += group[dataset_name + \"_\" + str(b)] mean_multipoles /= num_box mean_list.append(mean_multipoles) # calculation the covariance matrix and the standard deviation (sqrt of diag of cov) cov = np.zeros((self.num_bins_r, self.num_bins_r)) std = np.zeros(self.num_bins_r) if len(dataset_names) == 1: # covariance with itself dataset_name = dataset_names[0] group = data_file[f\"{self.snap_group}/{corr_type}/{dataset_name}_jk{num_box}\"] for b in np.arange(0, num_box): std += (group[dataset_name + \"_\" + str(b)] - mean_list[0]) ** 2 for i in np.arange(self.num_bins_r): cov[:, i] += (group[dataset_name + \"_\" + str(b)] - mean_list[0]) * ( group[dataset_name + \"_\" + str(b)][i] - mean_list[0][i] ) elif len(dataset_names) == 2: group0 = data_file[f\"{self.snap_group}/{corr_type}/{dataset_names[0]}_jk{num_box}\"] group1 = data_file[f\"{self.snap_group}/{corr_type}/{dataset_names[1]}_jk{num_box}\"] for b in np.arange(0, num_box): std += (group0[dataset_names[0] + \"_\" + str(b)] - mean_list[0]) * ( group1[dataset_names[1] + \"_\" + str(b)] - mean_list[1]) for i in np.arange(self.num_bins_r): cov[:, i] += (group0[dataset_names[0] + \"_\" + str(b)] - mean_list[0]) * ( group1[dataset_names[1] + \"_\" + str(b)][i] - mean_list[1][i] ) else: raise KeyError(\"Too many datasets given, choose either 1 or 2\") std *= (num_box - 1) / num_box # see Singh 2023 std = np.sqrt(std) # size of errorbars cov *= (num_box - 1) / num_box # cov not sqrt so to get std, sqrt of diag would need to be taken data_file.close() if (self.output_file_name != None) and (return_output == False): output_file = h5py.File(self.output_file_name, \"a\") group = create_group_hdf5(output_file, f\"{self.snap_group}/{corr_type}\") if len(dataset_names) == 2: write_dataset_hdf5(group, dataset_names[0] + \"_\" + dataset_names[1] + \"_jackknife_cov_\" + str( num_box), data=cov) write_dataset_hdf5(group, dataset_names[0] + \"_\" + dataset_names[1] + \"_jackknife_\" + str(num_box), data=std) else: write_dataset_hdf5(group, dataset_names[0] + \"_jackknife_cov_\" + str(num_box), data=cov) write_dataset_hdf5(group, dataset_names[0] + \"_jackknife_\" + str(num_box), data=std) output_file.close() return else: return cov, std def create_full_cov_matrix_projections(self, corr_type, dataset_names=[\"LOS_x\", \"LOS_y\", \"LOS_z\"], num_box=27, return_output=False): \"\"\"Function that creates the full covariance matrix for all 3 projections and combined covariance for 2 projections by combining previously obtained jackknife information. Generalised from Marta Garcia Escobar's code. Parameters ---------- corr_type : str Which type of correlation is measured. Takes 'w_g_plus', 'w_gg', 'multipoles_g_plus' or 'multipoles_gg'. num_box : int, optional Number of jackknife realisations. Default value is 27. dataset_names : list of str Dataset names of projections to be combined. Default value is [\"LOS_x\",\"LOS_y\",\"LOS_z\"]. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. Returns ------- ndarrays covariance for 3 projections, covariance for x and y, covariance for x and z, covariance for y and z \"\"\" self.measure_covariance_multiple_datasets(corr_type=corr_type, dataset_names=[dataset_names[0], dataset_names[1]], num_box=num_box) self.measure_covariance_multiple_datasets(corr_type=corr_type, dataset_names=[dataset_names[0], dataset_names[2]], num_box=num_box) self.measure_covariance_multiple_datasets(corr_type=corr_type, dataset_names=[dataset_names[1], dataset_names[2]], num_box=num_box) # import needed datasets output_file = h5py.File(self.output_file_name, \"a\") group = output_file[f\"{self.snap_group}/{corr_type}\"] # cov matrix between datasets cov_xx = group[f'{dataset_names[0]}_jackknife_cov_{num_box}'][:] cov_yy = group[f'{dataset_names[1]}_jackknife_cov_{num_box}'][:] cov_zz = group[f'{dataset_names[2]}_jackknife_cov_{num_box}'][:] cov_xy = group[f'{dataset_names[0]}_{dataset_names[1]}_jackknife_cov_{num_box}'][:] cov_yz = group[f'{dataset_names[0]}_{dataset_names[2]}_jackknife_cov_{num_box}'][:] cov_xz = group[f'{dataset_names[1]}_{dataset_names[2]}_jackknife_cov_{num_box}'][:] # 3 projections cov_top = np.concatenate((cov_xx, cov_xy, cov_xz), axis=1) cov_middle = np.concatenate((cov_xy.T, cov_yy, cov_yz), axis=1) # cov_xy.T = cov_yx cov_bottom = np.concatenate((cov_xz.T, cov_yz.T, cov_zz), axis=1) cov3 = np.concatenate((cov_top, cov_middle, cov_bottom), axis=0) # all 2 projections cov_top = np.concatenate((cov_xx, cov_xy), axis=1) cov_middle = np.concatenate((cov_xy.T, cov_yy), axis=1) # cov_xz.T = cov_zx cov2xy = np.concatenate((cov_top, cov_middle), axis=0) cov_top = np.concatenate((cov_xx, cov_xz), axis=1) cov_middle = np.concatenate((cov_xz.T, cov_zz), axis=1) # cov_xz.T = cov_zx cov2xz = np.concatenate((cov_top, cov_middle), axis=0) cov_top = np.concatenate((cov_yy, cov_yz), axis=1) cov_middle = np.concatenate((cov_yz.T, cov_zz), axis=1) # cov_xz.T = cov_zx cov2yz = np.concatenate((cov_top, cov_middle), axis=0) if return_output: return cov3, cov2xy, cov2xz, cov2yz else: write_dataset_hdf5(group, f\"{dataset_names[0]}_{dataset_names[1]}_{dataset_names[2]}_combined_jackknife_cov_{num_box}\", data=cov3) write_dataset_hdf5(group, f'{dataset_names[0]}_{dataset_names[1]}_combined_jackknife_cov_{num_box}', data=cov2xy) write_dataset_hdf5(group, f'{dataset_names[0]}_{dataset_names[2]}_combined_jackknife_cov_{num_box}', data=cov2xz) write_dataset_hdf5(group, f'{dataset_names[1]}_{dataset_names[2]}_combined_jackknife_cov_{num_box}', data=cov2yz) return __init__(data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True) The init method of the MeasureJackknife class. Notes Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. Source code in src/measureia/measure_jackknife.py def __init__( self, data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True, ): \"\"\" The __init__ method of the MeasureJackknife class. Notes ----- Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. \"\"\" super().__init__(data, output_file_name, simulation, snapshot, separation_limits, num_bins_r, num_bins_pi, pi_max, boxsize, periodicity) return measure_covariance_multiple_datasets(corr_type, dataset_names, num_box=27, return_output=False) Combines the jackknife measurements for different datasets into one covariance matrix. Author: Marta Garcia Escobar (starting from measure_jackknife methods); updated Parameters: corr_type ( str ) \u2013 Which type of correlation is measured. Takes 'w_g_plus', 'w_gg', 'multipoles_g_plus' or 'multipoles_gg'. dataset_names ( list of str ) \u2013 List of the dataset names. If there is only one value, it calculates the covariance matrix with itself. num_box ( int , default: 27 ) \u2013 Number of jackknife realisations. Default value is 27. return_output ( bool , default: False ) \u2013 If True, the output will be returned instead of written to a file. Default value is False. Returns: ( ndarray , ndarray ) \u2013 covariance, standard deviation Source code in src/measureia/measure_jackknife.py def measure_covariance_multiple_datasets(self, corr_type, dataset_names, num_box=27, return_output=False): \"\"\"Combines the jackknife measurements for different datasets into one covariance matrix. Author: Marta Garcia Escobar (starting from measure_jackknife methods); updated Parameters ---------- corr_type : str Which type of correlation is measured. Takes 'w_g_plus', 'w_gg', 'multipoles_g_plus' or 'multipoles_gg'. dataset_names : list of str List of the dataset names. If there is only one value, it calculates the covariance matrix with itself. num_box : int, optional Number of jackknife realisations. Default value is 27. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. Returns ------- ndarray, ndarray covariance, standard deviation \"\"\" # check if corr_type is valid valid_corr_types = [\"w_g_plus\", \"multipoles_g_plus\", \"w_gg\", \"multipoles_gg\"] if corr_type not in valid_corr_types: raise ValueError(\"corr_type must be 'w_g_plus', 'w_gg', 'multipoles_g_plus' or 'multipoles_gg'.\") data_file = h5py.File(self.output_file_name, \"a\") mean_list = [] # list of arrays for dataset_name in dataset_names: group = data_file[f\"{self.snap_group}/{corr_type}/{dataset_name}_jk{num_box}\"] mean_multipoles = np.zeros(self.num_bins_r) for b in np.arange(0, num_box): mean_multipoles += group[dataset_name + \"_\" + str(b)] mean_multipoles /= num_box mean_list.append(mean_multipoles) # calculation the covariance matrix and the standard deviation (sqrt of diag of cov) cov = np.zeros((self.num_bins_r, self.num_bins_r)) std = np.zeros(self.num_bins_r) if len(dataset_names) == 1: # covariance with itself dataset_name = dataset_names[0] group = data_file[f\"{self.snap_group}/{corr_type}/{dataset_name}_jk{num_box}\"] for b in np.arange(0, num_box): std += (group[dataset_name + \"_\" + str(b)] - mean_list[0]) ** 2 for i in np.arange(self.num_bins_r): cov[:, i] += (group[dataset_name + \"_\" + str(b)] - mean_list[0]) * ( group[dataset_name + \"_\" + str(b)][i] - mean_list[0][i] ) elif len(dataset_names) == 2: group0 = data_file[f\"{self.snap_group}/{corr_type}/{dataset_names[0]}_jk{num_box}\"] group1 = data_file[f\"{self.snap_group}/{corr_type}/{dataset_names[1]}_jk{num_box}\"] for b in np.arange(0, num_box): std += (group0[dataset_names[0] + \"_\" + str(b)] - mean_list[0]) * ( group1[dataset_names[1] + \"_\" + str(b)] - mean_list[1]) for i in np.arange(self.num_bins_r): cov[:, i] += (group0[dataset_names[0] + \"_\" + str(b)] - mean_list[0]) * ( group1[dataset_names[1] + \"_\" + str(b)][i] - mean_list[1][i] ) else: raise KeyError(\"Too many datasets given, choose either 1 or 2\") std *= (num_box - 1) / num_box # see Singh 2023 std = np.sqrt(std) # size of errorbars cov *= (num_box - 1) / num_box # cov not sqrt so to get std, sqrt of diag would need to be taken data_file.close() if (self.output_file_name != None) and (return_output == False): output_file = h5py.File(self.output_file_name, \"a\") group = create_group_hdf5(output_file, f\"{self.snap_group}/{corr_type}\") if len(dataset_names) == 2: write_dataset_hdf5(group, dataset_names[0] + \"_\" + dataset_names[1] + \"_jackknife_cov_\" + str( num_box), data=cov) write_dataset_hdf5(group, dataset_names[0] + \"_\" + dataset_names[1] + \"_jackknife_\" + str(num_box), data=std) else: write_dataset_hdf5(group, dataset_names[0] + \"_jackknife_cov_\" + str(num_box), data=cov) write_dataset_hdf5(group, dataset_names[0] + \"_jackknife_\" + str(num_box), data=std) output_file.close() return else: return cov, std create_full_cov_matrix_projections(corr_type, dataset_names=['LOS_x', 'LOS_y', 'LOS_z'], num_box=27, return_output=False) Function that creates the full covariance matrix for all 3 projections and combined covariance for 2 projections by combining previously obtained jackknife information. Generalised from Marta Garcia Escobar's code. Parameters: corr_type ( str ) \u2013 Which type of correlation is measured. Takes 'w_g_plus', 'w_gg', 'multipoles_g_plus' or 'multipoles_gg'. num_box ( int , default: 27 ) \u2013 Number of jackknife realisations. Default value is 27. dataset_names ( list of str , default: ['LOS_x', 'LOS_y', 'LOS_z'] ) \u2013 Dataset names of projections to be combined. Default value is [\"LOS_x\",\"LOS_y\",\"LOS_z\"]. return_output ( bool , default: False ) \u2013 If True, the output will be returned instead of written to a file. Default value is False. Returns: ndarrays \u2013 covariance for 3 projections, covariance for x and y, covariance for x and z, covariance for y and z Source code in src/measureia/measure_jackknife.py def create_full_cov_matrix_projections(self, corr_type, dataset_names=[\"LOS_x\", \"LOS_y\", \"LOS_z\"], num_box=27, return_output=False): \"\"\"Function that creates the full covariance matrix for all 3 projections and combined covariance for 2 projections by combining previously obtained jackknife information. Generalised from Marta Garcia Escobar's code. Parameters ---------- corr_type : str Which type of correlation is measured. Takes 'w_g_plus', 'w_gg', 'multipoles_g_plus' or 'multipoles_gg'. num_box : int, optional Number of jackknife realisations. Default value is 27. dataset_names : list of str Dataset names of projections to be combined. Default value is [\"LOS_x\",\"LOS_y\",\"LOS_z\"]. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. Returns ------- ndarrays covariance for 3 projections, covariance for x and y, covariance for x and z, covariance for y and z \"\"\" self.measure_covariance_multiple_datasets(corr_type=corr_type, dataset_names=[dataset_names[0], dataset_names[1]], num_box=num_box) self.measure_covariance_multiple_datasets(corr_type=corr_type, dataset_names=[dataset_names[0], dataset_names[2]], num_box=num_box) self.measure_covariance_multiple_datasets(corr_type=corr_type, dataset_names=[dataset_names[1], dataset_names[2]], num_box=num_box) # import needed datasets output_file = h5py.File(self.output_file_name, \"a\") group = output_file[f\"{self.snap_group}/{corr_type}\"] # cov matrix between datasets cov_xx = group[f'{dataset_names[0]}_jackknife_cov_{num_box}'][:] cov_yy = group[f'{dataset_names[1]}_jackknife_cov_{num_box}'][:] cov_zz = group[f'{dataset_names[2]}_jackknife_cov_{num_box}'][:] cov_xy = group[f'{dataset_names[0]}_{dataset_names[1]}_jackknife_cov_{num_box}'][:] cov_yz = group[f'{dataset_names[0]}_{dataset_names[2]}_jackknife_cov_{num_box}'][:] cov_xz = group[f'{dataset_names[1]}_{dataset_names[2]}_jackknife_cov_{num_box}'][:] # 3 projections cov_top = np.concatenate((cov_xx, cov_xy, cov_xz), axis=1) cov_middle = np.concatenate((cov_xy.T, cov_yy, cov_yz), axis=1) # cov_xy.T = cov_yx cov_bottom = np.concatenate((cov_xz.T, cov_yz.T, cov_zz), axis=1) cov3 = np.concatenate((cov_top, cov_middle, cov_bottom), axis=0) # all 2 projections cov_top = np.concatenate((cov_xx, cov_xy), axis=1) cov_middle = np.concatenate((cov_xy.T, cov_yy), axis=1) # cov_xz.T = cov_zx cov2xy = np.concatenate((cov_top, cov_middle), axis=0) cov_top = np.concatenate((cov_xx, cov_xz), axis=1) cov_middle = np.concatenate((cov_xz.T, cov_zz), axis=1) # cov_xz.T = cov_zx cov2xz = np.concatenate((cov_top, cov_middle), axis=0) cov_top = np.concatenate((cov_yy, cov_yz), axis=1) cov_middle = np.concatenate((cov_yz.T, cov_zz), axis=1) # cov_xz.T = cov_zx cov2yz = np.concatenate((cov_top, cov_middle), axis=0) if return_output: return cov3, cov2xy, cov2xz, cov2yz else: write_dataset_hdf5(group, f\"{dataset_names[0]}_{dataset_names[1]}_{dataset_names[2]}_combined_jackknife_cov_{num_box}\", data=cov3) write_dataset_hdf5(group, f'{dataset_names[0]}_{dataset_names[1]}_combined_jackknife_cov_{num_box}', data=cov2xy) write_dataset_hdf5(group, f'{dataset_names[0]}_{dataset_names[2]}_combined_jackknife_cov_{num_box}', data=cov2xz) write_dataset_hdf5(group, f'{dataset_names[1]}_{dataset_names[2]}_combined_jackknife_cov_{num_box}', data=cov2yz) return","title":"MeasureJackknife"},{"location":"api/MeasureJackknife/#measurejackknife","text":"","title":"MeasureJackknife"},{"location":"api/MeasureJackknife/#measureia.MeasureJackknife","text":"Bases: MeasureWLightcone , MeasureMultipolesLightcone Class that contains all methods for jackknife covariance measurements for IA correlation functions. Methods: Name Description _measure_jackknife_realisations_obs Measures all jackknife realisations for MeasureIALightcone using 1 or more CPUs. _measure_jackknife_covariance_obs Combines jackknife realisations for MeasureIALightcone into covariance. _measure_jackknife_realisations_obs_multiprocessing Measures all jackknife realisations for MeasureIALightcone using >1 CPU. measure_covariance_multiple_datasets Given the jackknife realisations of two datasets, creates the cross covariance. create_full_cov_matrix_projections Creates larger covariance matrix of multiple datasets including cross terms. Notes Inherits attributes from 'SimInfo', where 'boxsize', 'L_0p5' and 'snap_group' are used in this class. Inherits attributes from 'MeasureIABase', where 'data', 'output_file_name', 'periodicity', 'Num_position', 'Num_shape', 'r_min', 'r_max', 'num_bins_r', 'num_bins_pi', 'r_bins', 'pi_bins', 'mu_r_bins' are used. Source code in src/measureia/measure_jackknife.py class MeasureJackknife(MeasureWLightcone, MeasureMultipolesLightcone): \"\"\"Class that contains all methods for jackknife covariance measurements for IA correlation functions. Methods ------- _measure_jackknife_realisations_obs() Measures all jackknife realisations for MeasureIALightcone using 1 or more CPUs. _measure_jackknife_covariance_obs() Combines jackknife realisations for MeasureIALightcone into covariance. _measure_jackknife_realisations_obs_multiprocessing() Measures all jackknife realisations for MeasureIALightcone using >1 CPU. measure_covariance_multiple_datasets() Given the jackknife realisations of two datasets, creates the cross covariance. create_full_cov_matrix_projections() Creates larger covariance matrix of multiple datasets including cross terms. Notes ----- Inherits attributes from 'SimInfo', where 'boxsize', 'L_0p5' and 'snap_group' are used in this class. Inherits attributes from 'MeasureIABase', where 'data', 'output_file_name', 'periodicity', 'Num_position', 'Num_shape', 'r_min', 'r_max', 'num_bins_r', 'num_bins_pi', 'r_bins', 'pi_bins', 'mu_r_bins' are used. \"\"\" def __init__( self, data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True, ): \"\"\" The __init__ method of the MeasureJackknife class. Notes ----- Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. \"\"\" super().__init__(data, output_file_name, simulation, snapshot, separation_limits, num_bins_r, num_bins_pi, pi_max, boxsize, periodicity) return def _measure_jackknife_realisations_lightcone( self, patches_pos, patches_shape, corr_type, dataset_name, masks=None, rp_cut=None, over_h=False, cosmology=None, count_pairs=False, data_suffix=\"\", num_sample_names=[\"S\", \"D\"] ): \"\"\"Measures the jackknife realisations for the projected correlation functions in MeasureIALightcone using the jackknife method. The area is already divided into patches; the correlation function is calculated omitting one patch at a time. This method uses 1 or more CPUs. Parameters ---------- patches_pos : ndarray Array with the patch numbers of each object in the position sample. patches_shape : ndarray Array with the patch numbers of each object in the shape sample. dataset_name : str Name of the dataset in the output file. corr_type : iterable with 2 str entries Array with two entries. For first choose from [gg, g+, both], for second from [w, multipoles]. masks : dict or NoneType, optional See MeasureIALightcone methods. Default is None. rp_cut : float or NoneType, optional See MeasureIALightcone.measure_xi_multipoles. Default is None. over_h : bool, optional See MeasureIALightcone. Default value is False. cosmology : pyccl cosmology object or NoneType, optional See MeasureIALightcone. Default is None. count_pairs : bool, optional If True, only gg is measured, not g+. Default value is False. data_suffix : str, optional Addition to dataset name. Used to distinguish between DR,DD and RR measurements. Default value is \"\". num_sample_names : list with two entries, optional Keywords of the num_samples dictionary to access number of objects in position ond shape samples. Default value is [\"S\", \"D\"]. Returns ------- \"\"\" if count_pairs and data_suffix == \"\": raise ValueError(\"Enter a data suffix (like _DD) for your pair count.\") figname_dataset_name = dataset_name if \"/\" in dataset_name: figname_dataset_name = figname_dataset_name.replace(\"/\", \"_\") if \".\" in dataset_name: figname_dataset_name = figname_dataset_name.replace(\".\", \"p\") min_patch, max_patch = int(min(patches_pos)), int(max(patches_pos)) num_patches = max_patch - min_patch + 1 if min(patches_shape) != min_patch: print( \"Warning! Minimum patch number of shape sample is not equal to minimum patch number of position sample.\") if max(patches_shape) != max_patch: print( \"Warning! Maximum patch number of shape sample is not equal to maximum patch number of position sample.\") print( f\"Calculating jackknife realisations for {num_patches} patches for {dataset_name}.\") for i in np.arange(min_patch, max_patch + 1): mask_position = (patches_pos != i) mask_shape = (patches_shape != i) if masks != None: mask_position = mask_position * masks[\"Redshift\"] mask_shape = mask_shape * masks[\"Redshift_shape_sample\"] self.num_samples[f\"{i}\"][num_sample_names[0]] = sum(mask_shape) self.num_samples[f\"{i}\"][num_sample_names[1]] = sum(mask_position) masks_total = { \"Redshift\": mask_position, \"Redshift_shape_sample\": mask_shape, \"RA\": mask_position, \"RA_shape_sample\": mask_shape, \"DEC\": mask_position, \"DEC_shape_sample\": mask_shape, \"e1\": mask_shape, \"e2\": mask_shape, \"weight\": mask_position, \"weight_shape_sample\": mask_shape, } if corr_type[1] == \"multipoles\": if count_pairs: self._count_pairs_xi_r_mur_lightcone_brute(masks=masks_total, dataset_name=dataset_name + \"_\" + str(i), over_h=over_h, cosmology=cosmology, print_num=False, data_suffix=data_suffix, rp_cut=rp_cut, jk_group_name=f\"{dataset_name}_jk{num_patches}\") else: self._measure_xi_r_mur_lightcone_brute( masks=masks_total, rp_cut=rp_cut, dataset_name=dataset_name + \"_\" + str(i), print_num=False, over_h=over_h, cosmology=cosmology, jk_group_name=f\"{dataset_name}_jk{num_patches}\", ) else: if count_pairs: self._count_pairs_xi_rp_pi_lightcone_brute(masks=masks_total, dataset_name=dataset_name + \"_\" + str(i), over_h=over_h, cosmology=cosmology, print_num=False, data_suffix=data_suffix, jk_group_name=f\"{dataset_name}_jk{num_patches}\") else: self._measure_xi_rp_pi_lightcone_brute( masks=masks_total, dataset_name=dataset_name + \"_\" + str(i), print_num=False, over_h=over_h, cosmology=cosmology, jk_group_name=f\"{dataset_name}_jk{num_patches}\", ) return def _measure_jackknife_covariance_lightcone( self, IA_estimator, corr_type, dataset_name, max_patch, min_patch=1, randoms_suf=\"_randoms\" ): \"\"\"Combines the jackknife realisations measured with _measure_jackknife_realisations_obs or _measure_jackknife_realisations_obs_multiprocessing into a covariance. Parameters ---------- IA_estimator : str Choose which type of xi estimator is used. Choose from \"clusters\" or \"galaxies\". dataset_name : str Name of the dataset in the output file. corr_type : iterable with 2 str entries Array with two entries. For first choose from [gg, g+, both], for second from [w, multipoles]. max_patch : int Maximum patch number used. min_patch : int, optional Minimum patch number used. Default value is 1. randoms_suf : str, optional Suffix used to denote the datasets that have been created using the randoms. Default value is \"_randoms\". Returns ------- \"\"\" if corr_type[0] == \"both\": data = [corr_type[1] + \"_g_plus\", corr_type[1] + \"_gg\"] elif corr_type[0] == \"g+\": data = [corr_type[1] + \"_g_plus\"] elif corr_type[0] == \"gg\": data = [corr_type[1] + \"_gg\"] else: raise KeyError(\"Unknown value for corr_type. Choose from [g+, gg, both]\") figname_dataset_name = dataset_name if \"/\" in dataset_name: figname_dataset_name = figname_dataset_name.replace(\"/\", \"_\") if \".\" in dataset_name: figname_dataset_name = figname_dataset_name.replace(\".\", \"p\") num_patches = max_patch - min_patch + 1 print( f\"Calculating jackknife errors for {num_patches} patches for {dataset_name} with {dataset_name}{randoms_suf} as randoms.\") covs, stds = [], [] for d in np.arange(0, len(data)): for b in np.arange(min_patch, max_patch + 1): self._obs_estimator(corr_type, IA_estimator, f\"{dataset_name}_{b}\", f\"{dataset_name}{randoms_suf}_{b}\", self.num_samples[f\"{b}\"], jk_group_name=f\"{dataset_name}_jk{num_patches}\", jk_group_name_randoms=f\"{dataset_name}{randoms_suf}_jk{num_patches}\") if \"w\" in data[d]: self._measure_w_g_i(corr_type=corr_type[0], dataset_name=f\"{dataset_name}_{b}\", jk_group_name=f\"{dataset_name}_jk{num_patches}\") else: self._measure_multipoles(corr_type=corr_type[0], dataset_name=f\"{dataset_name}_{b}\", jk_group_name=f\"{dataset_name}_jk{num_patches}\") data_file = h5py.File(self.output_file_name, \"a\") group_multipoles = data_file[f\"{self.snap_group}/{data[d]}/{dataset_name}_jk{num_patches}\"] # calculating mean of the datavectors mean_multipoles = np.zeros(self.num_bins_r) for b in np.arange(min_patch, max_patch + 1): mean_multipoles += group_multipoles[f\"{dataset_name}_{b}\"][:] mean_multipoles /= num_patches # calculation the covariance matrix (multipoles) and the standard deviation (sqrt of diag of cov) cov = np.zeros((self.num_bins_r, self.num_bins_r)) std = np.zeros(self.num_bins_r) for b in np.arange(min_patch, max_patch + 1): correlation = group_multipoles[f\"{dataset_name}_{b}\"][:] std += (correlation - mean_multipoles) ** 2 for i in np.arange(self.num_bins_r): cov[:, i] += (correlation - mean_multipoles) * (correlation[i] - mean_multipoles[i]) std *= (num_patches - 1) / num_patches # see Singh 2023 std = np.sqrt(std) # size of errorbars cov *= (num_patches - 1) / num_patches # cov not sqrt so to get std, sqrt of diag would need to be taken data_file.close() if self.output_file_name != None: output_file = h5py.File(self.output_file_name, \"a\") group_multipoles = create_group_hdf5(output_file, f\"{self.snap_group}/\" + data[d]) write_dataset_hdf5(group_multipoles, dataset_name + \"_mean_\" + str(num_patches), data=mean_multipoles) write_dataset_hdf5(group_multipoles, dataset_name + \"_jackknife_\" + str(num_patches), data=std) write_dataset_hdf5(group_multipoles, dataset_name + \"_jackknife_cov_\" + str(num_patches), data=cov) output_file.close() else: covs.append(cov) stds.append(std) if self.output_file_name != None: return else: return covs, stds def _measure_jackknife_realisations_lightcone_multiprocessing( self, patches_pos, patches_shape, corr_type, dataset_name, masks=None, rp_cut=None, over_h=False, num_nodes=4, cosmology=None, count_pairs=False, data_suffix=\"\", num_sample_names=[\"S\", \"D\"] ): \"\"\"Measures the jackknife realisations for the projected correlation functions in MeasureIALightcone using the jackknife method. The area is already divided into patches; the correlation function is calculated omitting one patch at a time. This method uses >1 CPUs. Parameters ---------- patches_pos : ndarray Array with the patch numbers of each object in the position sample. patches_shape : ndarray Array with the patch numbers of each object in the shape sample. dataset_name : str Name of the dataset in the output file. corr_type : iterable with 2 str entries Array with two entries. For first choose from [gg, g+, both], for second from [w, multipoles]. masks : dict or NoneType, optional See MeasureIALightcone methods. Default is None. rp_cut : float or NoneType, optional See MeasureIALightcone.measure_xi_multipoles. Default is None. over_h : bool, optional See MeasureIALightcone. Default value is False. num_nodes : int, optional Number of cores to be used in multiprocessing. Default is 4. cosmology : pyccl cosmology object or NoneType, optional See MeasureIALightcone. Default is None. count_pairs : bool, optional If True, only gg is measured, not g+. Default value is False. data_suffix : str, optional Addition to dataset name. Used to distinguish between DR,DD and RR measurements. Default value is \"\". num_sample_names : list with two entries, optional Keywords of the num_samples dictionary to access number of objects in position ond shape samples. Default value is [\"S\", \"D\"]. Returns ------- \"\"\" if num_nodes == 1: self._measure_jackknife_realisations_lightcone(patches_pos, patches_shape, masks, corr_type, dataset_name, rp_cut, over_h, cosmology, count_pairs, data_suffix, num_sample_names) return if count_pairs == False: corr_type[0] = \"both\" if corr_type[0] == \"both\": data = [corr_type[1] + \"_g_plus\", corr_type[1] + \"_gg\"] corr_type_suff = [\"_g_plus\", \"_gg\"] xi_suff = [\"SplusD\", \"DD\"] elif corr_type[0] == \"g+\": data = [corr_type[1] + \"_g_plus\"] corr_type_suff = [\"_g_plus\"] xi_suff = [\"SplusD\"] elif corr_type[0] == \"gg\": data = [corr_type[1] + \"_gg\"] corr_type_suff = [\"_gg\"] xi_suff = [\"DD\"] else: raise KeyError(\"Unknown value for first entry of corr_type. Choose from [g+, gg, both]\") if corr_type[1] == \"multipoles\": bin_var_names = [\"r\", \"mu_r\"] elif corr_type[1] == \"w\": bin_var_names = [\"rp\", \"pi\"] else: raise KeyError(\"Unknown value for second entry of corr_type. Choose from [multipoles, w_g_plus]\") min_patch, max_patch = int(min(patches_pos)), int(max(patches_pos)) num_patches = max_patch - min_patch + 1 if min(patches_shape) != min_patch: print( \"Warning! Minimum patch number of shape sample is not equal to minimum patch number of position sample.\") if max(patches_shape) != max_patch: print( \"Warning! Maximum patch number of shape sample is not equal to maximum patch number of position sample.\") args_xi_g_plus, args_multipoles, tree_args = [], [], [] for i in np.arange(min_patch, max_patch + 1): mask_position = (patches_pos != i) mask_shape = (patches_shape != i) if masks != None: mask_position = mask_position * masks[\"Redshift\"] mask_shape = mask_shape * masks[\"Redshift_shape_sample\"] self.num_samples[f\"{i}\"][num_sample_names[0]] = sum(mask_shape) self.num_samples[f\"{i}\"][num_sample_names[1]] = sum(mask_position) masks_total = { \"Redshift\": mask_position, \"Redshift_shape_sample\": mask_shape, \"RA\": mask_position, \"RA_shape_sample\": mask_shape, \"DEC\": mask_position, \"DEC_shape_sample\": mask_shape, \"e1\": mask_shape, \"e2\": mask_shape, \"weight\": mask_position, \"weight_shape_sample\": mask_shape, } if corr_type[1] == \"multipoles\": args_xi_g_plus.append( ( dataset_name + \"_\" + str(i), masks_total, True, False, over_h, cosmology, rp_cut, data_suffix ) ) else: args_xi_g_plus.append( ( dataset_name + \"_\" + str(i), masks_total, True, False, over_h, cosmology, data_suffix ) ) args_xi_g_plus = np.array(args_xi_g_plus) multiproc_chuncks = np.array_split(np.arange(num_patches), np.ceil(num_patches / num_nodes)) for chunck in multiproc_chuncks: chunck = np.array(chunck, dtype=int) if corr_type[1] == \"multipoles\": if count_pairs: result = ProcessingPool(nodes=len(chunck)).map( self._count_pairs_xi_r_mur_lightcone_brute, args_xi_g_plus[chunck][:, 0], args_xi_g_plus[chunck][:, 1], args_xi_g_plus[chunck][:, 2], args_xi_g_plus[chunck][:, 3], args_xi_g_plus[chunck][:, 4], args_xi_g_plus[chunck][:, 5], args_xi_g_plus[chunck][:, 6], args_xi_g_plus[chunck][:, 7], ) else: result = ProcessingPool(nodes=len(chunck)).map( self._measure_xi_r_mur_lightcone_brute, args_xi_g_plus[chunck][:, 0], args_xi_g_plus[chunck][:, 1], args_xi_g_plus[chunck][:, 2], args_xi_g_plus[chunck][:, 3], args_xi_g_plus[chunck][:, 4], args_xi_g_plus[chunck][:, 5], args_xi_g_plus[chunck][:, 6], ) else: if count_pairs: result = ProcessingPool(nodes=len(chunck)).map( self._count_pairs_xi_rp_pi_lightcone_brute, args_xi_g_plus[chunck][:, 0], args_xi_g_plus[chunck][:, 1], args_xi_g_plus[chunck][:, 2], args_xi_g_plus[chunck][:, 3], args_xi_g_plus[chunck][:, 4], args_xi_g_plus[chunck][:, 5], args_xi_g_plus[chunck][:, 6], ) else: result = ProcessingPool(nodes=len(chunck)).map( self._measure_xi_rp_pi_lightcone_brute, args_xi_g_plus[chunck][:, 0], args_xi_g_plus[chunck][:, 1], args_xi_g_plus[chunck][:, 2], args_xi_g_plus[chunck][:, 3], args_xi_g_plus[chunck][:, 4], args_xi_g_plus[chunck][:, 5], ) output_file = h5py.File(self.output_file_name, \"a\") if count_pairs: for i in np.arange(0, len(chunck)): for j, data_j in enumerate(data): group_xigplus = create_group_hdf5( output_file, f\"{self.snap_group}/{corr_type[1]}/xi_gg/{dataset_name}_jk{num_patches}\" ) write_dataset_hdf5(group_xigplus, f\"{dataset_name}_{chunck[i] + min_patch}{data_suffix}\", data=result[i][j]) write_dataset_hdf5( group_xigplus, f\"{dataset_name}_{chunck[i] + min_patch}_{bin_var_names[0]}\", data=result[i][1] ) write_dataset_hdf5( group_xigplus, f\"{dataset_name}_{chunck[i] + min_patch}_{bin_var_names[1]}\", data=result[i][2] ) else: for i in np.arange(0, len(chunck)): for j, data_j in enumerate(data): group_xigplus = create_group_hdf5( output_file, f\"{self.snap_group}/{corr_type[1]}/xi{corr_type_suff[j]}/{dataset_name}_jk{num_patches}\" ) write_dataset_hdf5(group_xigplus, f\"{dataset_name}_{chunck[i] + min_patch}_{xi_suff[j]}\", data=result[i][j]) write_dataset_hdf5( group_xigplus, f\"{dataset_name}_{chunck[i] + min_patch}_{bin_var_names[0]}\", data=result[i][2] ) write_dataset_hdf5( group_xigplus, f\"{dataset_name}_{chunck[i] + min_patch}_{bin_var_names[1]}\", data=result[i][3] ) # write_dataset_hdf5(group_xigplus, f\"{dataset_name}_{chunck[i]}_sigmasq\", data=result[i][3]) output_file.close() # for i in np.arange(0, num_patches): # if corr_type[1] == \"multipoles\": # self.measure_multipoles(corr_type=args_multipoles[i, 0], dataset_name=args_multipoles[i, 1]) # else: # self.measure_w_g_i(corr_type=args_multipoles[i, 0], dataset_name=args_multipoles[i, 1]) return def measure_covariance_multiple_datasets(self, corr_type, dataset_names, num_box=27, return_output=False): \"\"\"Combines the jackknife measurements for different datasets into one covariance matrix. Author: Marta Garcia Escobar (starting from measure_jackknife methods); updated Parameters ---------- corr_type : str Which type of correlation is measured. Takes 'w_g_plus', 'w_gg', 'multipoles_g_plus' or 'multipoles_gg'. dataset_names : list of str List of the dataset names. If there is only one value, it calculates the covariance matrix with itself. num_box : int, optional Number of jackknife realisations. Default value is 27. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. Returns ------- ndarray, ndarray covariance, standard deviation \"\"\" # check if corr_type is valid valid_corr_types = [\"w_g_plus\", \"multipoles_g_plus\", \"w_gg\", \"multipoles_gg\"] if corr_type not in valid_corr_types: raise ValueError(\"corr_type must be 'w_g_plus', 'w_gg', 'multipoles_g_plus' or 'multipoles_gg'.\") data_file = h5py.File(self.output_file_name, \"a\") mean_list = [] # list of arrays for dataset_name in dataset_names: group = data_file[f\"{self.snap_group}/{corr_type}/{dataset_name}_jk{num_box}\"] mean_multipoles = np.zeros(self.num_bins_r) for b in np.arange(0, num_box): mean_multipoles += group[dataset_name + \"_\" + str(b)] mean_multipoles /= num_box mean_list.append(mean_multipoles) # calculation the covariance matrix and the standard deviation (sqrt of diag of cov) cov = np.zeros((self.num_bins_r, self.num_bins_r)) std = np.zeros(self.num_bins_r) if len(dataset_names) == 1: # covariance with itself dataset_name = dataset_names[0] group = data_file[f\"{self.snap_group}/{corr_type}/{dataset_name}_jk{num_box}\"] for b in np.arange(0, num_box): std += (group[dataset_name + \"_\" + str(b)] - mean_list[0]) ** 2 for i in np.arange(self.num_bins_r): cov[:, i] += (group[dataset_name + \"_\" + str(b)] - mean_list[0]) * ( group[dataset_name + \"_\" + str(b)][i] - mean_list[0][i] ) elif len(dataset_names) == 2: group0 = data_file[f\"{self.snap_group}/{corr_type}/{dataset_names[0]}_jk{num_box}\"] group1 = data_file[f\"{self.snap_group}/{corr_type}/{dataset_names[1]}_jk{num_box}\"] for b in np.arange(0, num_box): std += (group0[dataset_names[0] + \"_\" + str(b)] - mean_list[0]) * ( group1[dataset_names[1] + \"_\" + str(b)] - mean_list[1]) for i in np.arange(self.num_bins_r): cov[:, i] += (group0[dataset_names[0] + \"_\" + str(b)] - mean_list[0]) * ( group1[dataset_names[1] + \"_\" + str(b)][i] - mean_list[1][i] ) else: raise KeyError(\"Too many datasets given, choose either 1 or 2\") std *= (num_box - 1) / num_box # see Singh 2023 std = np.sqrt(std) # size of errorbars cov *= (num_box - 1) / num_box # cov not sqrt so to get std, sqrt of diag would need to be taken data_file.close() if (self.output_file_name != None) and (return_output == False): output_file = h5py.File(self.output_file_name, \"a\") group = create_group_hdf5(output_file, f\"{self.snap_group}/{corr_type}\") if len(dataset_names) == 2: write_dataset_hdf5(group, dataset_names[0] + \"_\" + dataset_names[1] + \"_jackknife_cov_\" + str( num_box), data=cov) write_dataset_hdf5(group, dataset_names[0] + \"_\" + dataset_names[1] + \"_jackknife_\" + str(num_box), data=std) else: write_dataset_hdf5(group, dataset_names[0] + \"_jackknife_cov_\" + str(num_box), data=cov) write_dataset_hdf5(group, dataset_names[0] + \"_jackknife_\" + str(num_box), data=std) output_file.close() return else: return cov, std def create_full_cov_matrix_projections(self, corr_type, dataset_names=[\"LOS_x\", \"LOS_y\", \"LOS_z\"], num_box=27, return_output=False): \"\"\"Function that creates the full covariance matrix for all 3 projections and combined covariance for 2 projections by combining previously obtained jackknife information. Generalised from Marta Garcia Escobar's code. Parameters ---------- corr_type : str Which type of correlation is measured. Takes 'w_g_plus', 'w_gg', 'multipoles_g_plus' or 'multipoles_gg'. num_box : int, optional Number of jackknife realisations. Default value is 27. dataset_names : list of str Dataset names of projections to be combined. Default value is [\"LOS_x\",\"LOS_y\",\"LOS_z\"]. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. Returns ------- ndarrays covariance for 3 projections, covariance for x and y, covariance for x and z, covariance for y and z \"\"\" self.measure_covariance_multiple_datasets(corr_type=corr_type, dataset_names=[dataset_names[0], dataset_names[1]], num_box=num_box) self.measure_covariance_multiple_datasets(corr_type=corr_type, dataset_names=[dataset_names[0], dataset_names[2]], num_box=num_box) self.measure_covariance_multiple_datasets(corr_type=corr_type, dataset_names=[dataset_names[1], dataset_names[2]], num_box=num_box) # import needed datasets output_file = h5py.File(self.output_file_name, \"a\") group = output_file[f\"{self.snap_group}/{corr_type}\"] # cov matrix between datasets cov_xx = group[f'{dataset_names[0]}_jackknife_cov_{num_box}'][:] cov_yy = group[f'{dataset_names[1]}_jackknife_cov_{num_box}'][:] cov_zz = group[f'{dataset_names[2]}_jackknife_cov_{num_box}'][:] cov_xy = group[f'{dataset_names[0]}_{dataset_names[1]}_jackknife_cov_{num_box}'][:] cov_yz = group[f'{dataset_names[0]}_{dataset_names[2]}_jackknife_cov_{num_box}'][:] cov_xz = group[f'{dataset_names[1]}_{dataset_names[2]}_jackknife_cov_{num_box}'][:] # 3 projections cov_top = np.concatenate((cov_xx, cov_xy, cov_xz), axis=1) cov_middle = np.concatenate((cov_xy.T, cov_yy, cov_yz), axis=1) # cov_xy.T = cov_yx cov_bottom = np.concatenate((cov_xz.T, cov_yz.T, cov_zz), axis=1) cov3 = np.concatenate((cov_top, cov_middle, cov_bottom), axis=0) # all 2 projections cov_top = np.concatenate((cov_xx, cov_xy), axis=1) cov_middle = np.concatenate((cov_xy.T, cov_yy), axis=1) # cov_xz.T = cov_zx cov2xy = np.concatenate((cov_top, cov_middle), axis=0) cov_top = np.concatenate((cov_xx, cov_xz), axis=1) cov_middle = np.concatenate((cov_xz.T, cov_zz), axis=1) # cov_xz.T = cov_zx cov2xz = np.concatenate((cov_top, cov_middle), axis=0) cov_top = np.concatenate((cov_yy, cov_yz), axis=1) cov_middle = np.concatenate((cov_yz.T, cov_zz), axis=1) # cov_xz.T = cov_zx cov2yz = np.concatenate((cov_top, cov_middle), axis=0) if return_output: return cov3, cov2xy, cov2xz, cov2yz else: write_dataset_hdf5(group, f\"{dataset_names[0]}_{dataset_names[1]}_{dataset_names[2]}_combined_jackknife_cov_{num_box}\", data=cov3) write_dataset_hdf5(group, f'{dataset_names[0]}_{dataset_names[1]}_combined_jackknife_cov_{num_box}', data=cov2xy) write_dataset_hdf5(group, f'{dataset_names[0]}_{dataset_names[2]}_combined_jackknife_cov_{num_box}', data=cov2xz) write_dataset_hdf5(group, f'{dataset_names[1]}_{dataset_names[2]}_combined_jackknife_cov_{num_box}', data=cov2yz) return","title":"MeasureJackknife"},{"location":"api/MeasureJackknife/#measureia.MeasureJackknife.__init__","text":"The init method of the MeasureJackknife class. Notes Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. Source code in src/measureia/measure_jackknife.py def __init__( self, data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True, ): \"\"\" The __init__ method of the MeasureJackknife class. Notes ----- Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. \"\"\" super().__init__(data, output_file_name, simulation, snapshot, separation_limits, num_bins_r, num_bins_pi, pi_max, boxsize, periodicity) return","title":"__init__"},{"location":"api/MeasureJackknife/#measureia.MeasureJackknife.measure_covariance_multiple_datasets","text":"Combines the jackknife measurements for different datasets into one covariance matrix. Author: Marta Garcia Escobar (starting from measure_jackknife methods); updated Parameters: corr_type ( str ) \u2013 Which type of correlation is measured. Takes 'w_g_plus', 'w_gg', 'multipoles_g_plus' or 'multipoles_gg'. dataset_names ( list of str ) \u2013 List of the dataset names. If there is only one value, it calculates the covariance matrix with itself. num_box ( int , default: 27 ) \u2013 Number of jackknife realisations. Default value is 27. return_output ( bool , default: False ) \u2013 If True, the output will be returned instead of written to a file. Default value is False. Returns: ( ndarray , ndarray ) \u2013 covariance, standard deviation Source code in src/measureia/measure_jackknife.py def measure_covariance_multiple_datasets(self, corr_type, dataset_names, num_box=27, return_output=False): \"\"\"Combines the jackknife measurements for different datasets into one covariance matrix. Author: Marta Garcia Escobar (starting from measure_jackknife methods); updated Parameters ---------- corr_type : str Which type of correlation is measured. Takes 'w_g_plus', 'w_gg', 'multipoles_g_plus' or 'multipoles_gg'. dataset_names : list of str List of the dataset names. If there is only one value, it calculates the covariance matrix with itself. num_box : int, optional Number of jackknife realisations. Default value is 27. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. Returns ------- ndarray, ndarray covariance, standard deviation \"\"\" # check if corr_type is valid valid_corr_types = [\"w_g_plus\", \"multipoles_g_plus\", \"w_gg\", \"multipoles_gg\"] if corr_type not in valid_corr_types: raise ValueError(\"corr_type must be 'w_g_plus', 'w_gg', 'multipoles_g_plus' or 'multipoles_gg'.\") data_file = h5py.File(self.output_file_name, \"a\") mean_list = [] # list of arrays for dataset_name in dataset_names: group = data_file[f\"{self.snap_group}/{corr_type}/{dataset_name}_jk{num_box}\"] mean_multipoles = np.zeros(self.num_bins_r) for b in np.arange(0, num_box): mean_multipoles += group[dataset_name + \"_\" + str(b)] mean_multipoles /= num_box mean_list.append(mean_multipoles) # calculation the covariance matrix and the standard deviation (sqrt of diag of cov) cov = np.zeros((self.num_bins_r, self.num_bins_r)) std = np.zeros(self.num_bins_r) if len(dataset_names) == 1: # covariance with itself dataset_name = dataset_names[0] group = data_file[f\"{self.snap_group}/{corr_type}/{dataset_name}_jk{num_box}\"] for b in np.arange(0, num_box): std += (group[dataset_name + \"_\" + str(b)] - mean_list[0]) ** 2 for i in np.arange(self.num_bins_r): cov[:, i] += (group[dataset_name + \"_\" + str(b)] - mean_list[0]) * ( group[dataset_name + \"_\" + str(b)][i] - mean_list[0][i] ) elif len(dataset_names) == 2: group0 = data_file[f\"{self.snap_group}/{corr_type}/{dataset_names[0]}_jk{num_box}\"] group1 = data_file[f\"{self.snap_group}/{corr_type}/{dataset_names[1]}_jk{num_box}\"] for b in np.arange(0, num_box): std += (group0[dataset_names[0] + \"_\" + str(b)] - mean_list[0]) * ( group1[dataset_names[1] + \"_\" + str(b)] - mean_list[1]) for i in np.arange(self.num_bins_r): cov[:, i] += (group0[dataset_names[0] + \"_\" + str(b)] - mean_list[0]) * ( group1[dataset_names[1] + \"_\" + str(b)][i] - mean_list[1][i] ) else: raise KeyError(\"Too many datasets given, choose either 1 or 2\") std *= (num_box - 1) / num_box # see Singh 2023 std = np.sqrt(std) # size of errorbars cov *= (num_box - 1) / num_box # cov not sqrt so to get std, sqrt of diag would need to be taken data_file.close() if (self.output_file_name != None) and (return_output == False): output_file = h5py.File(self.output_file_name, \"a\") group = create_group_hdf5(output_file, f\"{self.snap_group}/{corr_type}\") if len(dataset_names) == 2: write_dataset_hdf5(group, dataset_names[0] + \"_\" + dataset_names[1] + \"_jackknife_cov_\" + str( num_box), data=cov) write_dataset_hdf5(group, dataset_names[0] + \"_\" + dataset_names[1] + \"_jackknife_\" + str(num_box), data=std) else: write_dataset_hdf5(group, dataset_names[0] + \"_jackknife_cov_\" + str(num_box), data=cov) write_dataset_hdf5(group, dataset_names[0] + \"_jackknife_\" + str(num_box), data=std) output_file.close() return else: return cov, std","title":"measure_covariance_multiple_datasets"},{"location":"api/MeasureJackknife/#measureia.MeasureJackknife.create_full_cov_matrix_projections","text":"Function that creates the full covariance matrix for all 3 projections and combined covariance for 2 projections by combining previously obtained jackknife information. Generalised from Marta Garcia Escobar's code. Parameters: corr_type ( str ) \u2013 Which type of correlation is measured. Takes 'w_g_plus', 'w_gg', 'multipoles_g_plus' or 'multipoles_gg'. num_box ( int , default: 27 ) \u2013 Number of jackknife realisations. Default value is 27. dataset_names ( list of str , default: ['LOS_x', 'LOS_y', 'LOS_z'] ) \u2013 Dataset names of projections to be combined. Default value is [\"LOS_x\",\"LOS_y\",\"LOS_z\"]. return_output ( bool , default: False ) \u2013 If True, the output will be returned instead of written to a file. Default value is False. Returns: ndarrays \u2013 covariance for 3 projections, covariance for x and y, covariance for x and z, covariance for y and z Source code in src/measureia/measure_jackknife.py def create_full_cov_matrix_projections(self, corr_type, dataset_names=[\"LOS_x\", \"LOS_y\", \"LOS_z\"], num_box=27, return_output=False): \"\"\"Function that creates the full covariance matrix for all 3 projections and combined covariance for 2 projections by combining previously obtained jackknife information. Generalised from Marta Garcia Escobar's code. Parameters ---------- corr_type : str Which type of correlation is measured. Takes 'w_g_plus', 'w_gg', 'multipoles_g_plus' or 'multipoles_gg'. num_box : int, optional Number of jackknife realisations. Default value is 27. dataset_names : list of str Dataset names of projections to be combined. Default value is [\"LOS_x\",\"LOS_y\",\"LOS_z\"]. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. Returns ------- ndarrays covariance for 3 projections, covariance for x and y, covariance for x and z, covariance for y and z \"\"\" self.measure_covariance_multiple_datasets(corr_type=corr_type, dataset_names=[dataset_names[0], dataset_names[1]], num_box=num_box) self.measure_covariance_multiple_datasets(corr_type=corr_type, dataset_names=[dataset_names[0], dataset_names[2]], num_box=num_box) self.measure_covariance_multiple_datasets(corr_type=corr_type, dataset_names=[dataset_names[1], dataset_names[2]], num_box=num_box) # import needed datasets output_file = h5py.File(self.output_file_name, \"a\") group = output_file[f\"{self.snap_group}/{corr_type}\"] # cov matrix between datasets cov_xx = group[f'{dataset_names[0]}_jackknife_cov_{num_box}'][:] cov_yy = group[f'{dataset_names[1]}_jackknife_cov_{num_box}'][:] cov_zz = group[f'{dataset_names[2]}_jackknife_cov_{num_box}'][:] cov_xy = group[f'{dataset_names[0]}_{dataset_names[1]}_jackknife_cov_{num_box}'][:] cov_yz = group[f'{dataset_names[0]}_{dataset_names[2]}_jackknife_cov_{num_box}'][:] cov_xz = group[f'{dataset_names[1]}_{dataset_names[2]}_jackknife_cov_{num_box}'][:] # 3 projections cov_top = np.concatenate((cov_xx, cov_xy, cov_xz), axis=1) cov_middle = np.concatenate((cov_xy.T, cov_yy, cov_yz), axis=1) # cov_xy.T = cov_yx cov_bottom = np.concatenate((cov_xz.T, cov_yz.T, cov_zz), axis=1) cov3 = np.concatenate((cov_top, cov_middle, cov_bottom), axis=0) # all 2 projections cov_top = np.concatenate((cov_xx, cov_xy), axis=1) cov_middle = np.concatenate((cov_xy.T, cov_yy), axis=1) # cov_xz.T = cov_zx cov2xy = np.concatenate((cov_top, cov_middle), axis=0) cov_top = np.concatenate((cov_xx, cov_xz), axis=1) cov_middle = np.concatenate((cov_xz.T, cov_zz), axis=1) # cov_xz.T = cov_zx cov2xz = np.concatenate((cov_top, cov_middle), axis=0) cov_top = np.concatenate((cov_yy, cov_yz), axis=1) cov_middle = np.concatenate((cov_yz.T, cov_zz), axis=1) # cov_xz.T = cov_zx cov2yz = np.concatenate((cov_top, cov_middle), axis=0) if return_output: return cov3, cov2xy, cov2xz, cov2yz else: write_dataset_hdf5(group, f\"{dataset_names[0]}_{dataset_names[1]}_{dataset_names[2]}_combined_jackknife_cov_{num_box}\", data=cov3) write_dataset_hdf5(group, f'{dataset_names[0]}_{dataset_names[1]}_combined_jackknife_cov_{num_box}', data=cov2xy) write_dataset_hdf5(group, f'{dataset_names[0]}_{dataset_names[2]}_combined_jackknife_cov_{num_box}', data=cov2xz) write_dataset_hdf5(group, f'{dataset_names[1]}_{dataset_names[2]}_combined_jackknife_cov_{num_box}', data=cov2yz) return","title":"create_full_cov_matrix_projections"},{"location":"api/MeasureMBox/","text":"MeasureMultipolesBox measureia.MeasureMultipolesBox Bases: MeasureIABase , ReadData Class that contains all methods for the measurements of \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) for \\(\\tilde{\\xi}_{gg,0}\\) and \\(\\tilde{\\xi}_{g+,2}\\) with Cartesian simulation data. Methods: Name Description _measure_xi_r_mur_box_brute Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (r, mu_r) grid binning in a periodic box using 1 CPU. _measure_xi_r_mur_box_tree Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (r, mu_r) grid binning in a periodic box using 1 CPU and KDTree for extra speed. _measure_xi_r_mur_box_batch Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (r, mu_r) grid binning in a periodic box using 1 CPU for a batch of indices. Support function of _measure_xi_r_mur_box_multiprocessing(). _measure_xi_r_mur_box_multiprocessing Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (r, mu_r) grid binning in a periodic box using >1 CPUs. _measure_xi_r_pi_box_brute Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (r, pi) grid binning in a periodic box using 1 CPU. Notes Inherits attributes from 'SimInfo', where 'boxsize', 'L_0p5' and 'snap_group' are used in this class. Inherits attributes from 'MeasureIABase', where 'data', 'output_file_name', 'periodicity', 'Num_position', 'Num_shape', 'r_min', 'r_max', 'num_bins_r', 'num_bins_pi', 'r_bins', 'pi_bins', 'mu_r_bins' are used. Source code in src/measureia/measure_m_box.py class MeasureMultipolesBox(MeasureIABase, ReadData): r\"\"\"Class that contains all methods for the measurements of $\\xi_{gg}$ and $\\xi_{g+}$ for $\\tilde{\\xi}_{gg,0}$ and $\\tilde{\\xi}_{g+,2}$ with Cartesian simulation data. Methods ------- _measure_xi_r_mur_box_brute() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (r, mu_r) grid binning in a periodic box using 1 CPU. _measure_xi_r_mur_box_tree() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (r, mu_r) grid binning in a periodic box using 1 CPU and KDTree for extra speed. _measure_xi_r_mur_box_batch() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (r, mu_r) grid binning in a periodic box using 1 CPU for a batch of indices. Support function of _measure_xi_r_mur_box_multiprocessing(). _measure_xi_r_mur_box_multiprocessing() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (r, mu_r) grid binning in a periodic box using >1 CPUs. _measure_xi_r_pi_box_brute() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (r, pi) grid binning in a periodic box using 1 CPU. Notes ----- Inherits attributes from 'SimInfo', where 'boxsize', 'L_0p5' and 'snap_group' are used in this class. Inherits attributes from 'MeasureIABase', where 'data', 'output_file_name', 'periodicity', 'Num_position', 'Num_shape', 'r_min', 'r_max', 'num_bins_r', 'num_bins_pi', 'r_bins', 'pi_bins', 'mu_r_bins' are used. \"\"\" def __init__( self, data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True, ): \"\"\" The __init__ method of the MeasureMultipolesSimulations class. Notes ----- Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. \"\"\" super().__init__(data, output_file_name, simulation, snapshot, separation_limits, num_bins_r, num_bins_pi, pi_max, boxsize, periodicity) return def _measure_xi_r_pi_box_brute(self, dataset_name, masks=None, rp_cut=None, return_output=False, jk_group_name=\"\", ellipticity='distortion'): r\"\"\"Measures the projected correlation functions, $\\xi_{gg}$ and $\\xi_{g+}$, in (r, pi) bins for an object created with MeasureIABox. Uses 1 CPU. For each r bin, the pin bins are spaced between -r_max and r_max. Parameters ---------- dataset_name : str Name of the dataset in the output file. masks : dict or NoneType, optional Dictionary with masks for the data to select only part of the data. Uses same keywords as data dictionary. Default value is None. rp_cut : float, optional Limit for minimum r_p value for pairs to be included. Default value is None. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. jk_group_name : str, optional Group in output file (hdf5) where jackknife realisations are stored. Default value is \"\". ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. Returns ------- ndarrays $\\xi_{gg}$ and $\\xi_{g+}$, r bins, mu_r bins, S+D, DD, RR (if no output file is specified) \"\"\" if masks == None: positions = self.data[\"Position\"] positions_shape_sample = self.data[\"Position_shape_sample\"] axis_direction_v = self.data[\"Axis_Direction\"] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"] weight = self.data[\"weight\"] weight_shape = self.data[\"weight_shape_sample\"] else: positions = self.data[\"Position\"][masks[\"Position\"]] positions_shape_sample = self.data[\"Position_shape_sample\"][masks[\"Position_shape_sample\"]] axis_direction_v = self.data[\"Axis_Direction\"][masks[\"Axis_Direction\"]] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"][masks[\"q\"]] try: weight_mask = masks[\"weight\"] except: masks[\"weight\"] = np.ones(self.Num_position, dtype=bool) masks[\"weight\"][sum(masks[\"Position\"]):self.Num_position] = 0 try: weight_mask = masks[\"weight_shape_sample\"] except: masks[\"weight_shape_sample\"] = np.ones(self.Num_shape, dtype=bool) masks[\"weight_shape_sample\"][sum(masks[\"Position_shape_sample\"]):self.Num_shape] = 0 weight = self.data[\"weight\"][masks[\"weight\"]] weight_shape = self.data[\"weight_shape_sample\"][masks[\"weight_shape_sample\"]] Num_position = len(positions) Num_shape = len(positions_shape_sample) print( f\"There are {Num_shape} galaxies in the shape sample and {Num_position} galaxies in the position sample.\") if rp_cut == None: rp_cut = 0.0 LOS_ind = self.data[\"LOS\"] # eg 2 for z axis not_LOS = np.array([0, 1, 2])[np.isin([0, 1, 2], LOS_ind, invert=True)] # eg 0,1 for x&y if ellipticity == 'distortion': e = (1 - q ** 2) / (1 + q ** 2) # size of ellipticity elif ellipticity == 'ellipticity': e = (1 - q) / (1 + q) else: raise ValueError(\"Invalid value for ellipticity. Choose 'distortion' or 'ellipticity'.\") del q R = sum(weight_shape * (1 - e ** 2 / 2.0)) / sum(weight_shape) # R = 1 - np.mean(e ** 2) / 2.0 # responsivity factor L3 = self.boxsize ** 3 # box volume sub_box_len_logr = (np.log10(self.r_max) - np.log10(self.r_min)) / self.num_bins_r pi_bins = np.zeros((self.num_bins_r + 1, self.num_bins_pi + 1)) for i in np.arange(self.num_bins_r + 1): pi_bins[i] = np.linspace(-self.r_bins[i], self.r_bins[i], self.num_bins_pi + 1) sub_box_len_pi = (pi_bins[:, -1] - pi_bins[:, 0]) / self.num_bins_pi DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_g_plus = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_gg = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) for n in np.arange(0, len(positions)): # for Splus_D (calculate ellipticities around position sample) separation = positions_shape_sample - positions[n] if self.periodicity: separation[separation > self.L_0p5] -= self.boxsize # account for periodicity of box separation[separation < -self.L_0p5] += self.boxsize projected_sep = separation[:, not_LOS] LOS = separation[:, LOS_ind] projected_separation_len = np.sqrt(np.sum(projected_sep ** 2, axis=1)) with np.errstate(invalid='ignore'): separation_dir = ( projected_sep.transpose() / projected_separation_len).transpose() # normalisation of rp separation_len = np.sqrt(np.sum(separation ** 2, axis=1)) with np.errstate(invalid='ignore'): # mu_r = LOS / separation_len del projected_sep, separation phi = np.arccos(self.calculate_dot_product_arrays(separation_dir, axis_direction)) # [0,pi] e_plus, e_cross = self.get_ellipticity(e, phi) del phi, separation_dir e_plus[np.isnan(e_plus)] = 0.0 e_cross[np.isnan(e_cross)] = 0.0 LOS[np.isnan(e_plus)] = 0.0 # get the indices for the binning mask = ( (projected_separation_len > rp_cut) * (separation_len >= self.r_bins[0]) * (separation_len < self.r_bins[-1]) * (LOS >= -self.r_bins[-1]) * (LOS < self.r_bins[-1]) ) ind_r = np.floor( np.log10(separation_len[mask]) / sub_box_len_logr - np.log10(self.r_bins[0]) / sub_box_len_logr ) del separation_len, projected_separation_len ind_r = np.array(ind_r, dtype=int) for r_bin in np.arange(min(ind_r), max(ind_r) + 1): ind_mu_r = np.floor( LOS[mask] / sub_box_len_pi[r_bin] - pi_bins[r_bin, 0] / sub_box_len_pi[r_bin] ) # need length of LOS, so only positive values ind_mu_r = np.array(ind_mu_r, dtype=int) mask2 = (ind_mu_r >= 0) * (ind_mu_r < self.num_bins_pi) np.add.at(Splus_D, ([r_bin] * sum(mask2), ind_mu_r[mask2]), (weight[n] * weight_shape[mask][mask2] * e_plus[mask][mask2]) / (2 * R)) np.add.at(Scross_D, ([r_bin] * sum(mask2), ind_mu_r[mask2]), (weight[n] * weight_shape[mask][mask2] * e_cross[mask][mask2]) / (2 * R)) np.add.at(DD, ([r_bin] * sum(mask2), ind_mu_r[mask2]), weight[n] * weight_shape[mask][mask2]) del e_plus, e_cross, LOS # if Num_position == Num_shape: # corrtype = \"auto\" # DD = DD / 2.0 # auto correlation, all pairs are double # else: corrtype = \"cross\" # analytical calc is much more difficult for (r,mu_r) bins for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_g_plus[i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], pi_bins[i, p + 1], pi_bins[i, p], L3, \"cross\", Num_position, Num_shape) RR_gg[i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], pi_bins[i, p + 1], pi_bins[i, p], L3, corrtype, Num_position, Num_shape) correlation = Splus_D / RR_g_plus # (Splus_D - Splus_R) / RR_g_plus xi_g_cross = Scross_D / RR_g_plus # (Scross_D - Scross_R) / RR_g_plus dsep = (self.r_bins[1:] - self.r_bins[:-1]) / 2.0 separation_bins = self.r_bins[:-1] + abs(dsep) # middle of bins # mu_r_bins=[] # for i in np.arange(0, self.num_bins_pi): dmur = (pi_bins[:, 1:] - pi_bins[:, :-1]) / 2.0 mu_r_bins = pi_bins[:, :-1] + abs(dmur) # middle of bins # mu_r_bins = np.array(mu_r_bins, dtype=float) if (self.output_file_name != None) & return_output == False: output_file = h5py.File(self.output_file_name, \"a\") group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_plus/r_pi_grid/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=correlation) write_dataset_hdf5(group, dataset_name + \"_SplusD\", data=Splus_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_plus\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_cross/r_pi_grid/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=xi_g_cross) write_dataset_hdf5(group, dataset_name + \"_ScrossD\", data=Scross_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_cross\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_gg/r_pi_grid/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=(DD / RR_gg) - 1) write_dataset_hdf5(group, dataset_name + \"_DD\", data=DD) write_dataset_hdf5(group, dataset_name + \"_RR_gg\", data=RR_gg) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=mu_r_bins) output_file.close() return else: return correlation, (DD / RR_gg) - 1, separation_bins, mu_r_bins, Splus_D, DD, RR_g_plus def _measure_xi_r_mur_box_brute(self, dataset_name, masks=None, rp_cut=None, return_output=False, jk_group_name=\"\", ellipticity='distortion'): r\"\"\"Measures the projected correlation functions, $\\xi_{gg}$ and $\\xi_{g+}$, in (r, mu_r) bins for an object created with MeasureIABox. Uses 1 CPU. Parameters ---------- dataset_name : str Name of the dataset in the output file. masks : dict or NoneType, optional Dictionary with masks for the data to select only part of the data. Uses same keywords as data dictionary. Default value is None. rp_cut : float, optional Limit for minimum r_p value for pairs to be included. Default value is None. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. jk_group_name : str, optional Group in output file (hdf5) where jackknife realisations are stored. Default value is \"\". ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. Returns ------- ndarrays $\\xi_{gg}$ and $\\xi_{g+}$, r bins, mu_r bins, S+D, DD, RR (if no output file is specified) \"\"\" if masks == None: positions = self.data[\"Position\"] positions_shape_sample = self.data[\"Position_shape_sample\"] axis_direction_v = self.data[\"Axis_Direction\"] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"] weight = self.data[\"weight\"] weight_shape = self.data[\"weight_shape_sample\"] else: positions = self.data[\"Position\"][masks[\"Position\"]] positions_shape_sample = self.data[\"Position_shape_sample\"][masks[\"Position_shape_sample\"]] axis_direction_v = self.data[\"Axis_Direction\"][masks[\"Axis_Direction\"]] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"][masks[\"q\"]] try: weight_mask = masks[\"weight\"] except: masks[\"weight\"] = np.ones(self.Num_position, dtype=bool) masks[\"weight\"][sum(masks[\"Position\"]):self.Num_position] = 0 try: weight_mask = masks[\"weight_shape_sample\"] except: masks[\"weight_shape_sample\"] = np.ones(self.Num_shape, dtype=bool) masks[\"weight_shape_sample\"][sum(masks[\"Position_shape_sample\"]):self.Num_shape] = 0 weight = self.data[\"weight\"][masks[\"weight\"]] weight_shape = self.data[\"weight_shape_sample\"][masks[\"weight_shape_sample\"]] Num_position = len(positions) Num_shape = len(positions_shape_sample) print( f\"There are {Num_shape} galaxies in the shape sample and {Num_position} galaxies in the position sample.\") if rp_cut == None: rp_cut = 0.0 LOS_ind = self.data[\"LOS\"] # eg 2 for z axis not_LOS = np.array([0, 1, 2])[np.isin([0, 1, 2], LOS_ind, invert=True)] # eg 0,1 for x&y if ellipticity == 'distortion': e = (1 - q ** 2) / (1 + q ** 2) # size of ellipticity elif ellipticity == 'ellipticity': e = (1 - q) / (1 + q) else: raise ValueError(\"Invalid value for ellipticity. Choose 'distortion' or 'ellipticity'.\") del q R = sum(weight_shape * (1 - e ** 2 / 2.0)) / sum(weight_shape) # R = 1 - np.mean(e ** 2) / 2.0 # responsivity factor L3 = self.boxsize ** 3 # box volume sub_box_len_logr = (np.log10(self.r_max) - np.log10(self.r_min)) / self.num_bins_r sub_box_len_mu_r = 2.0 / self.num_bins_pi # mu_r ranges from -1 to 1. Same number of bins as pi DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_g_plus = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_gg = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) for n in np.arange(0, len(positions)): # for Splus_D (calculate ellipticities around position sample) separation = positions_shape_sample - positions[n] if self.periodicity: separation[separation > self.L_0p5] -= self.boxsize # account for periodicity of box separation[separation < -self.L_0p5] += self.boxsize projected_sep = separation[:, not_LOS] LOS = separation[:, LOS_ind] projected_separation_len = np.sqrt(np.sum(projected_sep ** 2, axis=1)) with np.errstate(invalid='ignore'): separation_dir = ( projected_sep.transpose() / projected_separation_len).transpose() # normalisation of rp separation_len = np.sqrt(np.sum(separation ** 2, axis=1)) with np.errstate(invalid='ignore'): mu_r = LOS / separation_len del LOS, projected_sep, separation phi = np.arccos(self.calculate_dot_product_arrays(separation_dir, axis_direction)) # [0,pi] e_plus, e_cross = self.get_ellipticity(e, phi) del phi, separation_dir e_plus[np.isnan(e_plus)] = 0.0 e_cross[np.isnan(e_cross)] = 0.0 mu_r[np.isnan(e_plus)] = 0.0 # get the indices for the binning mask = ( (projected_separation_len > rp_cut) * (separation_len >= self.r_bins[0]) * (separation_len < self.r_bins[-1]) ) ind_r = np.floor( np.log10(separation_len[mask]) / sub_box_len_logr - np.log10(self.r_bins[0]) / sub_box_len_logr ) del separation_len, projected_separation_len ind_r = np.array(ind_r, dtype=int) ind_mu_r = np.floor( mu_r[mask] / sub_box_len_mu_r - self.mu_r_bins[0] / sub_box_len_mu_r ) # need length of LOS, so only positive values ind_mu_r = np.array(ind_mu_r, dtype=int) if np.any(ind_mu_r == self.num_bins_pi): ind_mu_r[ind_mu_r >= self.num_bins_pi] -= 1 if np.any(ind_r == self.num_bins_r): ind_r[ind_r >= self.num_bins_r] -= 1 np.add.at(Splus_D, (ind_r, ind_mu_r), (weight[n] * weight_shape[mask] * e_plus[mask]) / (2 * R)) np.add.at(Scross_D, (ind_r, ind_mu_r), (weight[n] * weight_shape[mask] * e_cross[mask]) / (2 * R)) del e_plus, e_cross, mu_r np.add.at(DD, (ind_r, ind_mu_r), weight[n] * weight_shape[mask]) # if Num_position == Num_shape: # corrtype = \"auto\" # DD = DD / 2.0 # auto correlation, all pairs are double # else: corrtype = \"cross\" # analytical calc is much more difficult for (r,mu_r) bins for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_g_plus[i, p] = self.get_random_pairs_r_mur( self.r_bins[i + 1], self.r_bins[i], self.mu_r_bins[p + 1], self.mu_r_bins[p], L3, \"cross\", Num_position, Num_shape) RR_gg[i, p] = self.get_random_pairs_r_mur( self.r_bins[i + 1], self.r_bins[i], self.mu_r_bins[p + 1], self.mu_r_bins[p], L3, corrtype, Num_position, Num_shape) correlation = Splus_D / RR_g_plus # (Splus_D - Splus_R) / RR_g_plus xi_g_cross = Scross_D / RR_g_plus # (Scross_D - Scross_R) / RR_g_plus dsep = (self.r_bins[1:] - self.r_bins[:-1]) / 2.0 separation_bins = self.r_bins[:-1] + abs(dsep) # middle of bins dmur = (self.mu_r_bins[1:] - self.mu_r_bins[:-1]) / 2.0 mu_r_bins = self.mu_r_bins[:-1] + abs(dmur) # middle of bins if (self.output_file_name != None) & return_output == False: output_file = h5py.File(self.output_file_name, \"a\") group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_plus/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=correlation) write_dataset_hdf5(group, dataset_name + \"_SplusD\", data=Splus_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_plus\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_cross/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=xi_g_cross) write_dataset_hdf5(group, dataset_name + \"_ScrossD\", data=Scross_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_cross\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_gg/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=(DD / RR_gg) - 1) write_dataset_hdf5(group, dataset_name + \"_DD\", data=DD) write_dataset_hdf5(group, dataset_name + \"_RR_gg\", data=RR_gg) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) output_file.close() return else: return correlation, (DD / RR_gg) - 1, separation_bins, mu_r_bins, Splus_D, DD, RR_g_plus def _measure_xi_r_mur_box_tree(self, dataset_name, masks=None, rp_cut=None, return_output=False, jk_group_name=\"\", ellipticity='distortion'): r\"\"\"Measures the projected correlation functions, $\\xi_{gg}$ and $\\xi_{g+}$, in (r, mu_r) bins for an object created with MeasureIABox. Uses 1 CPU. Uses KDTree for speedup. Parameters ---------- dataset_name : str Name of the dataset in the output file. masks : dict or NoneType, optional Dictionary with masks for the data to select only part of the data. Uses same keywords as data dictionary. Default value is None. rp_cut : float, optional Limit for minimum r_p value for pairs to be included. Default value is None. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. jk_group_name : str, optional Group in output file (hdf5) where jackknife realisations are stored. Default value is \"\". ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. Returns ------- ndarrays $\\xi_{gg}$ and $\\xi_{g+}$, r bins, mu_r bins, S+D, DD, RR (if no output file is specified) \"\"\" if masks == None: positions = self.data[\"Position\"] positions_shape_sample = self.data[\"Position_shape_sample\"] axis_direction_v = self.data[\"Axis_Direction\"] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"] weight = self.data[\"weight\"] weight_shape = self.data[\"weight_shape_sample\"] else: positions = self.data[\"Position\"][masks[\"Position\"]] positions_shape_sample = self.data[\"Position_shape_sample\"][masks[\"Position_shape_sample\"]] axis_direction_v = self.data[\"Axis_Direction\"][masks[\"Axis_Direction\"]] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"][masks[\"q\"]] try: weight_mask = masks[\"weight\"] except: masks[\"weight\"] = np.ones(self.Num_position, dtype=bool) masks[\"weight\"][sum(masks[\"Position\"]):self.Num_position] = 0 try: weight_mask = masks[\"weight_shape_sample\"] except: masks[\"weight_shape_sample\"] = np.ones(self.Num_shape, dtype=bool) masks[\"weight_shape_sample\"][sum(masks[\"Position_shape_sample\"]):self.Num_shape] = 0 weight = self.data[\"weight\"][masks[\"weight\"]] weight_shape = self.data[\"weight_shape_sample\"][masks[\"weight_shape_sample\"]] # masking changes the number of galaxies Num_position = len(positions) # number of halos in position sample Num_shape = len(positions_shape_sample) # number of halos in shape sample if rp_cut == None: rp_cut = 0.0 LOS_ind = self.data[\"LOS\"] # eg 2 for z axis not_LOS = np.array([0, 1, 2])[np.isin([0, 1, 2], LOS_ind, invert=True)] # eg 0,1 for x&y if ellipticity == 'distortion': e = (1 - q ** 2) / (1 + q ** 2) # size of ellipticity elif ellipticity == 'ellipticity': e = (1 - q) / (1 + q) else: raise ValueError(\"Invalid value for ellipticity. Choose 'distortion' or 'ellipticity'.\") R = sum(weight_shape * (1 - e ** 2 / 2.0)) / sum(weight_shape) # R = 1 - np.mean(e ** 2) / 2.0 # responsivity factor L3 = self.boxsize ** 3 # box volume sub_box_len_logr = (np.log10(self.r_max) - np.log10(self.r_min)) / self.num_bins_r sub_box_len_mu_r = 2.0 / self.num_bins_pi # mu_r ranges from -1 to 1. Same number of bins as pi DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_g_plus = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_gg = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) print( f\"There are {Num_shape} galaxies in the shape sample and {Num_position} galaxies in the position sample.\") pos_tree = KDTree(positions, boxsize=self.boxsize) for i in np.arange(0, len(positions_shape_sample), 100): i2 = min(len(positions_shape_sample), i + 100) positions_shape_sample_i = positions_shape_sample[i:i2] axis_direction_i = axis_direction[i:i2] e_i = e[i:i2] weight_shape_i = weight_shape[i:i2] shape_tree = KDTree(positions_shape_sample_i, boxsize=self.boxsize) ind_min_i = shape_tree.query_ball_tree(pos_tree, self.r_min) ind_max_i = shape_tree.query_ball_tree(pos_tree, self.r_max) ind_rbin_i = self.setdiff2D(ind_max_i, ind_min_i) for n in np.arange(0, len(positions_shape_sample_i)): if len(ind_rbin_i[n]) > 0: # for Splus_D (calculate ellipticities around position sample) separation = positions_shape_sample_i[n] - positions[ind_rbin_i[n]] if self.periodicity: separation[separation > self.L_0p5] -= self.boxsize # account for periodicity of box separation[separation < -self.L_0p5] += self.boxsize projected_sep = separation[:, not_LOS] LOS = separation[:, LOS_ind] projected_separation_len = np.sqrt(np.sum(projected_sep ** 2, axis=1)) with np.errstate(invalid='ignore'): separation_dir = ( projected_sep.transpose() / projected_separation_len).transpose() # normalisation of rp separation_len = np.sqrt(np.sum(separation ** 2, axis=1)) del separation, projected_sep with np.errstate(invalid='ignore'): mu_r = LOS / separation_len phi = np.arccos( separation_dir[:, 0] * axis_direction_i[n, 0] + separation_dir[:, 1] * axis_direction_i[ n, 1]) # [0,pi] e_plus, e_cross = self.get_ellipticity(e_i[n], phi) del phi, LOS, separation_dir e_plus[np.isnan(e_plus)] = 0.0 mu_r[np.isnan(e_plus)] = 0.0 e_cross[np.isnan(e_cross)] = 0.0 # get the indices for the binning mask = ( (projected_separation_len > rp_cut) * (separation_len >= self.r_bins[0]) * (separation_len < self.r_bins[-1]) ) ind_r = np.floor( np.log10(separation_len[mask]) / sub_box_len_logr - np.log10( self.r_bins[0]) / sub_box_len_logr ) ind_r = np.array(ind_r, dtype=int) ind_mu_r = np.floor( mu_r[mask] / sub_box_len_mu_r - self.mu_r_bins[0] / sub_box_len_mu_r ) # need length of LOS, so only positive values ind_mu_r = np.array(ind_mu_r, dtype=int) if np.any(ind_mu_r == self.num_bins_pi): ind_mu_r[ind_mu_r >= self.num_bins_pi] -= 1 if np.any(ind_r == self.num_bins_r): ind_r[ind_r >= self.num_bins_r] -= 1 np.add.at(Splus_D, (ind_r, ind_mu_r), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_plus[mask]) / (2 * R)) np.add.at(Scross_D, (ind_r, ind_mu_r), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_cross[mask]) / (2 * R)) np.add.at(DD, (ind_r, ind_mu_r), weight[ind_rbin_i[n]][mask] * weight_shape_i[n]) del e_plus, e_cross, mask, separation_len # if Num_position == Num_shape: # corrtype = \"auto\" # DD = DD / 2.0 # auto correlation, all pairs are double # else: corrtype = \"cross\" # analytical calc is much more difficult for (r,mu_r) bins for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_g_plus[i, p] = self.get_random_pairs_r_mur( self.r_bins[i + 1], self.r_bins[i], self.mu_r_bins[p + 1], self.mu_r_bins[p], L3, \"cross\", Num_position, Num_shape) RR_gg[i, p] = self.get_random_pairs_r_mur( self.r_bins[i + 1], self.r_bins[i], self.mu_r_bins[p + 1], self.mu_r_bins[p], L3, corrtype, Num_position, Num_shape) correlation = Splus_D / RR_g_plus # (Splus_D - Splus_R) / RR_g_plus xi_g_cross = Scross_D / RR_g_plus # (Scross_D - Scross_R) / RR_g_plus dsep = (self.r_bins[1:] - self.r_bins[:-1]) / 2.0 separation_bins = self.r_bins[:-1] + abs(dsep) # middle of bins dmur = (self.mu_r_bins[1:] - self.mu_r_bins[:-1]) / 2.0 mu_r_bins = self.mu_r_bins[:-1] + abs(dmur) # middle of bins if (self.output_file_name != None) & return_output == False: output_file = h5py.File(self.output_file_name, \"a\") group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_plus/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=correlation) write_dataset_hdf5(group, dataset_name + \"_SplusD\", data=Splus_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_plus\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_cross/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=xi_g_cross) write_dataset_hdf5(group, dataset_name + \"_ScrossD\", data=Scross_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_cross\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_gg/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=(DD / RR_gg) - 1) write_dataset_hdf5(group, dataset_name + \"_DD\", data=DD) write_dataset_hdf5(group, dataset_name + \"_RR_gg\", data=RR_gg) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) output_file.close() return else: return correlation, (DD / RR_gg) - 1, separation_bins, mu_r_bins, Splus_D, DD, RR_g_plus def _measure_xi_r_mur_box_batch(self, i): r\"\"\"Measures components of $\\xi_{gg}$ and $\\xi_{g+}$ in (r, mu_r) bins including jackknife realisations for a batch of indices from i to i+chunk_size. Support function for _measure_xi_r_mu_r_box_jk_multiprocessing(). Parameters ---------- i: int Start index of the batch. Returns ------- ndarrays S+D, SxD, DD, DD_jk, S+D_jk where the _jk versions store the necessary information of DD of S+D for each jackknife realisation. \"\"\" if i + self.chunk_size > self.Num_shape_masked: i2 = self.Num_shape_masked else: i2 = i + self.chunk_size DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) positions_shape_sample_i = self.temp_data_obj_m.read_cat(\"positions_shape_sample\", [i, i2]) axis_direction_i = self.temp_data_obj_m.read_cat(\"axis_direction\", [i, i2]) weight_shape_i = self.temp_data_obj_m.read_cat(\"weight_shape\", [i, i2]) positions = self.temp_data_obj_m.read_cat(\"positions\") weight = self.temp_data_obj_m.read_cat(\"weight\") e_i = self.e[i:i2] shape_tree = KDTree(positions_shape_sample_i, boxsize=self.boxsize) ind_min_i = shape_tree.query_ball_tree(self.pos_tree, self.r_min) ind_max_i = shape_tree.query_ball_tree(self.pos_tree, self.r_max) ind_rbin_i = self.setdiff2D(ind_max_i, ind_min_i) for n in np.arange(0, len(positions_shape_sample_i)): if len(ind_rbin_i[n]) > 0: # for Splus_D (calculate ellipticities around position sample) separation = positions_shape_sample_i[n] - positions[ind_rbin_i[n]] if self.periodicity: separation[separation > self.L_0p5] -= self.boxsize # account for periodicity of box separation[separation < -self.L_0p5] += self.boxsize projected_sep = separation[:, self.not_LOS] LOS = separation[:, self.LOS_ind] projected_separation_len = np.sqrt(np.sum(projected_sep ** 2, axis=1)) with np.errstate(invalid='ignore'): separation_dir = ( projected_sep.transpose() / projected_separation_len).transpose() # normalisation of rp separation_len = np.sqrt(np.sum(separation ** 2, axis=1)) del separation, projected_sep with np.errstate(invalid='ignore'): mu_r = LOS / separation_len phi = np.arccos( separation_dir[:, 0] * axis_direction_i[n, 0] + separation_dir[:, 1] * axis_direction_i[ n, 1]) # [0,pi] e_plus, e_cross = self.get_ellipticity(e_i[n], phi) del phi, LOS, separation_dir e_plus[np.isnan(e_plus)] = 0.0 mu_r[np.isnan(e_plus)] = 0.0 e_cross[np.isnan(e_cross)] = 0.0 # get the indices for the binning mask = ( (projected_separation_len > self.rp_cut) * (separation_len >= self.r_bins[0]) * (separation_len < self.r_bins[-1]) ) ind_r = np.floor( np.log10(separation_len[mask]) / self.sub_box_len_logr - np.log10( self.r_bins[0]) / self.sub_box_len_logr ) ind_r = np.array(ind_r, dtype=int) ind_mu_r = np.floor( mu_r[mask] / self.sub_box_len_mu_r - self.mu_r_bins[0] / self.sub_box_len_mu_r ) # need length of LOS, so only positive values ind_mu_r = np.array(ind_mu_r, dtype=int) if np.any(ind_mu_r == self.num_bins_pi): ind_mu_r[ind_mu_r >= self.num_bins_pi] -= 1 if np.any(ind_r == self.num_bins_r): ind_r[ind_r >= self.num_bins_r] -= 1 np.add.at(Splus_D, (ind_r, ind_mu_r), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_plus[mask]) / (2 * self.R)) np.add.at(Scross_D, (ind_r, ind_mu_r), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_cross[mask]) / (2 * self.R)) np.add.at(DD, (ind_r, ind_mu_r), weight[ind_rbin_i[n]][mask] * weight_shape_i[n]) del separation_len, e_cross, e_plus return Splus_D, Scross_D, DD def _measure_xi_r_mur_box_multiprocessing(self, dataset_name, temp_file_path, masks=None, rp_cut=None, return_output=False, jk_group_name=\"\", chunk_size=100, num_nodes=1, ellipticity='distortion'): r\"\"\"Measures the projected correlation functions, $\\xi_{gg}$ and $\\xi_{g+}$, in (r, mu_r) bins for an object created with MeasureIABox. Uses >1 CPU. Uses KDTree for speedup. Parameters ---------- dataset_name : str Name of the dataset in the output file. temp_file_path : str or NoneType, optional Path to where the data is temporarily stored [file name generated automatically]. num_nodes : int, optional Number of CPUs used in the multiprocessing. Default is 1. masks : dict or NoneType, optional Dictionary with masks for the data to select only part of the data. Uses same keywords as data dictionary. Default value = None. rp_cut : float, optional Limit for minimum r_p value for pairs to be included. Default value is None. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. jk_group_name : str, optional Group in output file (hdf5) where jackknife realisations are stored. Default value is \"\". ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. Returns ------- ndarrays $\\xi_{gg}$ and $\\xi_{g+}$, r bins, mu_r bins, S+D, DD, RR (if no output file is specified) \"\"\" if masks == None: positions = self.data[\"Position\"] positions_shape_sample = self.data[\"Position_shape_sample\"] axis_direction_v = self.data[\"Axis_Direction\"] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"] weight = self.data[\"weight\"] weight_shape = self.data[\"weight_shape_sample\"] else: positions = self.data[\"Position\"][masks[\"Position\"]] positions_shape_sample = self.data[\"Position_shape_sample\"][masks[\"Position_shape_sample\"]] axis_direction_v = self.data[\"Axis_Direction\"][masks[\"Axis_Direction\"]] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"][masks[\"q\"]] try: weight_mask = masks[\"weight\"] except: masks[\"weight\"] = np.ones(self.Num_position, dtype=bool) masks[\"weight\"][sum(masks[\"Position\"]):self.Num_position] = 0 try: weight_mask = masks[\"weight_shape_sample\"] except: masks[\"weight_shape_sample\"] = np.ones(self.Num_shape, dtype=bool) masks[\"weight_shape_sample\"][sum(masks[\"Position_shape_sample\"]):self.Num_shape] = 0 weight = self.data[\"weight\"][masks[\"weight\"]] weight_shape = self.data[\"weight_shape_sample\"][masks[\"weight_shape_sample\"]] # masking changes the number of galaxies self.Num_position_masked = len(positions) self.Num_shape_masked = len(positions_shape_sample) print( f\"There are {self.Num_shape_masked} galaxies in the shape sample and {self.Num_position_masked} galaxies in the position sample.\") # create temp hdf5 from which data can be read. del self.data, but save it in this method to reduce RAM figname_dataset_name = dataset_name if \"/\" in dataset_name: figname_dataset_name = figname_dataset_name.replace(\"/\", \"_\") if \".\" in dataset_name: figname_dataset_name = figname_dataset_name.replace(\".\", \"p\") file_temp = h5py.File(f\"{temp_file_path}/m_{self.simname}_temp_data_{figname_dataset_name}.hdf5\", \"w\") write_dataset_hdf5(file_temp, \"positions\", positions) write_dataset_hdf5(file_temp, \"weight\", weight) write_dataset_hdf5(file_temp, \"weight_shape\", weight_shape) write_dataset_hdf5(file_temp, \"positions_shape_sample\", positions_shape_sample) write_dataset_hdf5(file_temp, \"axis_direction\", axis_direction) file_temp.close() self.temp_data_obj_m = ReadData(self.simname, f\"m_{self.simname}_temp_data_{figname_dataset_name}\", None, data_path=temp_file_path) if rp_cut == None: self.rp_cut = 0.0 else: self.rp_cut = rp_cut self.LOS_ind = self.data[\"LOS\"] # eg 2 for z axis self.not_LOS = np.array([0, 1, 2])[np.isin([0, 1, 2], self.LOS_ind, invert=True)] # eg 0,1 for x&y if ellipticity == 'distortion': self.e = (1 - q ** 2) / (1 + q ** 2) # size of ellipticity elif ellipticity == 'ellipticity': self.e = (1 - q) / (1 + q) else: raise ValueError(\"Invalid value for ellipticity. Choose 'distortion' or 'ellipticity'.\") self.R = sum(weight_shape * (1 - self.e ** 2 / 2.0)) / sum(weight_shape) # self.R = 1 - np.mean(self.e ** 2) / 2.0 # responsivity factor L3 = self.boxsize ** 3 # box volume self.sub_box_len_logr = (np.log10(self.r_max) - np.log10(self.r_min)) / self.num_bins_r self.sub_box_len_mu_r = 2.0 / self.num_bins_pi # mu_r ranges from -1 to 1. Same number of bins as pi DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_g_plus = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_gg = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) data_temp = self.data # make sure data is not sent to every CPU self.data = None self.pos_tree = KDTree(positions, boxsize=self.boxsize) indices = np.arange(0, len(positions_shape_sample), chunk_size) self.chunk_size = chunk_size with Pool(num_nodes) as p: result = p.map(self._measure_xi_r_mur_box_batch, indices) os.remove( f\"{temp_file_path}/m_{self.simname}_temp_data_{figname_dataset_name}.hdf5\") self.data = data_temp del data_temp for i in np.arange(len(result)): Splus_D += result[i][0] Scross_D += result[i][1] DD += result[i][2] # if Num_position == Num_shape: # corrtype = \"auto\" # DD = DD / 2.0 # auto correlation, all pairs are double # else: corrtype = \"cross\" # analytical calc is much more difficult for (r,mu_r) bins for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_g_plus[i, p] = self.get_random_pairs_r_mur( self.r_bins[i + 1], self.r_bins[i], self.mu_r_bins[p + 1], self.mu_r_bins[p], L3, \"cross\", self.Num_position_masked, self.Num_shape_masked) RR_gg[i, p] = self.get_random_pairs_r_mur( self.r_bins[i + 1], self.r_bins[i], self.mu_r_bins[p + 1], self.mu_r_bins[p], L3, corrtype, self.Num_position_masked, self.Num_shape_masked) correlation = Splus_D / RR_g_plus # (Splus_D - Splus_R) / RR_g_plus xi_g_cross = Scross_D / RR_g_plus # (Scross_D - Scross_R) / RR_g_plus dsep = (self.r_bins[1:] - self.r_bins[:-1]) / 2.0 separation_bins = self.r_bins[:-1] + abs(dsep) # middle of bins dmur = (self.mu_r_bins[1:] - self.mu_r_bins[:-1]) / 2.0 mu_r_bins = self.mu_r_bins[:-1] + abs(dmur) # middle of bins if (self.output_file_name != None) & return_output == False: output_file = h5py.File(self.output_file_name, \"a\") group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_plus/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=correlation) write_dataset_hdf5(group, dataset_name + \"_SplusD\", data=Splus_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_plus\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_cross/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=xi_g_cross) write_dataset_hdf5(group, dataset_name + \"_ScrossD\", data=Scross_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_cross\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_gg/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=(DD / RR_gg) - 1) write_dataset_hdf5(group, dataset_name + \"_DD\", data=DD) write_dataset_hdf5(group, dataset_name + \"_RR_gg\", data=RR_gg) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) output_file.close() return else: return correlation, (DD / RR_gg) - 1, separation_bins, mu_r_bins, Splus_D, DD, RR_g_plus __init__(data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True) The init method of the MeasureMultipolesSimulations class. Notes Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. Source code in src/measureia/measure_m_box.py def __init__( self, data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True, ): \"\"\" The __init__ method of the MeasureMultipolesSimulations class. Notes ----- Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. \"\"\" super().__init__(data, output_file_name, simulation, snapshot, separation_limits, num_bins_r, num_bins_pi, pi_max, boxsize, periodicity) return","title":"MeasureMultipolesBox"},{"location":"api/MeasureMBox/#measuremultipolesbox","text":"","title":"MeasureMultipolesBox"},{"location":"api/MeasureMBox/#measureia.MeasureMultipolesBox","text":"Bases: MeasureIABase , ReadData Class that contains all methods for the measurements of \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) for \\(\\tilde{\\xi}_{gg,0}\\) and \\(\\tilde{\\xi}_{g+,2}\\) with Cartesian simulation data. Methods: Name Description _measure_xi_r_mur_box_brute Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (r, mu_r) grid binning in a periodic box using 1 CPU. _measure_xi_r_mur_box_tree Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (r, mu_r) grid binning in a periodic box using 1 CPU and KDTree for extra speed. _measure_xi_r_mur_box_batch Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (r, mu_r) grid binning in a periodic box using 1 CPU for a batch of indices. Support function of _measure_xi_r_mur_box_multiprocessing(). _measure_xi_r_mur_box_multiprocessing Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (r, mu_r) grid binning in a periodic box using >1 CPUs. _measure_xi_r_pi_box_brute Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (r, pi) grid binning in a periodic box using 1 CPU. Notes Inherits attributes from 'SimInfo', where 'boxsize', 'L_0p5' and 'snap_group' are used in this class. Inherits attributes from 'MeasureIABase', where 'data', 'output_file_name', 'periodicity', 'Num_position', 'Num_shape', 'r_min', 'r_max', 'num_bins_r', 'num_bins_pi', 'r_bins', 'pi_bins', 'mu_r_bins' are used. Source code in src/measureia/measure_m_box.py class MeasureMultipolesBox(MeasureIABase, ReadData): r\"\"\"Class that contains all methods for the measurements of $\\xi_{gg}$ and $\\xi_{g+}$ for $\\tilde{\\xi}_{gg,0}$ and $\\tilde{\\xi}_{g+,2}$ with Cartesian simulation data. Methods ------- _measure_xi_r_mur_box_brute() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (r, mu_r) grid binning in a periodic box using 1 CPU. _measure_xi_r_mur_box_tree() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (r, mu_r) grid binning in a periodic box using 1 CPU and KDTree for extra speed. _measure_xi_r_mur_box_batch() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (r, mu_r) grid binning in a periodic box using 1 CPU for a batch of indices. Support function of _measure_xi_r_mur_box_multiprocessing(). _measure_xi_r_mur_box_multiprocessing() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (r, mu_r) grid binning in a periodic box using >1 CPUs. _measure_xi_r_pi_box_brute() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (r, pi) grid binning in a periodic box using 1 CPU. Notes ----- Inherits attributes from 'SimInfo', where 'boxsize', 'L_0p5' and 'snap_group' are used in this class. Inherits attributes from 'MeasureIABase', where 'data', 'output_file_name', 'periodicity', 'Num_position', 'Num_shape', 'r_min', 'r_max', 'num_bins_r', 'num_bins_pi', 'r_bins', 'pi_bins', 'mu_r_bins' are used. \"\"\" def __init__( self, data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True, ): \"\"\" The __init__ method of the MeasureMultipolesSimulations class. Notes ----- Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. \"\"\" super().__init__(data, output_file_name, simulation, snapshot, separation_limits, num_bins_r, num_bins_pi, pi_max, boxsize, periodicity) return def _measure_xi_r_pi_box_brute(self, dataset_name, masks=None, rp_cut=None, return_output=False, jk_group_name=\"\", ellipticity='distortion'): r\"\"\"Measures the projected correlation functions, $\\xi_{gg}$ and $\\xi_{g+}$, in (r, pi) bins for an object created with MeasureIABox. Uses 1 CPU. For each r bin, the pin bins are spaced between -r_max and r_max. Parameters ---------- dataset_name : str Name of the dataset in the output file. masks : dict or NoneType, optional Dictionary with masks for the data to select only part of the data. Uses same keywords as data dictionary. Default value is None. rp_cut : float, optional Limit for minimum r_p value for pairs to be included. Default value is None. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. jk_group_name : str, optional Group in output file (hdf5) where jackknife realisations are stored. Default value is \"\". ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. Returns ------- ndarrays $\\xi_{gg}$ and $\\xi_{g+}$, r bins, mu_r bins, S+D, DD, RR (if no output file is specified) \"\"\" if masks == None: positions = self.data[\"Position\"] positions_shape_sample = self.data[\"Position_shape_sample\"] axis_direction_v = self.data[\"Axis_Direction\"] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"] weight = self.data[\"weight\"] weight_shape = self.data[\"weight_shape_sample\"] else: positions = self.data[\"Position\"][masks[\"Position\"]] positions_shape_sample = self.data[\"Position_shape_sample\"][masks[\"Position_shape_sample\"]] axis_direction_v = self.data[\"Axis_Direction\"][masks[\"Axis_Direction\"]] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"][masks[\"q\"]] try: weight_mask = masks[\"weight\"] except: masks[\"weight\"] = np.ones(self.Num_position, dtype=bool) masks[\"weight\"][sum(masks[\"Position\"]):self.Num_position] = 0 try: weight_mask = masks[\"weight_shape_sample\"] except: masks[\"weight_shape_sample\"] = np.ones(self.Num_shape, dtype=bool) masks[\"weight_shape_sample\"][sum(masks[\"Position_shape_sample\"]):self.Num_shape] = 0 weight = self.data[\"weight\"][masks[\"weight\"]] weight_shape = self.data[\"weight_shape_sample\"][masks[\"weight_shape_sample\"]] Num_position = len(positions) Num_shape = len(positions_shape_sample) print( f\"There are {Num_shape} galaxies in the shape sample and {Num_position} galaxies in the position sample.\") if rp_cut == None: rp_cut = 0.0 LOS_ind = self.data[\"LOS\"] # eg 2 for z axis not_LOS = np.array([0, 1, 2])[np.isin([0, 1, 2], LOS_ind, invert=True)] # eg 0,1 for x&y if ellipticity == 'distortion': e = (1 - q ** 2) / (1 + q ** 2) # size of ellipticity elif ellipticity == 'ellipticity': e = (1 - q) / (1 + q) else: raise ValueError(\"Invalid value for ellipticity. Choose 'distortion' or 'ellipticity'.\") del q R = sum(weight_shape * (1 - e ** 2 / 2.0)) / sum(weight_shape) # R = 1 - np.mean(e ** 2) / 2.0 # responsivity factor L3 = self.boxsize ** 3 # box volume sub_box_len_logr = (np.log10(self.r_max) - np.log10(self.r_min)) / self.num_bins_r pi_bins = np.zeros((self.num_bins_r + 1, self.num_bins_pi + 1)) for i in np.arange(self.num_bins_r + 1): pi_bins[i] = np.linspace(-self.r_bins[i], self.r_bins[i], self.num_bins_pi + 1) sub_box_len_pi = (pi_bins[:, -1] - pi_bins[:, 0]) / self.num_bins_pi DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_g_plus = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_gg = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) for n in np.arange(0, len(positions)): # for Splus_D (calculate ellipticities around position sample) separation = positions_shape_sample - positions[n] if self.periodicity: separation[separation > self.L_0p5] -= self.boxsize # account for periodicity of box separation[separation < -self.L_0p5] += self.boxsize projected_sep = separation[:, not_LOS] LOS = separation[:, LOS_ind] projected_separation_len = np.sqrt(np.sum(projected_sep ** 2, axis=1)) with np.errstate(invalid='ignore'): separation_dir = ( projected_sep.transpose() / projected_separation_len).transpose() # normalisation of rp separation_len = np.sqrt(np.sum(separation ** 2, axis=1)) with np.errstate(invalid='ignore'): # mu_r = LOS / separation_len del projected_sep, separation phi = np.arccos(self.calculate_dot_product_arrays(separation_dir, axis_direction)) # [0,pi] e_plus, e_cross = self.get_ellipticity(e, phi) del phi, separation_dir e_plus[np.isnan(e_plus)] = 0.0 e_cross[np.isnan(e_cross)] = 0.0 LOS[np.isnan(e_plus)] = 0.0 # get the indices for the binning mask = ( (projected_separation_len > rp_cut) * (separation_len >= self.r_bins[0]) * (separation_len < self.r_bins[-1]) * (LOS >= -self.r_bins[-1]) * (LOS < self.r_bins[-1]) ) ind_r = np.floor( np.log10(separation_len[mask]) / sub_box_len_logr - np.log10(self.r_bins[0]) / sub_box_len_logr ) del separation_len, projected_separation_len ind_r = np.array(ind_r, dtype=int) for r_bin in np.arange(min(ind_r), max(ind_r) + 1): ind_mu_r = np.floor( LOS[mask] / sub_box_len_pi[r_bin] - pi_bins[r_bin, 0] / sub_box_len_pi[r_bin] ) # need length of LOS, so only positive values ind_mu_r = np.array(ind_mu_r, dtype=int) mask2 = (ind_mu_r >= 0) * (ind_mu_r < self.num_bins_pi) np.add.at(Splus_D, ([r_bin] * sum(mask2), ind_mu_r[mask2]), (weight[n] * weight_shape[mask][mask2] * e_plus[mask][mask2]) / (2 * R)) np.add.at(Scross_D, ([r_bin] * sum(mask2), ind_mu_r[mask2]), (weight[n] * weight_shape[mask][mask2] * e_cross[mask][mask2]) / (2 * R)) np.add.at(DD, ([r_bin] * sum(mask2), ind_mu_r[mask2]), weight[n] * weight_shape[mask][mask2]) del e_plus, e_cross, LOS # if Num_position == Num_shape: # corrtype = \"auto\" # DD = DD / 2.0 # auto correlation, all pairs are double # else: corrtype = \"cross\" # analytical calc is much more difficult for (r,mu_r) bins for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_g_plus[i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], pi_bins[i, p + 1], pi_bins[i, p], L3, \"cross\", Num_position, Num_shape) RR_gg[i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], pi_bins[i, p + 1], pi_bins[i, p], L3, corrtype, Num_position, Num_shape) correlation = Splus_D / RR_g_plus # (Splus_D - Splus_R) / RR_g_plus xi_g_cross = Scross_D / RR_g_plus # (Scross_D - Scross_R) / RR_g_plus dsep = (self.r_bins[1:] - self.r_bins[:-1]) / 2.0 separation_bins = self.r_bins[:-1] + abs(dsep) # middle of bins # mu_r_bins=[] # for i in np.arange(0, self.num_bins_pi): dmur = (pi_bins[:, 1:] - pi_bins[:, :-1]) / 2.0 mu_r_bins = pi_bins[:, :-1] + abs(dmur) # middle of bins # mu_r_bins = np.array(mu_r_bins, dtype=float) if (self.output_file_name != None) & return_output == False: output_file = h5py.File(self.output_file_name, \"a\") group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_plus/r_pi_grid/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=correlation) write_dataset_hdf5(group, dataset_name + \"_SplusD\", data=Splus_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_plus\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_cross/r_pi_grid/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=xi_g_cross) write_dataset_hdf5(group, dataset_name + \"_ScrossD\", data=Scross_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_cross\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_gg/r_pi_grid/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=(DD / RR_gg) - 1) write_dataset_hdf5(group, dataset_name + \"_DD\", data=DD) write_dataset_hdf5(group, dataset_name + \"_RR_gg\", data=RR_gg) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=mu_r_bins) output_file.close() return else: return correlation, (DD / RR_gg) - 1, separation_bins, mu_r_bins, Splus_D, DD, RR_g_plus def _measure_xi_r_mur_box_brute(self, dataset_name, masks=None, rp_cut=None, return_output=False, jk_group_name=\"\", ellipticity='distortion'): r\"\"\"Measures the projected correlation functions, $\\xi_{gg}$ and $\\xi_{g+}$, in (r, mu_r) bins for an object created with MeasureIABox. Uses 1 CPU. Parameters ---------- dataset_name : str Name of the dataset in the output file. masks : dict or NoneType, optional Dictionary with masks for the data to select only part of the data. Uses same keywords as data dictionary. Default value is None. rp_cut : float, optional Limit for minimum r_p value for pairs to be included. Default value is None. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. jk_group_name : str, optional Group in output file (hdf5) where jackknife realisations are stored. Default value is \"\". ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. Returns ------- ndarrays $\\xi_{gg}$ and $\\xi_{g+}$, r bins, mu_r bins, S+D, DD, RR (if no output file is specified) \"\"\" if masks == None: positions = self.data[\"Position\"] positions_shape_sample = self.data[\"Position_shape_sample\"] axis_direction_v = self.data[\"Axis_Direction\"] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"] weight = self.data[\"weight\"] weight_shape = self.data[\"weight_shape_sample\"] else: positions = self.data[\"Position\"][masks[\"Position\"]] positions_shape_sample = self.data[\"Position_shape_sample\"][masks[\"Position_shape_sample\"]] axis_direction_v = self.data[\"Axis_Direction\"][masks[\"Axis_Direction\"]] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"][masks[\"q\"]] try: weight_mask = masks[\"weight\"] except: masks[\"weight\"] = np.ones(self.Num_position, dtype=bool) masks[\"weight\"][sum(masks[\"Position\"]):self.Num_position] = 0 try: weight_mask = masks[\"weight_shape_sample\"] except: masks[\"weight_shape_sample\"] = np.ones(self.Num_shape, dtype=bool) masks[\"weight_shape_sample\"][sum(masks[\"Position_shape_sample\"]):self.Num_shape] = 0 weight = self.data[\"weight\"][masks[\"weight\"]] weight_shape = self.data[\"weight_shape_sample\"][masks[\"weight_shape_sample\"]] Num_position = len(positions) Num_shape = len(positions_shape_sample) print( f\"There are {Num_shape} galaxies in the shape sample and {Num_position} galaxies in the position sample.\") if rp_cut == None: rp_cut = 0.0 LOS_ind = self.data[\"LOS\"] # eg 2 for z axis not_LOS = np.array([0, 1, 2])[np.isin([0, 1, 2], LOS_ind, invert=True)] # eg 0,1 for x&y if ellipticity == 'distortion': e = (1 - q ** 2) / (1 + q ** 2) # size of ellipticity elif ellipticity == 'ellipticity': e = (1 - q) / (1 + q) else: raise ValueError(\"Invalid value for ellipticity. Choose 'distortion' or 'ellipticity'.\") del q R = sum(weight_shape * (1 - e ** 2 / 2.0)) / sum(weight_shape) # R = 1 - np.mean(e ** 2) / 2.0 # responsivity factor L3 = self.boxsize ** 3 # box volume sub_box_len_logr = (np.log10(self.r_max) - np.log10(self.r_min)) / self.num_bins_r sub_box_len_mu_r = 2.0 / self.num_bins_pi # mu_r ranges from -1 to 1. Same number of bins as pi DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_g_plus = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_gg = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) for n in np.arange(0, len(positions)): # for Splus_D (calculate ellipticities around position sample) separation = positions_shape_sample - positions[n] if self.periodicity: separation[separation > self.L_0p5] -= self.boxsize # account for periodicity of box separation[separation < -self.L_0p5] += self.boxsize projected_sep = separation[:, not_LOS] LOS = separation[:, LOS_ind] projected_separation_len = np.sqrt(np.sum(projected_sep ** 2, axis=1)) with np.errstate(invalid='ignore'): separation_dir = ( projected_sep.transpose() / projected_separation_len).transpose() # normalisation of rp separation_len = np.sqrt(np.sum(separation ** 2, axis=1)) with np.errstate(invalid='ignore'): mu_r = LOS / separation_len del LOS, projected_sep, separation phi = np.arccos(self.calculate_dot_product_arrays(separation_dir, axis_direction)) # [0,pi] e_plus, e_cross = self.get_ellipticity(e, phi) del phi, separation_dir e_plus[np.isnan(e_plus)] = 0.0 e_cross[np.isnan(e_cross)] = 0.0 mu_r[np.isnan(e_plus)] = 0.0 # get the indices for the binning mask = ( (projected_separation_len > rp_cut) * (separation_len >= self.r_bins[0]) * (separation_len < self.r_bins[-1]) ) ind_r = np.floor( np.log10(separation_len[mask]) / sub_box_len_logr - np.log10(self.r_bins[0]) / sub_box_len_logr ) del separation_len, projected_separation_len ind_r = np.array(ind_r, dtype=int) ind_mu_r = np.floor( mu_r[mask] / sub_box_len_mu_r - self.mu_r_bins[0] / sub_box_len_mu_r ) # need length of LOS, so only positive values ind_mu_r = np.array(ind_mu_r, dtype=int) if np.any(ind_mu_r == self.num_bins_pi): ind_mu_r[ind_mu_r >= self.num_bins_pi] -= 1 if np.any(ind_r == self.num_bins_r): ind_r[ind_r >= self.num_bins_r] -= 1 np.add.at(Splus_D, (ind_r, ind_mu_r), (weight[n] * weight_shape[mask] * e_plus[mask]) / (2 * R)) np.add.at(Scross_D, (ind_r, ind_mu_r), (weight[n] * weight_shape[mask] * e_cross[mask]) / (2 * R)) del e_plus, e_cross, mu_r np.add.at(DD, (ind_r, ind_mu_r), weight[n] * weight_shape[mask]) # if Num_position == Num_shape: # corrtype = \"auto\" # DD = DD / 2.0 # auto correlation, all pairs are double # else: corrtype = \"cross\" # analytical calc is much more difficult for (r,mu_r) bins for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_g_plus[i, p] = self.get_random_pairs_r_mur( self.r_bins[i + 1], self.r_bins[i], self.mu_r_bins[p + 1], self.mu_r_bins[p], L3, \"cross\", Num_position, Num_shape) RR_gg[i, p] = self.get_random_pairs_r_mur( self.r_bins[i + 1], self.r_bins[i], self.mu_r_bins[p + 1], self.mu_r_bins[p], L3, corrtype, Num_position, Num_shape) correlation = Splus_D / RR_g_plus # (Splus_D - Splus_R) / RR_g_plus xi_g_cross = Scross_D / RR_g_plus # (Scross_D - Scross_R) / RR_g_plus dsep = (self.r_bins[1:] - self.r_bins[:-1]) / 2.0 separation_bins = self.r_bins[:-1] + abs(dsep) # middle of bins dmur = (self.mu_r_bins[1:] - self.mu_r_bins[:-1]) / 2.0 mu_r_bins = self.mu_r_bins[:-1] + abs(dmur) # middle of bins if (self.output_file_name != None) & return_output == False: output_file = h5py.File(self.output_file_name, \"a\") group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_plus/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=correlation) write_dataset_hdf5(group, dataset_name + \"_SplusD\", data=Splus_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_plus\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_cross/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=xi_g_cross) write_dataset_hdf5(group, dataset_name + \"_ScrossD\", data=Scross_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_cross\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_gg/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=(DD / RR_gg) - 1) write_dataset_hdf5(group, dataset_name + \"_DD\", data=DD) write_dataset_hdf5(group, dataset_name + \"_RR_gg\", data=RR_gg) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) output_file.close() return else: return correlation, (DD / RR_gg) - 1, separation_bins, mu_r_bins, Splus_D, DD, RR_g_plus def _measure_xi_r_mur_box_tree(self, dataset_name, masks=None, rp_cut=None, return_output=False, jk_group_name=\"\", ellipticity='distortion'): r\"\"\"Measures the projected correlation functions, $\\xi_{gg}$ and $\\xi_{g+}$, in (r, mu_r) bins for an object created with MeasureIABox. Uses 1 CPU. Uses KDTree for speedup. Parameters ---------- dataset_name : str Name of the dataset in the output file. masks : dict or NoneType, optional Dictionary with masks for the data to select only part of the data. Uses same keywords as data dictionary. Default value is None. rp_cut : float, optional Limit for minimum r_p value for pairs to be included. Default value is None. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. jk_group_name : str, optional Group in output file (hdf5) where jackknife realisations are stored. Default value is \"\". ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. Returns ------- ndarrays $\\xi_{gg}$ and $\\xi_{g+}$, r bins, mu_r bins, S+D, DD, RR (if no output file is specified) \"\"\" if masks == None: positions = self.data[\"Position\"] positions_shape_sample = self.data[\"Position_shape_sample\"] axis_direction_v = self.data[\"Axis_Direction\"] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"] weight = self.data[\"weight\"] weight_shape = self.data[\"weight_shape_sample\"] else: positions = self.data[\"Position\"][masks[\"Position\"]] positions_shape_sample = self.data[\"Position_shape_sample\"][masks[\"Position_shape_sample\"]] axis_direction_v = self.data[\"Axis_Direction\"][masks[\"Axis_Direction\"]] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"][masks[\"q\"]] try: weight_mask = masks[\"weight\"] except: masks[\"weight\"] = np.ones(self.Num_position, dtype=bool) masks[\"weight\"][sum(masks[\"Position\"]):self.Num_position] = 0 try: weight_mask = masks[\"weight_shape_sample\"] except: masks[\"weight_shape_sample\"] = np.ones(self.Num_shape, dtype=bool) masks[\"weight_shape_sample\"][sum(masks[\"Position_shape_sample\"]):self.Num_shape] = 0 weight = self.data[\"weight\"][masks[\"weight\"]] weight_shape = self.data[\"weight_shape_sample\"][masks[\"weight_shape_sample\"]] # masking changes the number of galaxies Num_position = len(positions) # number of halos in position sample Num_shape = len(positions_shape_sample) # number of halos in shape sample if rp_cut == None: rp_cut = 0.0 LOS_ind = self.data[\"LOS\"] # eg 2 for z axis not_LOS = np.array([0, 1, 2])[np.isin([0, 1, 2], LOS_ind, invert=True)] # eg 0,1 for x&y if ellipticity == 'distortion': e = (1 - q ** 2) / (1 + q ** 2) # size of ellipticity elif ellipticity == 'ellipticity': e = (1 - q) / (1 + q) else: raise ValueError(\"Invalid value for ellipticity. Choose 'distortion' or 'ellipticity'.\") R = sum(weight_shape * (1 - e ** 2 / 2.0)) / sum(weight_shape) # R = 1 - np.mean(e ** 2) / 2.0 # responsivity factor L3 = self.boxsize ** 3 # box volume sub_box_len_logr = (np.log10(self.r_max) - np.log10(self.r_min)) / self.num_bins_r sub_box_len_mu_r = 2.0 / self.num_bins_pi # mu_r ranges from -1 to 1. Same number of bins as pi DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_g_plus = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_gg = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) print( f\"There are {Num_shape} galaxies in the shape sample and {Num_position} galaxies in the position sample.\") pos_tree = KDTree(positions, boxsize=self.boxsize) for i in np.arange(0, len(positions_shape_sample), 100): i2 = min(len(positions_shape_sample), i + 100) positions_shape_sample_i = positions_shape_sample[i:i2] axis_direction_i = axis_direction[i:i2] e_i = e[i:i2] weight_shape_i = weight_shape[i:i2] shape_tree = KDTree(positions_shape_sample_i, boxsize=self.boxsize) ind_min_i = shape_tree.query_ball_tree(pos_tree, self.r_min) ind_max_i = shape_tree.query_ball_tree(pos_tree, self.r_max) ind_rbin_i = self.setdiff2D(ind_max_i, ind_min_i) for n in np.arange(0, len(positions_shape_sample_i)): if len(ind_rbin_i[n]) > 0: # for Splus_D (calculate ellipticities around position sample) separation = positions_shape_sample_i[n] - positions[ind_rbin_i[n]] if self.periodicity: separation[separation > self.L_0p5] -= self.boxsize # account for periodicity of box separation[separation < -self.L_0p5] += self.boxsize projected_sep = separation[:, not_LOS] LOS = separation[:, LOS_ind] projected_separation_len = np.sqrt(np.sum(projected_sep ** 2, axis=1)) with np.errstate(invalid='ignore'): separation_dir = ( projected_sep.transpose() / projected_separation_len).transpose() # normalisation of rp separation_len = np.sqrt(np.sum(separation ** 2, axis=1)) del separation, projected_sep with np.errstate(invalid='ignore'): mu_r = LOS / separation_len phi = np.arccos( separation_dir[:, 0] * axis_direction_i[n, 0] + separation_dir[:, 1] * axis_direction_i[ n, 1]) # [0,pi] e_plus, e_cross = self.get_ellipticity(e_i[n], phi) del phi, LOS, separation_dir e_plus[np.isnan(e_plus)] = 0.0 mu_r[np.isnan(e_plus)] = 0.0 e_cross[np.isnan(e_cross)] = 0.0 # get the indices for the binning mask = ( (projected_separation_len > rp_cut) * (separation_len >= self.r_bins[0]) * (separation_len < self.r_bins[-1]) ) ind_r = np.floor( np.log10(separation_len[mask]) / sub_box_len_logr - np.log10( self.r_bins[0]) / sub_box_len_logr ) ind_r = np.array(ind_r, dtype=int) ind_mu_r = np.floor( mu_r[mask] / sub_box_len_mu_r - self.mu_r_bins[0] / sub_box_len_mu_r ) # need length of LOS, so only positive values ind_mu_r = np.array(ind_mu_r, dtype=int) if np.any(ind_mu_r == self.num_bins_pi): ind_mu_r[ind_mu_r >= self.num_bins_pi] -= 1 if np.any(ind_r == self.num_bins_r): ind_r[ind_r >= self.num_bins_r] -= 1 np.add.at(Splus_D, (ind_r, ind_mu_r), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_plus[mask]) / (2 * R)) np.add.at(Scross_D, (ind_r, ind_mu_r), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_cross[mask]) / (2 * R)) np.add.at(DD, (ind_r, ind_mu_r), weight[ind_rbin_i[n]][mask] * weight_shape_i[n]) del e_plus, e_cross, mask, separation_len # if Num_position == Num_shape: # corrtype = \"auto\" # DD = DD / 2.0 # auto correlation, all pairs are double # else: corrtype = \"cross\" # analytical calc is much more difficult for (r,mu_r) bins for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_g_plus[i, p] = self.get_random_pairs_r_mur( self.r_bins[i + 1], self.r_bins[i], self.mu_r_bins[p + 1], self.mu_r_bins[p], L3, \"cross\", Num_position, Num_shape) RR_gg[i, p] = self.get_random_pairs_r_mur( self.r_bins[i + 1], self.r_bins[i], self.mu_r_bins[p + 1], self.mu_r_bins[p], L3, corrtype, Num_position, Num_shape) correlation = Splus_D / RR_g_plus # (Splus_D - Splus_R) / RR_g_plus xi_g_cross = Scross_D / RR_g_plus # (Scross_D - Scross_R) / RR_g_plus dsep = (self.r_bins[1:] - self.r_bins[:-1]) / 2.0 separation_bins = self.r_bins[:-1] + abs(dsep) # middle of bins dmur = (self.mu_r_bins[1:] - self.mu_r_bins[:-1]) / 2.0 mu_r_bins = self.mu_r_bins[:-1] + abs(dmur) # middle of bins if (self.output_file_name != None) & return_output == False: output_file = h5py.File(self.output_file_name, \"a\") group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_plus/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=correlation) write_dataset_hdf5(group, dataset_name + \"_SplusD\", data=Splus_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_plus\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_cross/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=xi_g_cross) write_dataset_hdf5(group, dataset_name + \"_ScrossD\", data=Scross_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_cross\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_gg/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=(DD / RR_gg) - 1) write_dataset_hdf5(group, dataset_name + \"_DD\", data=DD) write_dataset_hdf5(group, dataset_name + \"_RR_gg\", data=RR_gg) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) output_file.close() return else: return correlation, (DD / RR_gg) - 1, separation_bins, mu_r_bins, Splus_D, DD, RR_g_plus def _measure_xi_r_mur_box_batch(self, i): r\"\"\"Measures components of $\\xi_{gg}$ and $\\xi_{g+}$ in (r, mu_r) bins including jackknife realisations for a batch of indices from i to i+chunk_size. Support function for _measure_xi_r_mu_r_box_jk_multiprocessing(). Parameters ---------- i: int Start index of the batch. Returns ------- ndarrays S+D, SxD, DD, DD_jk, S+D_jk where the _jk versions store the necessary information of DD of S+D for each jackknife realisation. \"\"\" if i + self.chunk_size > self.Num_shape_masked: i2 = self.Num_shape_masked else: i2 = i + self.chunk_size DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) positions_shape_sample_i = self.temp_data_obj_m.read_cat(\"positions_shape_sample\", [i, i2]) axis_direction_i = self.temp_data_obj_m.read_cat(\"axis_direction\", [i, i2]) weight_shape_i = self.temp_data_obj_m.read_cat(\"weight_shape\", [i, i2]) positions = self.temp_data_obj_m.read_cat(\"positions\") weight = self.temp_data_obj_m.read_cat(\"weight\") e_i = self.e[i:i2] shape_tree = KDTree(positions_shape_sample_i, boxsize=self.boxsize) ind_min_i = shape_tree.query_ball_tree(self.pos_tree, self.r_min) ind_max_i = shape_tree.query_ball_tree(self.pos_tree, self.r_max) ind_rbin_i = self.setdiff2D(ind_max_i, ind_min_i) for n in np.arange(0, len(positions_shape_sample_i)): if len(ind_rbin_i[n]) > 0: # for Splus_D (calculate ellipticities around position sample) separation = positions_shape_sample_i[n] - positions[ind_rbin_i[n]] if self.periodicity: separation[separation > self.L_0p5] -= self.boxsize # account for periodicity of box separation[separation < -self.L_0p5] += self.boxsize projected_sep = separation[:, self.not_LOS] LOS = separation[:, self.LOS_ind] projected_separation_len = np.sqrt(np.sum(projected_sep ** 2, axis=1)) with np.errstate(invalid='ignore'): separation_dir = ( projected_sep.transpose() / projected_separation_len).transpose() # normalisation of rp separation_len = np.sqrt(np.sum(separation ** 2, axis=1)) del separation, projected_sep with np.errstate(invalid='ignore'): mu_r = LOS / separation_len phi = np.arccos( separation_dir[:, 0] * axis_direction_i[n, 0] + separation_dir[:, 1] * axis_direction_i[ n, 1]) # [0,pi] e_plus, e_cross = self.get_ellipticity(e_i[n], phi) del phi, LOS, separation_dir e_plus[np.isnan(e_plus)] = 0.0 mu_r[np.isnan(e_plus)] = 0.0 e_cross[np.isnan(e_cross)] = 0.0 # get the indices for the binning mask = ( (projected_separation_len > self.rp_cut) * (separation_len >= self.r_bins[0]) * (separation_len < self.r_bins[-1]) ) ind_r = np.floor( np.log10(separation_len[mask]) / self.sub_box_len_logr - np.log10( self.r_bins[0]) / self.sub_box_len_logr ) ind_r = np.array(ind_r, dtype=int) ind_mu_r = np.floor( mu_r[mask] / self.sub_box_len_mu_r - self.mu_r_bins[0] / self.sub_box_len_mu_r ) # need length of LOS, so only positive values ind_mu_r = np.array(ind_mu_r, dtype=int) if np.any(ind_mu_r == self.num_bins_pi): ind_mu_r[ind_mu_r >= self.num_bins_pi] -= 1 if np.any(ind_r == self.num_bins_r): ind_r[ind_r >= self.num_bins_r] -= 1 np.add.at(Splus_D, (ind_r, ind_mu_r), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_plus[mask]) / (2 * self.R)) np.add.at(Scross_D, (ind_r, ind_mu_r), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_cross[mask]) / (2 * self.R)) np.add.at(DD, (ind_r, ind_mu_r), weight[ind_rbin_i[n]][mask] * weight_shape_i[n]) del separation_len, e_cross, e_plus return Splus_D, Scross_D, DD def _measure_xi_r_mur_box_multiprocessing(self, dataset_name, temp_file_path, masks=None, rp_cut=None, return_output=False, jk_group_name=\"\", chunk_size=100, num_nodes=1, ellipticity='distortion'): r\"\"\"Measures the projected correlation functions, $\\xi_{gg}$ and $\\xi_{g+}$, in (r, mu_r) bins for an object created with MeasureIABox. Uses >1 CPU. Uses KDTree for speedup. Parameters ---------- dataset_name : str Name of the dataset in the output file. temp_file_path : str or NoneType, optional Path to where the data is temporarily stored [file name generated automatically]. num_nodes : int, optional Number of CPUs used in the multiprocessing. Default is 1. masks : dict or NoneType, optional Dictionary with masks for the data to select only part of the data. Uses same keywords as data dictionary. Default value = None. rp_cut : float, optional Limit for minimum r_p value for pairs to be included. Default value is None. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. jk_group_name : str, optional Group in output file (hdf5) where jackknife realisations are stored. Default value is \"\". ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. Returns ------- ndarrays $\\xi_{gg}$ and $\\xi_{g+}$, r bins, mu_r bins, S+D, DD, RR (if no output file is specified) \"\"\" if masks == None: positions = self.data[\"Position\"] positions_shape_sample = self.data[\"Position_shape_sample\"] axis_direction_v = self.data[\"Axis_Direction\"] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"] weight = self.data[\"weight\"] weight_shape = self.data[\"weight_shape_sample\"] else: positions = self.data[\"Position\"][masks[\"Position\"]] positions_shape_sample = self.data[\"Position_shape_sample\"][masks[\"Position_shape_sample\"]] axis_direction_v = self.data[\"Axis_Direction\"][masks[\"Axis_Direction\"]] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"][masks[\"q\"]] try: weight_mask = masks[\"weight\"] except: masks[\"weight\"] = np.ones(self.Num_position, dtype=bool) masks[\"weight\"][sum(masks[\"Position\"]):self.Num_position] = 0 try: weight_mask = masks[\"weight_shape_sample\"] except: masks[\"weight_shape_sample\"] = np.ones(self.Num_shape, dtype=bool) masks[\"weight_shape_sample\"][sum(masks[\"Position_shape_sample\"]):self.Num_shape] = 0 weight = self.data[\"weight\"][masks[\"weight\"]] weight_shape = self.data[\"weight_shape_sample\"][masks[\"weight_shape_sample\"]] # masking changes the number of galaxies self.Num_position_masked = len(positions) self.Num_shape_masked = len(positions_shape_sample) print( f\"There are {self.Num_shape_masked} galaxies in the shape sample and {self.Num_position_masked} galaxies in the position sample.\") # create temp hdf5 from which data can be read. del self.data, but save it in this method to reduce RAM figname_dataset_name = dataset_name if \"/\" in dataset_name: figname_dataset_name = figname_dataset_name.replace(\"/\", \"_\") if \".\" in dataset_name: figname_dataset_name = figname_dataset_name.replace(\".\", \"p\") file_temp = h5py.File(f\"{temp_file_path}/m_{self.simname}_temp_data_{figname_dataset_name}.hdf5\", \"w\") write_dataset_hdf5(file_temp, \"positions\", positions) write_dataset_hdf5(file_temp, \"weight\", weight) write_dataset_hdf5(file_temp, \"weight_shape\", weight_shape) write_dataset_hdf5(file_temp, \"positions_shape_sample\", positions_shape_sample) write_dataset_hdf5(file_temp, \"axis_direction\", axis_direction) file_temp.close() self.temp_data_obj_m = ReadData(self.simname, f\"m_{self.simname}_temp_data_{figname_dataset_name}\", None, data_path=temp_file_path) if rp_cut == None: self.rp_cut = 0.0 else: self.rp_cut = rp_cut self.LOS_ind = self.data[\"LOS\"] # eg 2 for z axis self.not_LOS = np.array([0, 1, 2])[np.isin([0, 1, 2], self.LOS_ind, invert=True)] # eg 0,1 for x&y if ellipticity == 'distortion': self.e = (1 - q ** 2) / (1 + q ** 2) # size of ellipticity elif ellipticity == 'ellipticity': self.e = (1 - q) / (1 + q) else: raise ValueError(\"Invalid value for ellipticity. Choose 'distortion' or 'ellipticity'.\") self.R = sum(weight_shape * (1 - self.e ** 2 / 2.0)) / sum(weight_shape) # self.R = 1 - np.mean(self.e ** 2) / 2.0 # responsivity factor L3 = self.boxsize ** 3 # box volume self.sub_box_len_logr = (np.log10(self.r_max) - np.log10(self.r_min)) / self.num_bins_r self.sub_box_len_mu_r = 2.0 / self.num_bins_pi # mu_r ranges from -1 to 1. Same number of bins as pi DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_g_plus = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_gg = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) data_temp = self.data # make sure data is not sent to every CPU self.data = None self.pos_tree = KDTree(positions, boxsize=self.boxsize) indices = np.arange(0, len(positions_shape_sample), chunk_size) self.chunk_size = chunk_size with Pool(num_nodes) as p: result = p.map(self._measure_xi_r_mur_box_batch, indices) os.remove( f\"{temp_file_path}/m_{self.simname}_temp_data_{figname_dataset_name}.hdf5\") self.data = data_temp del data_temp for i in np.arange(len(result)): Splus_D += result[i][0] Scross_D += result[i][1] DD += result[i][2] # if Num_position == Num_shape: # corrtype = \"auto\" # DD = DD / 2.0 # auto correlation, all pairs are double # else: corrtype = \"cross\" # analytical calc is much more difficult for (r,mu_r) bins for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_g_plus[i, p] = self.get_random_pairs_r_mur( self.r_bins[i + 1], self.r_bins[i], self.mu_r_bins[p + 1], self.mu_r_bins[p], L3, \"cross\", self.Num_position_masked, self.Num_shape_masked) RR_gg[i, p] = self.get_random_pairs_r_mur( self.r_bins[i + 1], self.r_bins[i], self.mu_r_bins[p + 1], self.mu_r_bins[p], L3, corrtype, self.Num_position_masked, self.Num_shape_masked) correlation = Splus_D / RR_g_plus # (Splus_D - Splus_R) / RR_g_plus xi_g_cross = Scross_D / RR_g_plus # (Scross_D - Scross_R) / RR_g_plus dsep = (self.r_bins[1:] - self.r_bins[:-1]) / 2.0 separation_bins = self.r_bins[:-1] + abs(dsep) # middle of bins dmur = (self.mu_r_bins[1:] - self.mu_r_bins[:-1]) / 2.0 mu_r_bins = self.mu_r_bins[:-1] + abs(dmur) # middle of bins if (self.output_file_name != None) & return_output == False: output_file = h5py.File(self.output_file_name, \"a\") group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_plus/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=correlation) write_dataset_hdf5(group, dataset_name + \"_SplusD\", data=Splus_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_plus\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_cross/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=xi_g_cross) write_dataset_hdf5(group, dataset_name + \"_ScrossD\", data=Scross_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_cross\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_gg/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=(DD / RR_gg) - 1) write_dataset_hdf5(group, dataset_name + \"_DD\", data=DD) write_dataset_hdf5(group, dataset_name + \"_RR_gg\", data=RR_gg) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) output_file.close() return else: return correlation, (DD / RR_gg) - 1, separation_bins, mu_r_bins, Splus_D, DD, RR_g_plus","title":"MeasureMultipolesBox"},{"location":"api/MeasureMBox/#measureia.MeasureMultipolesBox.__init__","text":"The init method of the MeasureMultipolesSimulations class. Notes Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. Source code in src/measureia/measure_m_box.py def __init__( self, data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True, ): \"\"\" The __init__ method of the MeasureMultipolesSimulations class. Notes ----- Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. \"\"\" super().__init__(data, output_file_name, simulation, snapshot, separation_limits, num_bins_r, num_bins_pi, pi_max, boxsize, periodicity) return","title":"__init__"},{"location":"api/MeasureMBoxJackknife/","text":"MeasureMBoxJackknife Bases: MeasureIABase , ReadData Class that contains all methods for the measurements of \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) for \\(\\tilde{\\xi}_{gg,0}\\) and \\(\\tilde{\\xi}_{g+,2}\\) including the jackknife realisations needed for the covariance estimation with Cartesian simulation data. Methods: Name Description _measure_xi_r_mur_box_jk_brute Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (r, mu_r) grid binning including jackknife realisations in a periodic box using 1 CPU. _measure_xi_r_mur_box_jk_tree Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (r, mu_r) grid binning including jackknife realisations in a periodic box using 1 CPU and KDTree for extra speed. _measure_xi_r_mur_box_jk_batch Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (r, mu_r) grid binning including jackknife realisations in a periodic box using 1 CPU for a batch of indices. Support function of _measure_xi_r_mur_box_jk_multiprocessing(). _measure_xi_r_mur_box_jk_multiprocessing Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (r, mu_r) grid binning including jackknife realisations in a periodic box using >1 CPUs. Notes Inherits attributes from 'SimInfo', where 'boxsize', 'L_0p5' and 'snap_group' are used in this class. Inherits attributes from 'MeasureIABase', where 'data', 'output_file_name', 'periodicity', 'Num_position', 'Num_shape', 'r_min', 'r_max', 'num_bins_r', 'num_bins_pi', 'r_bins', 'pi_bins', 'mu_r_bins' are used. Source code in src/measureia/measure_m_box_jk.py class MeasureMBoxJackknife(MeasureIABase, ReadData): r\"\"\"Class that contains all methods for the measurements of $\\xi_{gg}$ and $\\xi_{g+}$ for $\\tilde{\\xi}_{gg,0}$ and $\\tilde{\\xi}_{g+,2}$ including the jackknife realisations needed for the covariance estimation with Cartesian simulation data. Methods ------- _measure_xi_r_mur_box_jk_brute() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (r, mu_r) grid binning including jackknife realisations in a periodic box using 1 CPU. _measure_xi_r_mur_box_jk_tree() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (r, mu_r) grid binning including jackknife realisations in a periodic box using 1 CPU and KDTree for extra speed. _measure_xi_r_mur_box_jk_batch() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (r, mu_r) grid binning including jackknife realisations in a periodic box using 1 CPU for a batch of indices. Support function of _measure_xi_r_mur_box_jk_multiprocessing(). _measure_xi_r_mur_box_jk_multiprocessing() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (r, mu_r) grid binning including jackknife realisations in a periodic box using >1 CPUs. Notes ----- Inherits attributes from 'SimInfo', where 'boxsize', 'L_0p5' and 'snap_group' are used in this class. Inherits attributes from 'MeasureIABase', where 'data', 'output_file_name', 'periodicity', 'Num_position', 'Num_shape', 'r_min', 'r_max', 'num_bins_r', 'num_bins_pi', 'r_bins', 'pi_bins', 'mu_r_bins' are used. \"\"\" def __init__( self, data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True, ): \"\"\" The __init__ method of the MeasureWSimulations class. Notes ----- Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. \"\"\" super().__init__(data, output_file_name, simulation, snapshot, separation_limits, num_bins_r, num_bins_pi, pi_max, boxsize, periodicity) return def _measure_xi_r_mur_box_jk_brute(self, dataset_name, L_subboxes, masks=None, rp_cut=None, return_output=False, jk_group_name=\"\", ellipticity='distortion'): r\"\"\"Measures the projected correlation functions including jackknife realisations, $\\xi_{gg}$ and $\\xi_{g+}$, in (r, mu_r) bins for an object created with MeasureIABox. Uses 1 CPU. Parameters ---------- dataset_name : str Name of the dataset in the output file. L_subboxes: int Number of subboxes on one side of the box. L_subboxes^3 is the total number of jackknife realisations. masks : dict or NoneType, optional Dictionary with masks for the data to select only part of the data. Uses same keywords as data dictionary. Default value is None. rp_cut : float, optional Limit for minimum r_p value for pairs to be included. Default value is None. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. jk_group_name : str, optional Group in output file (hdf5) where jackknife realisations are stored. Default value is \"\". ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. Returns ------- ndarrays $\\xi_{gg}$ and $\\xi_{g+}$, r bins, mu_r bins, S+D, DD, RR (if no output file is specified) \"\"\" if masks == None: positions = self.data[\"Position\"] positions_shape_sample = self.data[\"Position_shape_sample\"] axis_direction_v = self.data[\"Axis_Direction\"] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"] weight = self.data[\"weight\"] weight_shape = self.data[\"weight_shape_sample\"] else: positions = self.data[\"Position\"][masks[\"Position\"]] positions_shape_sample = self.data[\"Position_shape_sample\"][masks[\"Position_shape_sample\"]] axis_direction_v = self.data[\"Axis_Direction\"][masks[\"Axis_Direction\"]] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"][masks[\"q\"]] try: weight_mask = masks[\"weight\"] except: masks[\"weight\"] = np.ones(self.Num_position, dtype=bool) masks[\"weight\"][sum(masks[\"Position\"]):self.Num_position] = 0 try: weight_mask = masks[\"weight_shape_sample\"] except: masks[\"weight_shape_sample\"] = np.ones(self.Num_shape, dtype=bool) masks[\"weight_shape_sample\"][sum(masks[\"Position_shape_sample\"]):self.Num_shape] = 0 weight = self.data[\"weight\"][masks[\"weight\"]] weight_shape = self.data[\"weight_shape_sample\"][masks[\"weight_shape_sample\"]] Num_position = len(positions) Num_shape = len(positions_shape_sample) print( f\"There are {Num_shape} galaxies in the shape sample and {Num_position} galaxies in the position sample.\") LOS_ind = self.data[\"LOS\"] # eg 2 for z axis not_LOS = np.array([0, 1, 2])[np.isin([0, 1, 2], LOS_ind, invert=True)] # eg 0,1 for x&y if ellipticity == 'distortion': e = (1 - q ** 2) / (1 + q ** 2) # size of ellipticity elif ellipticity == 'ellipticity': e = (1 - q) / (1 + q) else: raise ValueError(\"Invalid value for ellipticity. Choose 'distortion' or 'ellipticity'.\") del q R = sum(weight_shape * (1 - e ** 2 / 2.0)) / sum(weight_shape) # R = 1 - np.mean(e ** 2) / 2.0 # responsitivity factor L3 = self.boxsize ** 3 # box volume if rp_cut == None: rp_cut = 0.0 sub_box_len_logr = (np.log10(self.r_max) - np.log10(self.r_min)) / self.num_bins_r sub_box_len_mu_r = 2.0 / self.num_bins_pi # mu_r ranges from -1 to 1. Same number of bins as pi DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_g_plus = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_gg = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) variance = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) jackknife_region_indices_pos, jackknife_region_indices_shape = self._get_jackknife_region_indices(masks, L_subboxes) num_box = L_subboxes ** 3 DD_jk = np.zeros((num_box, self.num_bins_r, self.num_bins_pi)) Splus_D_jk = np.zeros((num_box, self.num_bins_r, self.num_bins_pi)) for n in np.arange(0, len(positions)): separation = positions_shape_sample - positions[n] if self.periodicity: separation[separation > self.L_0p5] -= self.boxsize # account for periodicity of box separation[separation < -self.L_0p5] += self.boxsize projected_sep = separation[:, not_LOS] LOS = separation[:, LOS_ind] projected_separation_len = np.sqrt(np.sum(projected_sep ** 2, axis=1)) with np.errstate(invalid='ignore'): separation_dir = ( projected_sep.transpose() / projected_separation_len).transpose() # normalisation of rp separation_len = np.sqrt(np.sum(separation ** 2, axis=1)) with np.errstate(invalid='ignore'): mu_r = LOS / separation_len del LOS, projected_sep, separation phi = np.arccos(self.calculate_dot_product_arrays(separation_dir, axis_direction)) # [0,pi] e_plus, e_cross = self.get_ellipticity(e, phi) del phi, separation_dir e_plus[np.isnan(e_plus)] = 0.0 e_cross[np.isnan(e_cross)] = 0.0 mu_r[np.isnan(e_plus)] = 0.0 # get the indices for the binning mask = ( (projected_separation_len > rp_cut) * (separation_len >= self.r_bins[0]) * (separation_len < self.r_bins[-1]) ) ind_r = np.floor( np.log10(separation_len[mask]) / sub_box_len_logr - np.log10(self.r_bins[0]) / sub_box_len_logr ) del separation_len, projected_separation_len ind_r = np.array(ind_r, dtype=int) ind_mu_r = np.floor( mu_r[mask] / sub_box_len_mu_r - self.mu_r_bins[0] / sub_box_len_mu_r ) # need length of LOS, so only positive values ind_mu_r = np.array(ind_mu_r, dtype=int) if np.any(ind_mu_r == self.num_bins_pi): ind_mu_r[ind_mu_r >= self.num_bins_pi] -= 1 if np.any(ind_r == self.num_bins_r): ind_r[ind_r >= self.num_bins_r] -= 1 np.add.at(Splus_D, (ind_r, ind_mu_r), (weight[n] * weight_shape[mask] * e_plus[mask]) / (2 * R)) np.add.at(Scross_D, (ind_r, ind_mu_r), (weight[n] * weight_shape[mask] * e_cross[mask]) / (2 * R)) np.add.at(variance, (ind_r, ind_mu_r), ((weight[n] * weight_shape[mask] * e_plus[mask]) / (2 * R)) ** 2) shape_mask = np.where(jackknife_region_indices_shape[mask] != jackknife_region_indices_pos[n])[0] np.add.at(Splus_D_jk, (jackknife_region_indices_pos[n], ind_r, ind_mu_r), (weight[n] * weight_shape[mask] * e_plus[mask])) # responsivity added later np.add.at(Splus_D_jk, (jackknife_region_indices_shape[mask][shape_mask], ind_r[shape_mask], ind_mu_r[shape_mask]), (weight[n] * weight_shape[mask][shape_mask] * e_plus[mask][ shape_mask])) # responsivity added later del e_plus, e_cross np.add.at(DD, (ind_r, ind_mu_r), weight[n] * weight_shape[mask]) np.add.at(DD_jk, (jackknife_region_indices_pos[n], ind_r, ind_mu_r), (weight[n] * weight_shape[mask])) np.add.at(DD_jk, (jackknife_region_indices_shape[mask][shape_mask], ind_r[shape_mask], ind_mu_r[shape_mask]), (weight[n] * weight_shape[mask][shape_mask])) R_jk = np.zeros(num_box) for i in np.arange(num_box): jk_mask = np.where(jackknife_region_indices_shape != i) R_jk[i] = sum(weight_shape[jk_mask] * (1 - e[jk_mask] ** 2 / 2.0)) / sum(weight_shape[jk_mask]) corrtype = \"cross\" for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_g_plus[i, p] = self.get_random_pairs_r_mur( self.r_bins[i + 1], self.r_bins[i], self.mu_r_bins[p + 1], self.mu_r_bins[p], L3, \"cross\", Num_position, Num_shape) RR_gg[i, p] = self.get_random_pairs_r_mur( self.r_bins[i + 1], self.r_bins[i], self.mu_r_bins[p + 1], self.mu_r_bins[p], L3, corrtype, Num_position, Num_shape) RR_jk = np.zeros((num_box, self.num_bins_r, self.num_bins_pi)) volume_jk = L3 * (num_box - 1) / (num_box) for jk in np.arange(num_box): Num_position_jk, Num_shape_jk = len(np.where(jackknife_region_indices_pos != jk)[0]), len( np.where(jackknife_region_indices_shape != jk)[0]) for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_jk[jk, i, p] = self.get_random_pairs_r_mur( self.r_bins[i + 1], self.r_bins[i], self.mu_r_bins[p + 1], self.mu_r_bins[p], volume_jk, \"cross\", Num_position_jk, Num_shape_jk) correlation = Splus_D / RR_g_plus # (Splus_D - Splus_R) / RR_g_plus xi_g_cross = Scross_D / RR_g_plus # (Scross_D - Scross_R) / RR_g_plus sigsq = variance / RR_g_plus ** 2 dsep = (self.r_bins[1:] - self.r_bins[:-1]) / 2.0 separation_bins = self.r_bins[:-1] + abs(dsep) # middle of bins dmur = (self.mu_r_bins[1:] - self.mu_r_bins[:-1]) / 2.0 mu_r_bins = self.mu_r_bins[:-1] + abs(dmur) # middle of bins if (self.output_file_name != None) and (return_output == False): output_file = h5py.File(self.output_file_name, \"a\") group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_plus/\") write_dataset_hdf5(group, dataset_name, data=correlation) write_dataset_hdf5(group, dataset_name + \"_SplusD\", data=Splus_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_plus\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_sigmasq\", data=sigsq) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_plus/{jk_group_name}\") for i in np.arange(0, num_box): corr = (Splus_D * (2 * R) - Splus_D_jk[i]) / ( RR_jk[i] * 2 * R_jk[i]) # Responsivity will be different for each realisation write_dataset_hdf5(group, dataset_name + f\"_{i}\", data=corr) write_dataset_hdf5(group, dataset_name + f\"_{i}_SplusD\", data=(Splus_D * (2 * R) - Splus_D_jk[i]) / ( 2 * R_jk[i])) # Splus_D_jk[i]/(2*R_jk[i])) write_dataset_hdf5(group, dataset_name + f\"_{i}_RR\", data=RR_jk[i]) write_dataset_hdf5(group, dataset_name + f\"_{i}_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + f\"_{i}_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_cross/{jk_group_name}\") write_dataset_hdf5(group, dataset_name + \"_ScrossD\", data=Scross_D) write_dataset_hdf5(group, dataset_name, data=xi_g_cross) write_dataset_hdf5(group, dataset_name + \"_RR_g_cross\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_sigmasq\", data=sigsq) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_gg/\") write_dataset_hdf5(group, dataset_name, data=(DD / RR_gg) - 1) write_dataset_hdf5(group, dataset_name + \"_DD\", data=DD) write_dataset_hdf5(group, dataset_name + \"_RR_gg\", data=RR_gg) write_dataset_hdf5(group, dataset_name + \"_sigmasq\", data=sigsq) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_gg/{jk_group_name}\") for i in np.arange(0, num_box): write_dataset_hdf5(group, dataset_name + f\"_{i}\", data=((DD - DD_jk[i]) / RR_jk[i]) - 1) write_dataset_hdf5(group, dataset_name + f\"_{i}_DD\", data=(DD - DD_jk[i])) write_dataset_hdf5(group, dataset_name + f\"_{i}_RR\", data=RR_jk[i]) write_dataset_hdf5(group, dataset_name + f\"_{i}_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + f\"_{i}_mu_r\", data=mu_r_bins) output_file.close() return else: return correlation, (DD / RR_gg) - 1, separation_bins, mu_r_bins, Splus_D, DD, RR_g_plus def _measure_xi_r_mur_box_jk_tree(self, dataset_name, L_subboxes, masks=None, rp_cut=None, return_output=False, jk_group_name=\"\", ellipticity='distortion'): r\"\"\"Measures the projected correlation functions including jackknife realisations, $\\xi_{gg}$ and $\\xi_{g+}$, in (r, mu_r) bins for an object created with MeasureIABox. Uses 1 CPU. Uses KDTree for speedup. Parameters ---------- dataset_name : str Name of the dataset in the output file. L_subboxes: int Number of subboxes on one side of the box. L_subboxes^3 is the total number of jackknife realisations. masks : dict or NoneType, optional Dictionary with masks for the data to select only part of the data. Uses same keywords as data dictionary. Default value = None. rp_cut : float, optional Limit for minimum r_p value for pairs to be included. Default value is None. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. jk_group_name : str, optional Group in output file (hdf5) where jackknife realisations are stored. Default value is \"\". ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. Returns ------- ndarrays $\\xi_{gg}$ and $\\xi_{g+}$, r bins, mu_r bins, S+D, DD, RR (if no output file is specified) \"\"\" if masks == None: positions = self.data[\"Position\"] positions_shape_sample = self.data[\"Position_shape_sample\"] axis_direction_v = self.data[\"Axis_Direction\"] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"] weight = self.data[\"weight\"] weight_shape = self.data[\"weight_shape_sample\"] else: positions = self.data[\"Position\"][masks[\"Position\"]] positions_shape_sample = self.data[\"Position_shape_sample\"][masks[\"Position_shape_sample\"]] axis_direction_v = self.data[\"Axis_Direction\"][masks[\"Axis_Direction\"]] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"][masks[\"q\"]] try: weight_mask = masks[\"weight\"] except: masks[\"weight\"] = np.ones(self.Num_position, dtype=bool) masks[\"weight\"][sum(masks[\"Position\"]):self.Num_position] = 0 try: weight_mask = masks[\"weight_shape_sample\"] except: masks[\"weight_shape_sample\"] = np.ones(self.Num_shape, dtype=bool) masks[\"weight_shape_sample\"][sum(masks[\"Position_shape_sample\"]):self.Num_shape] = 0 weight = self.data[\"weight\"][masks[\"weight\"]] weight_shape = self.data[\"weight_shape_sample\"][masks[\"weight_shape_sample\"]] Num_position = len(positions) Num_shape = len(positions_shape_sample) print( f\"There are {Num_shape} galaxies in the shape sample and {Num_position} galaxies in the position sample.\") LOS_ind = self.data[\"LOS\"] # eg 2 for z axis not_LOS = np.array([0, 1, 2])[np.isin([0, 1, 2], LOS_ind, invert=True)] # eg 0,1 for x&y if ellipticity == 'distortion': e = (1 - q ** 2) / (1 + q ** 2) # size of ellipticity elif ellipticity == 'ellipticity': e = (1 - q) / (1 + q) else: raise ValueError(\"Invalid value for ellipticity. Choose 'distortion' or 'ellipticity'.\") del q R = sum(weight_shape * (1 - e ** 2 / 2.0)) / sum(weight_shape) # R = 1 - np.mean(e ** 2) / 2.0 # responsitivity factor L3 = self.boxsize ** 3 # box volume if rp_cut == None: rp_cut = 0.0 sub_box_len_logr = (np.log10(self.r_max) - np.log10(self.r_min)) / self.num_bins_r sub_box_len_mu_r = 2.0 / self.num_bins_pi # mu_r ranges from -1 to 1. Same number of bins as pi DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_g_plus = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_gg = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) variance = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) jackknife_region_indices_pos, jackknife_region_indices_shape = self._get_jackknife_region_indices(masks, L_subboxes) num_box = L_subboxes ** 3 DD_jk = np.zeros((num_box, self.num_bins_r, self.num_bins_pi)) Splus_D_jk = np.zeros((num_box, self.num_bins_r, self.num_bins_pi)) figname_dataset_name = dataset_name if \"/\" in dataset_name: figname_dataset_name = figname_dataset_name.replace(\"/\", \"_\") if \".\" in dataset_name: figname_dataset_name = figname_dataset_name.replace(\".\", \"p\") pos_tree = KDTree(positions, boxsize=self.boxsize) for i in np.arange(0, len(positions_shape_sample), 100): i2 = min(len(positions_shape_sample), i + 100) positions_shape_sample_i = positions_shape_sample[i:i2] axis_direction_i = axis_direction[i:i2] e_i = e[i:i2] weight_shape_i = weight_shape[i:i2] jackknife_region_indices_shape_i = jackknife_region_indices_shape[i:i2] shape_tree = KDTree(positions_shape_sample_i, boxsize=self.boxsize) ind_min_i = shape_tree.query_ball_tree(pos_tree, self.r_min) ind_max_i = shape_tree.query_ball_tree(pos_tree, self.r_max) ind_rbin_i = self.setdiff2D(ind_max_i, ind_min_i) for n in np.arange(0, len(positions_shape_sample_i)): if len(ind_rbin_i[n]) > 0: # for Splus_D (calculate ellipticities around position sample) separation = positions_shape_sample_i[n] - positions[ind_rbin_i[n]] if self.periodicity: separation[separation > self.L_0p5] -= self.boxsize # account for periodicity of box separation[separation < -self.L_0p5] += self.boxsize projected_sep = separation[:, not_LOS] LOS = separation[:, LOS_ind] projected_separation_len = np.sqrt(np.sum(projected_sep ** 2, axis=1)) with np.errstate(invalid='ignore'): separation_dir = ( projected_sep.transpose() / projected_separation_len).transpose() # normalisation of rp separation_len = np.sqrt(np.sum(separation ** 2, axis=1)) del separation, projected_sep with np.errstate(invalid='ignore'): mu_r = LOS / separation_len phi = np.arccos( separation_dir[:, 0] * axis_direction_i[n, 0] + separation_dir[:, 1] * axis_direction_i[ n, 1]) # [0,pi] e_plus, e_cross = self.get_ellipticity(e_i[n], phi) del phi, LOS, separation_dir e_plus[np.isnan(e_plus)] = 0.0 mu_r[np.isnan(e_plus)] = 0.0 e_cross[np.isnan(e_cross)] = 0.0 # get the indices for the binning mask = ( (projected_separation_len > rp_cut) * (separation_len >= self.r_bins[0]) * (separation_len < self.r_bins[-1]) ) ind_r = np.floor( np.log10(separation_len[mask]) / sub_box_len_logr - np.log10( self.r_bins[0]) / sub_box_len_logr ) ind_r = np.array(ind_r, dtype=int) ind_mu_r = np.floor( mu_r[mask] / sub_box_len_mu_r - self.mu_r_bins[0] / sub_box_len_mu_r ) # need length of LOS, so only positive values ind_mu_r = np.array(ind_mu_r, dtype=int) if np.any(ind_mu_r == self.num_bins_pi): ind_mu_r[ind_mu_r >= self.num_bins_pi] -= 1 if np.any(ind_r == self.num_bins_r): ind_r[ind_r >= self.num_bins_r] -= 1 np.add.at(Splus_D, (ind_r, ind_mu_r), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_plus[mask]) / (2 * R)) np.add.at(Scross_D, (ind_r, ind_mu_r), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_cross[mask]) / (2 * R)) del separation_len np.add.at(DD, (ind_r, ind_mu_r), weight[ind_rbin_i[n]][mask] * weight_shape_i[n]) pos_mask = \\ np.where( jackknife_region_indices_pos[ind_rbin_i[n]][mask] != jackknife_region_indices_shape_i[n])[ 0] np.add.at(Splus_D_jk, (jackknife_region_indices_shape_i[n], ind_r, ind_mu_r), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_plus[ mask])) # responsivity added later np.add.at(Splus_D_jk, (jackknife_region_indices_pos[ind_rbin_i[n]][mask][pos_mask], ind_r[pos_mask], ind_mu_r[pos_mask]), (weight[ind_rbin_i[n]][mask][pos_mask] * weight_shape_i[n] * e_plus[mask][ pos_mask])) # responsivity added later del e_plus, e_cross np.add.at(DD_jk, (jackknife_region_indices_shape_i[n], ind_r, ind_mu_r), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n])) # responsivity added later np.add.at(DD_jk, (jackknife_region_indices_pos[ind_rbin_i[n]][mask][pos_mask], ind_r[pos_mask], ind_mu_r[pos_mask]), (weight[ind_rbin_i[n]][mask][pos_mask] * weight_shape_i[n])) # responsivity added later R_jk = np.zeros(num_box) for i in np.arange(num_box): jk_mask = np.where(jackknife_region_indices_shape != i) R_jk[i] = sum(weight_shape[jk_mask] * (1 - e[jk_mask] ** 2 / 2.0)) / sum(weight_shape[jk_mask]) corrtype = \"cross\" for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_g_plus[i, p] = self.get_random_pairs_r_mur( self.r_bins[i + 1], self.r_bins[i], self.mu_r_bins[p + 1], self.mu_r_bins[p], L3, \"cross\", Num_position, Num_shape) RR_gg[i, p] = self.get_random_pairs_r_mur( self.r_bins[i + 1], self.r_bins[i], self.mu_r_bins[p + 1], self.mu_r_bins[p], L3, corrtype, Num_position, Num_shape) RR_jk = np.zeros((num_box, self.num_bins_r, self.num_bins_pi)) volume_jk = L3 * (num_box - 1) / num_box for jk in np.arange(num_box): Num_position_jk, Num_shape_jk = len(np.where(jackknife_region_indices_pos != jk)[0]), len( np.where(jackknife_region_indices_shape != jk)[0]) for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_jk[jk, i, p] = self.get_random_pairs_r_mur( self.r_bins[i + 1], self.r_bins[i], self.mu_r_bins[p + 1], self.mu_r_bins[p], volume_jk, \"cross\", Num_position_jk, Num_shape_jk) correlation = Splus_D / RR_g_plus # (Splus_D - Splus_R) / RR_g_plus xi_g_cross = Scross_D / RR_g_plus # (Scross_D - Scross_R) / RR_g_plus sigsq = variance / RR_g_plus ** 2 dsep = (self.r_bins[1:] - self.r_bins[:-1]) / 2.0 separation_bins = self.r_bins[:-1] + abs(dsep) # middle of bins dmur = (self.mu_r_bins[1:] - self.mu_r_bins[:-1]) / 2.0 mu_r_bins = self.mu_r_bins[:-1] + abs(dmur) # middle of bins if (self.output_file_name != None) and (return_output == False): output_file = h5py.File(self.output_file_name, \"a\") group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_plus/\") write_dataset_hdf5(group, dataset_name, data=correlation) write_dataset_hdf5(group, dataset_name + \"_SplusD\", data=Splus_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_plus\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_sigmasq\", data=sigsq) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_plus/{jk_group_name}\") for i in np.arange(0, num_box): corr = (Splus_D * (2 * R) - Splus_D_jk[i]) / ( RR_jk[i] * 2 * R_jk[i]) # Responsivity will be different for each realisation write_dataset_hdf5(group, dataset_name + f\"_{i}\", data=corr) write_dataset_hdf5(group, dataset_name + f\"_{i}_SplusD\", data=(Splus_D * (2 * R) - Splus_D_jk[i]) / ( 2 * R_jk[i])) # Splus_D_jk[i]/(2*R_jk[i])) write_dataset_hdf5(group, dataset_name + f\"_{i}_RR\", data=RR_jk[i]) write_dataset_hdf5(group, dataset_name + f\"_{i}_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + f\"_{i}_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_cross/{jk_group_name}\") write_dataset_hdf5(group, dataset_name + \"_ScrossD\", data=Scross_D) write_dataset_hdf5(group, dataset_name, data=xi_g_cross) write_dataset_hdf5(group, dataset_name + \"_RR_g_cross\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_sigmasq\", data=sigsq) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_gg/\") write_dataset_hdf5(group, dataset_name, data=(DD / RR_gg) - 1) write_dataset_hdf5(group, dataset_name + \"_DD\", data=DD) write_dataset_hdf5(group, dataset_name + \"_RR_gg\", data=RR_gg) write_dataset_hdf5(group, dataset_name + \"_sigmasq\", data=sigsq) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_gg/{jk_group_name}\") for i in np.arange(0, num_box): write_dataset_hdf5(group, dataset_name + f\"_{i}\", data=((DD - DD_jk[i]) / RR_jk[i]) - 1) write_dataset_hdf5(group, dataset_name + f\"_{i}_DD\", data=(DD - DD_jk[i])) write_dataset_hdf5(group, dataset_name + f\"_{i}_RR\", data=RR_jk[i]) write_dataset_hdf5(group, dataset_name + f\"_{i}_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + f\"_{i}_mu_r\", data=mu_r_bins) output_file.close() return else: return correlation, (DD / RR_gg) - 1, separation_bins, mu_r_bins, Splus_D, DD, RR_g_plus def _measure_xi_r_mur_box_jk_batch(self, i): r\"\"\"Measures components of $\\xi_{gg}$ and $\\xi_{g+}$ in (r, mu_r) bins including jackknife realisations for a batch of indices from i to i+chunk_size. Support function for _measure_xi_r_mu_r_box_jk_multiprocessing(). Parameters ---------- i: int Start index of the batch. Returns ------- ndarrays S+D, SxD, DD, DD_jk, S+D_jk where the _jk versions store the necessary information of DD of S+D for each jackknife realisation. \"\"\" if i + self.chunk_size > self.Num_shape_masked: i2 = self.Num_shape_masked else: i2 = i + self.chunk_size DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) DD_jk = np.zeros((self.num_box, self.num_bins_r, self.num_bins_pi)) Splus_D_jk = np.zeros((self.num_box, self.num_bins_r, self.num_bins_pi)) positions_shape_sample_i = self.temp_data_obj_m.read_cat(\"positions_shape_sample\", [i, i2]) axis_direction_i = self.temp_data_obj_m.read_cat(\"axis_direction\", [i, i2]) weight_shape_i = self.temp_data_obj_m.read_cat(\"weight_shape\", [i, i2]) positions = self.temp_data_obj_m.read_cat(\"positions\") weight = self.temp_data_obj_m.read_cat(\"weight\") e_i = self.e[i:i2] jackknife_region_indices_shape_i = self.jackknife_region_indices_shape[i:i2] shape_tree = KDTree(positions_shape_sample_i, boxsize=self.boxsize) ind_min_i = shape_tree.query_ball_tree(self.pos_tree, self.r_min) ind_max_i = shape_tree.query_ball_tree(self.pos_tree, self.r_max) ind_rbin_i = self.setdiff2D(ind_max_i, ind_min_i) for n in np.arange(0, len(positions_shape_sample_i)): if len(ind_rbin_i[n]) > 0: # for Splus_D (calculate ellipticities around position sample) separation = positions_shape_sample_i[n] - positions[ind_rbin_i[n]] if self.periodicity: separation[separation > self.L_0p5] -= self.boxsize # account for periodicity of box separation[separation < -self.L_0p5] += self.boxsize projected_sep = separation[:, self.not_LOS] LOS = separation[:, self.LOS_ind] projected_separation_len = np.sqrt(np.sum(projected_sep ** 2, axis=1)) with np.errstate(invalid='ignore'): separation_dir = ( projected_sep.transpose() / projected_separation_len).transpose() # normalisation of rp separation_len = np.sqrt(np.sum(separation ** 2, axis=1)) del separation, projected_sep with np.errstate(invalid='ignore'): mu_r = LOS / separation_len phi = np.arccos( separation_dir[:, 0] * axis_direction_i[n, 0] + separation_dir[:, 1] * axis_direction_i[ n, 1]) # [0,pi] e_plus, e_cross = self.get_ellipticity(e_i[n], phi) del phi, LOS, separation_dir e_plus[np.isnan(e_plus)] = 0.0 mu_r[np.isnan(e_plus)] = 0.0 e_cross[np.isnan(e_cross)] = 0.0 # get the indices for the binning mask = ( (projected_separation_len > self.rp_cut) * (separation_len >= self.r_bins[0]) * (separation_len < self.r_bins[-1]) ) ind_r = np.floor( np.log10(separation_len[mask]) / self.sub_box_len_logr - np.log10( self.r_bins[0]) / self.sub_box_len_logr ) ind_r = np.array(ind_r, dtype=int) ind_mu_r = np.floor( mu_r[mask] / self.sub_box_len_mu_r - self.mu_r_bins[0] / self.sub_box_len_mu_r ) # need length of LOS, so only positive values ind_mu_r = np.array(ind_mu_r, dtype=int) if np.any(ind_mu_r == self.num_bins_pi): ind_mu_r[ind_mu_r >= self.num_bins_pi] -= 1 if np.any(ind_r == self.num_bins_r): ind_r[ind_r >= self.num_bins_r] -= 1 np.add.at(Splus_D, (ind_r, ind_mu_r), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_plus[mask]) / (2 * self.R)) np.add.at(Scross_D, (ind_r, ind_mu_r), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_cross[mask]) / (2 * self.R)) np.add.at(DD, (ind_r, ind_mu_r), weight[ind_rbin_i[n]][mask] * weight_shape_i[n]) del separation_len pos_mask = \\ np.where( self.jackknife_region_indices_pos[ind_rbin_i[n]][mask] != jackknife_region_indices_shape_i[n])[ 0] np.add.at(Splus_D_jk, (jackknife_region_indices_shape_i[n], ind_r, ind_mu_r), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_plus[ mask])) # responsivity added later np.add.at(Splus_D_jk, (self.jackknife_region_indices_pos[ind_rbin_i[n]][mask][pos_mask], ind_r[pos_mask], ind_mu_r[pos_mask]), (weight[ind_rbin_i[n]][mask][pos_mask] * weight_shape_i[n] * e_plus[mask][ pos_mask])) # responsivity added later del e_plus, e_cross np.add.at(DD_jk, (jackknife_region_indices_shape_i[n], ind_r, ind_mu_r), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n])) # responsivity added later np.add.at(DD_jk, (self.jackknife_region_indices_pos[ind_rbin_i[n]][mask][pos_mask], ind_r[pos_mask], ind_mu_r[pos_mask]), (weight[ind_rbin_i[n]][mask][pos_mask] * weight_shape_i[n])) # responsivity added later return Splus_D, Scross_D, DD, DD_jk, Splus_D_jk def _measure_xi_r_mur_box_jk_multiprocessing(self, dataset_name, L_subboxes, file_tree_path, masks=None, rp_cut=None, return_output=False, jk_group_name=\"\", chunk_size=100, num_nodes=1, ellipticity='distortion'): r\"\"\"Measures the projected correlation functions including jackknife realisations, $\\xi_{gg}$ and $\\xi_{g+}$, in (r, mu_r) bins for an object created with MeasureIABox. Uses >1 CPU. Uses KDTree for speedup. Parameters ---------- dataset_name : str Name of the dataset in the output file. L_subboxes: int Number of subboxes on one side of the box. L_subboxes^3 is the total number of jackknife realisations. temp_file_path : str or NoneType, optional Path to where the data is temporarily stored [file name generated automatically]. masks : dict or NoneType, optional Dictionary with masks for the data to select only part of the data. Uses same keywords as data dictionary. Default value = None. rp_cut : float, optional Limit for minimum r_p value for pairs to be included. Default value is None. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. jk_group_name : str, optional Group in output file (hdf5) where jackknife realisations are stored. Default value is \"\". chunk_size: int, optional Size of the chunks of data sent to each multiprocessing node. If larger, more RAM is needed per node. Default is 1000. num_nodes : int, optional Number of CPUs used in the multiprocessing. Default is 1. ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. Returns ------- ndarrays $\\xi_{gg}$ and $\\xi_{g+}$, r bins, mu_r bins, S+D, DD, RR (if no output file is specified) \"\"\" if masks == None: positions = self.data[\"Position\"] positions_shape_sample = self.data[\"Position_shape_sample\"] axis_direction_v = self.data[\"Axis_Direction\"] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"] weight = self.data[\"weight\"] weight_shape = self.data[\"weight_shape_sample\"] else: positions = self.data[\"Position\"][masks[\"Position\"]] positions_shape_sample = self.data[\"Position_shape_sample\"][masks[\"Position_shape_sample\"]] axis_direction_v = self.data[\"Axis_Direction\"][masks[\"Axis_Direction\"]] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"][masks[\"q\"]] try: weight_mask = masks[\"weight\"] except: masks[\"weight\"] = np.ones(self.Num_position, dtype=bool) masks[\"weight\"][sum(masks[\"Position\"]):self.Num_position] = 0 try: weight_mask = masks[\"weight_shape_sample\"] except: masks[\"weight_shape_sample\"] = np.ones(self.Num_shape, dtype=bool) masks[\"weight_shape_sample\"][sum(masks[\"Position_shape_sample\"]):self.Num_shape] = 0 weight = self.data[\"weight\"][masks[\"weight\"]] weight_shape = self.data[\"weight_shape_sample\"][masks[\"weight_shape_sample\"]] self.Num_position_masked = len(positions) self.Num_shape_masked = len(positions_shape_sample) print( f\"There are {self.Num_shape_masked} galaxies in the shape sample and {self.Num_position_masked} galaxies in the position sample.\") # create temp hdf5 from which data can be read. del self.data, but save it in this method to reduce RAM figname_dataset_name = dataset_name if \"/\" in dataset_name: figname_dataset_name = figname_dataset_name.replace(\"/\", \"_\") if \".\" in dataset_name: figname_dataset_name = figname_dataset_name.replace(\".\", \"p\") file_temp = h5py.File(f\"{file_tree_path}/m_{self.simname}_temp_data_{figname_dataset_name}.hdf5\", \"w\") write_dataset_hdf5(file_temp, \"positions\", positions) write_dataset_hdf5(file_temp, \"weight\", weight) write_dataset_hdf5(file_temp, \"weight_shape\", weight_shape) write_dataset_hdf5(file_temp, \"positions_shape_sample\", positions_shape_sample) write_dataset_hdf5(file_temp, \"axis_direction\", axis_direction) file_temp.close() self.temp_data_obj_m = ReadData(self.simname, f\"m_{self.simname}_temp_data_{figname_dataset_name}\", None, data_path=file_tree_path) self.LOS_ind = self.data[\"LOS\"] # eg 2 for z axis self.not_LOS = np.array([0, 1, 2])[np.isin([0, 1, 2], self.LOS_ind, invert=True)] # eg 0,1 for x&y if ellipticity == 'distortion': self.e = (1 - q ** 2) / (1 + q ** 2) # size of ellipticity elif ellipticity == 'ellipticity': self.e = (1 - q) / (1 + q) else: raise ValueError(\"Invalid value for ellipticity. Choose 'distortion' or 'ellipticity'.\") del q self.R = sum(weight_shape * (1 - self.e ** 2 / 2.0)) / sum(weight_shape) # R = 1 - np.mean(e ** 2) / 2.0 # responsitivity factor L3 = self.boxsize ** 3 # box volume if rp_cut == None: self.rp_cut = 0.0 else: self.rp_cut = rp_cut self.sub_box_len_logr = (np.log10(self.r_max) - np.log10(self.r_min)) / self.num_bins_r self.sub_box_len_mu_r = 2.0 / self.num_bins_pi # mu_r ranges from -1 to 1. Same number of bins as pi DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_g_plus = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_gg = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) self.jackknife_region_indices_pos, self.jackknife_region_indices_shape = self._get_jackknife_region_indices( masks, L_subboxes) self.num_box = L_subboxes ** 3 DD_jk = np.zeros((self.num_box, self.num_bins_r, self.num_bins_pi)) Splus_D_jk = np.zeros((self.num_box, self.num_bins_r, self.num_bins_pi)) data_temp = self.data # make sure data is not sent to every CPU self.data = None self.pos_tree = KDTree(positions, boxsize=self.boxsize) indices = np.arange(0, len(positions_shape_sample), chunk_size) self.chunk_size = chunk_size with Pool(num_nodes) as p: result = p.map(self._measure_xi_r_mur_box_jk_batch, indices) os.remove( f\"{file_tree_path}/m_{self.simname}_temp_data_{figname_dataset_name}.hdf5\") self.data = data_temp del data_temp for i in np.arange(len(result)): Splus_D += result[i][0] Scross_D += result[i][1] DD += result[i][2] DD_jk += result[i][3] Splus_D_jk += result[i][4] R_jk = np.zeros(self.num_box) for i in np.arange(self.num_box): jk_mask = np.where(self.jackknife_region_indices_shape != i) R_jk[i] = sum(weight_shape[jk_mask] * (1 - self.e[jk_mask] ** 2 / 2.0)) / sum(weight_shape[jk_mask]) corrtype = \"cross\" for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_g_plus[i, p] = self.get_random_pairs_r_mur( self.r_bins[i + 1], self.r_bins[i], self.mu_r_bins[p + 1], self.mu_r_bins[p], L3, \"cross\", self.Num_position_masked, self.Num_shape_masked) RR_gg[i, p] = self.get_random_pairs_r_mur( self.r_bins[i + 1], self.r_bins[i], self.mu_r_bins[p + 1], self.mu_r_bins[p], L3, corrtype, self.Num_position_masked, self.Num_shape_masked) RR_jk = np.zeros((self.num_box, self.num_bins_r, self.num_bins_pi)) volume_jk = L3 * (self.num_box - 1) / self.num_box for jk in np.arange(self.num_box): Num_position_jk, Num_shape_jk = len(np.where(self.jackknife_region_indices_pos != jk)[0]), len( np.where(self.jackknife_region_indices_shape != jk)[0]) for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_jk[jk, i, p] = self.get_random_pairs_r_mur( self.r_bins[i + 1], self.r_bins[i], self.mu_r_bins[p + 1], self.mu_r_bins[p], volume_jk, \"cross\", Num_position_jk, Num_shape_jk) correlation = Splus_D / RR_g_plus # (Splus_D - Splus_R) / RR_g_plus xi_g_cross = Scross_D / RR_g_plus # (Scross_D - Scross_R) / RR_g_plus dsep = (self.r_bins[1:] - self.r_bins[:-1]) / 2.0 separation_bins = self.r_bins[:-1] + abs(dsep) # middle of bins dmur = (self.mu_r_bins[1:] - self.mu_r_bins[:-1]) / 2.0 mu_r_bins = self.mu_r_bins[:-1] + abs(dmur) # middle of bins if (self.output_file_name != None) and (return_output == False): output_file = h5py.File(self.output_file_name, \"a\") group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_plus/\") write_dataset_hdf5(group, dataset_name, data=correlation) write_dataset_hdf5(group, dataset_name + \"_SplusD\", data=Splus_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_plus\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_plus/{jk_group_name}\") for i in np.arange(0, self.num_box): corr = (Splus_D * (2 * self.R) - Splus_D_jk[i]) / ( RR_jk[i] * 2 * R_jk[i]) # Responsivity will be different for each realisation write_dataset_hdf5(group, dataset_name + f\"_{i}\", data=corr) write_dataset_hdf5(group, dataset_name + f\"_{i}_SplusD\", data=(Splus_D * (2 * self.R) - Splus_D_jk[i]) / ( 2 * R_jk[i])) # Splus_D_jk[i]/(2*R_jk[i])) write_dataset_hdf5(group, dataset_name + f\"_{i}_RR\", data=RR_jk[i]) write_dataset_hdf5(group, dataset_name + f\"_{i}_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + f\"_{i}_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_cross/{jk_group_name}\") write_dataset_hdf5(group, dataset_name + \"_ScrossD\", data=Scross_D) write_dataset_hdf5(group, dataset_name, data=xi_g_cross) write_dataset_hdf5(group, dataset_name + \"_RR_g_cross\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_gg/\") write_dataset_hdf5(group, dataset_name, data=(DD / RR_gg) - 1) write_dataset_hdf5(group, dataset_name + \"_DD\", data=DD) write_dataset_hdf5(group, dataset_name + \"_RR_gg\", data=RR_gg) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_gg/{jk_group_name}\") for i in np.arange(0, self.num_box): write_dataset_hdf5(group, dataset_name + f\"_{i}\", data=((DD - DD_jk[i]) / RR_jk[i]) - 1) write_dataset_hdf5(group, dataset_name + f\"_{i}_DD\", data=(DD - DD_jk[i])) write_dataset_hdf5(group, dataset_name + f\"_{i}_RR\", data=RR_jk[i]) write_dataset_hdf5(group, dataset_name + f\"_{i}_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + f\"_{i}_mu_r\", data=mu_r_bins) output_file.close() return else: return correlation, (DD / RR_gg) - 1, separation_bins, mu_r_bins, Splus_D, DD, RR_g_plus __init__(data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True) The init method of the MeasureWSimulations class. Notes Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. Source code in src/measureia/measure_m_box_jk.py def __init__( self, data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True, ): \"\"\" The __init__ method of the MeasureWSimulations class. Notes ----- Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. \"\"\" super().__init__(data, output_file_name, simulation, snapshot, separation_limits, num_bins_r, num_bins_pi, pi_max, boxsize, periodicity) return handler: python options: show_source: true members_order: source show_root_heading: true heading_level: 2","title":"MeasureMBoxJackknife"},{"location":"api/MeasureMBoxJackknife/#measuremboxjackknife","text":"Bases: MeasureIABase , ReadData Class that contains all methods for the measurements of \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) for \\(\\tilde{\\xi}_{gg,0}\\) and \\(\\tilde{\\xi}_{g+,2}\\) including the jackknife realisations needed for the covariance estimation with Cartesian simulation data. Methods: Name Description _measure_xi_r_mur_box_jk_brute Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (r, mu_r) grid binning including jackknife realisations in a periodic box using 1 CPU. _measure_xi_r_mur_box_jk_tree Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (r, mu_r) grid binning including jackknife realisations in a periodic box using 1 CPU and KDTree for extra speed. _measure_xi_r_mur_box_jk_batch Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (r, mu_r) grid binning including jackknife realisations in a periodic box using 1 CPU for a batch of indices. Support function of _measure_xi_r_mur_box_jk_multiprocessing(). _measure_xi_r_mur_box_jk_multiprocessing Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (r, mu_r) grid binning including jackknife realisations in a periodic box using >1 CPUs. Notes Inherits attributes from 'SimInfo', where 'boxsize', 'L_0p5' and 'snap_group' are used in this class. Inherits attributes from 'MeasureIABase', where 'data', 'output_file_name', 'periodicity', 'Num_position', 'Num_shape', 'r_min', 'r_max', 'num_bins_r', 'num_bins_pi', 'r_bins', 'pi_bins', 'mu_r_bins' are used. Source code in src/measureia/measure_m_box_jk.py class MeasureMBoxJackknife(MeasureIABase, ReadData): r\"\"\"Class that contains all methods for the measurements of $\\xi_{gg}$ and $\\xi_{g+}$ for $\\tilde{\\xi}_{gg,0}$ and $\\tilde{\\xi}_{g+,2}$ including the jackknife realisations needed for the covariance estimation with Cartesian simulation data. Methods ------- _measure_xi_r_mur_box_jk_brute() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (r, mu_r) grid binning including jackknife realisations in a periodic box using 1 CPU. _measure_xi_r_mur_box_jk_tree() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (r, mu_r) grid binning including jackknife realisations in a periodic box using 1 CPU and KDTree for extra speed. _measure_xi_r_mur_box_jk_batch() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (r, mu_r) grid binning including jackknife realisations in a periodic box using 1 CPU for a batch of indices. Support function of _measure_xi_r_mur_box_jk_multiprocessing(). _measure_xi_r_mur_box_jk_multiprocessing() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (r, mu_r) grid binning including jackknife realisations in a periodic box using >1 CPUs. Notes ----- Inherits attributes from 'SimInfo', where 'boxsize', 'L_0p5' and 'snap_group' are used in this class. Inherits attributes from 'MeasureIABase', where 'data', 'output_file_name', 'periodicity', 'Num_position', 'Num_shape', 'r_min', 'r_max', 'num_bins_r', 'num_bins_pi', 'r_bins', 'pi_bins', 'mu_r_bins' are used. \"\"\" def __init__( self, data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True, ): \"\"\" The __init__ method of the MeasureWSimulations class. Notes ----- Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. \"\"\" super().__init__(data, output_file_name, simulation, snapshot, separation_limits, num_bins_r, num_bins_pi, pi_max, boxsize, periodicity) return def _measure_xi_r_mur_box_jk_brute(self, dataset_name, L_subboxes, masks=None, rp_cut=None, return_output=False, jk_group_name=\"\", ellipticity='distortion'): r\"\"\"Measures the projected correlation functions including jackknife realisations, $\\xi_{gg}$ and $\\xi_{g+}$, in (r, mu_r) bins for an object created with MeasureIABox. Uses 1 CPU. Parameters ---------- dataset_name : str Name of the dataset in the output file. L_subboxes: int Number of subboxes on one side of the box. L_subboxes^3 is the total number of jackknife realisations. masks : dict or NoneType, optional Dictionary with masks for the data to select only part of the data. Uses same keywords as data dictionary. Default value is None. rp_cut : float, optional Limit for minimum r_p value for pairs to be included. Default value is None. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. jk_group_name : str, optional Group in output file (hdf5) where jackknife realisations are stored. Default value is \"\". ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. Returns ------- ndarrays $\\xi_{gg}$ and $\\xi_{g+}$, r bins, mu_r bins, S+D, DD, RR (if no output file is specified) \"\"\" if masks == None: positions = self.data[\"Position\"] positions_shape_sample = self.data[\"Position_shape_sample\"] axis_direction_v = self.data[\"Axis_Direction\"] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"] weight = self.data[\"weight\"] weight_shape = self.data[\"weight_shape_sample\"] else: positions = self.data[\"Position\"][masks[\"Position\"]] positions_shape_sample = self.data[\"Position_shape_sample\"][masks[\"Position_shape_sample\"]] axis_direction_v = self.data[\"Axis_Direction\"][masks[\"Axis_Direction\"]] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"][masks[\"q\"]] try: weight_mask = masks[\"weight\"] except: masks[\"weight\"] = np.ones(self.Num_position, dtype=bool) masks[\"weight\"][sum(masks[\"Position\"]):self.Num_position] = 0 try: weight_mask = masks[\"weight_shape_sample\"] except: masks[\"weight_shape_sample\"] = np.ones(self.Num_shape, dtype=bool) masks[\"weight_shape_sample\"][sum(masks[\"Position_shape_sample\"]):self.Num_shape] = 0 weight = self.data[\"weight\"][masks[\"weight\"]] weight_shape = self.data[\"weight_shape_sample\"][masks[\"weight_shape_sample\"]] Num_position = len(positions) Num_shape = len(positions_shape_sample) print( f\"There are {Num_shape} galaxies in the shape sample and {Num_position} galaxies in the position sample.\") LOS_ind = self.data[\"LOS\"] # eg 2 for z axis not_LOS = np.array([0, 1, 2])[np.isin([0, 1, 2], LOS_ind, invert=True)] # eg 0,1 for x&y if ellipticity == 'distortion': e = (1 - q ** 2) / (1 + q ** 2) # size of ellipticity elif ellipticity == 'ellipticity': e = (1 - q) / (1 + q) else: raise ValueError(\"Invalid value for ellipticity. Choose 'distortion' or 'ellipticity'.\") del q R = sum(weight_shape * (1 - e ** 2 / 2.0)) / sum(weight_shape) # R = 1 - np.mean(e ** 2) / 2.0 # responsitivity factor L3 = self.boxsize ** 3 # box volume if rp_cut == None: rp_cut = 0.0 sub_box_len_logr = (np.log10(self.r_max) - np.log10(self.r_min)) / self.num_bins_r sub_box_len_mu_r = 2.0 / self.num_bins_pi # mu_r ranges from -1 to 1. Same number of bins as pi DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_g_plus = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_gg = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) variance = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) jackknife_region_indices_pos, jackknife_region_indices_shape = self._get_jackknife_region_indices(masks, L_subboxes) num_box = L_subboxes ** 3 DD_jk = np.zeros((num_box, self.num_bins_r, self.num_bins_pi)) Splus_D_jk = np.zeros((num_box, self.num_bins_r, self.num_bins_pi)) for n in np.arange(0, len(positions)): separation = positions_shape_sample - positions[n] if self.periodicity: separation[separation > self.L_0p5] -= self.boxsize # account for periodicity of box separation[separation < -self.L_0p5] += self.boxsize projected_sep = separation[:, not_LOS] LOS = separation[:, LOS_ind] projected_separation_len = np.sqrt(np.sum(projected_sep ** 2, axis=1)) with np.errstate(invalid='ignore'): separation_dir = ( projected_sep.transpose() / projected_separation_len).transpose() # normalisation of rp separation_len = np.sqrt(np.sum(separation ** 2, axis=1)) with np.errstate(invalid='ignore'): mu_r = LOS / separation_len del LOS, projected_sep, separation phi = np.arccos(self.calculate_dot_product_arrays(separation_dir, axis_direction)) # [0,pi] e_plus, e_cross = self.get_ellipticity(e, phi) del phi, separation_dir e_plus[np.isnan(e_plus)] = 0.0 e_cross[np.isnan(e_cross)] = 0.0 mu_r[np.isnan(e_plus)] = 0.0 # get the indices for the binning mask = ( (projected_separation_len > rp_cut) * (separation_len >= self.r_bins[0]) * (separation_len < self.r_bins[-1]) ) ind_r = np.floor( np.log10(separation_len[mask]) / sub_box_len_logr - np.log10(self.r_bins[0]) / sub_box_len_logr ) del separation_len, projected_separation_len ind_r = np.array(ind_r, dtype=int) ind_mu_r = np.floor( mu_r[mask] / sub_box_len_mu_r - self.mu_r_bins[0] / sub_box_len_mu_r ) # need length of LOS, so only positive values ind_mu_r = np.array(ind_mu_r, dtype=int) if np.any(ind_mu_r == self.num_bins_pi): ind_mu_r[ind_mu_r >= self.num_bins_pi] -= 1 if np.any(ind_r == self.num_bins_r): ind_r[ind_r >= self.num_bins_r] -= 1 np.add.at(Splus_D, (ind_r, ind_mu_r), (weight[n] * weight_shape[mask] * e_plus[mask]) / (2 * R)) np.add.at(Scross_D, (ind_r, ind_mu_r), (weight[n] * weight_shape[mask] * e_cross[mask]) / (2 * R)) np.add.at(variance, (ind_r, ind_mu_r), ((weight[n] * weight_shape[mask] * e_plus[mask]) / (2 * R)) ** 2) shape_mask = np.where(jackknife_region_indices_shape[mask] != jackknife_region_indices_pos[n])[0] np.add.at(Splus_D_jk, (jackknife_region_indices_pos[n], ind_r, ind_mu_r), (weight[n] * weight_shape[mask] * e_plus[mask])) # responsivity added later np.add.at(Splus_D_jk, (jackknife_region_indices_shape[mask][shape_mask], ind_r[shape_mask], ind_mu_r[shape_mask]), (weight[n] * weight_shape[mask][shape_mask] * e_plus[mask][ shape_mask])) # responsivity added later del e_plus, e_cross np.add.at(DD, (ind_r, ind_mu_r), weight[n] * weight_shape[mask]) np.add.at(DD_jk, (jackknife_region_indices_pos[n], ind_r, ind_mu_r), (weight[n] * weight_shape[mask])) np.add.at(DD_jk, (jackknife_region_indices_shape[mask][shape_mask], ind_r[shape_mask], ind_mu_r[shape_mask]), (weight[n] * weight_shape[mask][shape_mask])) R_jk = np.zeros(num_box) for i in np.arange(num_box): jk_mask = np.where(jackknife_region_indices_shape != i) R_jk[i] = sum(weight_shape[jk_mask] * (1 - e[jk_mask] ** 2 / 2.0)) / sum(weight_shape[jk_mask]) corrtype = \"cross\" for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_g_plus[i, p] = self.get_random_pairs_r_mur( self.r_bins[i + 1], self.r_bins[i], self.mu_r_bins[p + 1], self.mu_r_bins[p], L3, \"cross\", Num_position, Num_shape) RR_gg[i, p] = self.get_random_pairs_r_mur( self.r_bins[i + 1], self.r_bins[i], self.mu_r_bins[p + 1], self.mu_r_bins[p], L3, corrtype, Num_position, Num_shape) RR_jk = np.zeros((num_box, self.num_bins_r, self.num_bins_pi)) volume_jk = L3 * (num_box - 1) / (num_box) for jk in np.arange(num_box): Num_position_jk, Num_shape_jk = len(np.where(jackknife_region_indices_pos != jk)[0]), len( np.where(jackknife_region_indices_shape != jk)[0]) for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_jk[jk, i, p] = self.get_random_pairs_r_mur( self.r_bins[i + 1], self.r_bins[i], self.mu_r_bins[p + 1], self.mu_r_bins[p], volume_jk, \"cross\", Num_position_jk, Num_shape_jk) correlation = Splus_D / RR_g_plus # (Splus_D - Splus_R) / RR_g_plus xi_g_cross = Scross_D / RR_g_plus # (Scross_D - Scross_R) / RR_g_plus sigsq = variance / RR_g_plus ** 2 dsep = (self.r_bins[1:] - self.r_bins[:-1]) / 2.0 separation_bins = self.r_bins[:-1] + abs(dsep) # middle of bins dmur = (self.mu_r_bins[1:] - self.mu_r_bins[:-1]) / 2.0 mu_r_bins = self.mu_r_bins[:-1] + abs(dmur) # middle of bins if (self.output_file_name != None) and (return_output == False): output_file = h5py.File(self.output_file_name, \"a\") group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_plus/\") write_dataset_hdf5(group, dataset_name, data=correlation) write_dataset_hdf5(group, dataset_name + \"_SplusD\", data=Splus_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_plus\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_sigmasq\", data=sigsq) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_plus/{jk_group_name}\") for i in np.arange(0, num_box): corr = (Splus_D * (2 * R) - Splus_D_jk[i]) / ( RR_jk[i] * 2 * R_jk[i]) # Responsivity will be different for each realisation write_dataset_hdf5(group, dataset_name + f\"_{i}\", data=corr) write_dataset_hdf5(group, dataset_name + f\"_{i}_SplusD\", data=(Splus_D * (2 * R) - Splus_D_jk[i]) / ( 2 * R_jk[i])) # Splus_D_jk[i]/(2*R_jk[i])) write_dataset_hdf5(group, dataset_name + f\"_{i}_RR\", data=RR_jk[i]) write_dataset_hdf5(group, dataset_name + f\"_{i}_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + f\"_{i}_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_cross/{jk_group_name}\") write_dataset_hdf5(group, dataset_name + \"_ScrossD\", data=Scross_D) write_dataset_hdf5(group, dataset_name, data=xi_g_cross) write_dataset_hdf5(group, dataset_name + \"_RR_g_cross\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_sigmasq\", data=sigsq) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_gg/\") write_dataset_hdf5(group, dataset_name, data=(DD / RR_gg) - 1) write_dataset_hdf5(group, dataset_name + \"_DD\", data=DD) write_dataset_hdf5(group, dataset_name + \"_RR_gg\", data=RR_gg) write_dataset_hdf5(group, dataset_name + \"_sigmasq\", data=sigsq) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_gg/{jk_group_name}\") for i in np.arange(0, num_box): write_dataset_hdf5(group, dataset_name + f\"_{i}\", data=((DD - DD_jk[i]) / RR_jk[i]) - 1) write_dataset_hdf5(group, dataset_name + f\"_{i}_DD\", data=(DD - DD_jk[i])) write_dataset_hdf5(group, dataset_name + f\"_{i}_RR\", data=RR_jk[i]) write_dataset_hdf5(group, dataset_name + f\"_{i}_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + f\"_{i}_mu_r\", data=mu_r_bins) output_file.close() return else: return correlation, (DD / RR_gg) - 1, separation_bins, mu_r_bins, Splus_D, DD, RR_g_plus def _measure_xi_r_mur_box_jk_tree(self, dataset_name, L_subboxes, masks=None, rp_cut=None, return_output=False, jk_group_name=\"\", ellipticity='distortion'): r\"\"\"Measures the projected correlation functions including jackknife realisations, $\\xi_{gg}$ and $\\xi_{g+}$, in (r, mu_r) bins for an object created with MeasureIABox. Uses 1 CPU. Uses KDTree for speedup. Parameters ---------- dataset_name : str Name of the dataset in the output file. L_subboxes: int Number of subboxes on one side of the box. L_subboxes^3 is the total number of jackknife realisations. masks : dict or NoneType, optional Dictionary with masks for the data to select only part of the data. Uses same keywords as data dictionary. Default value = None. rp_cut : float, optional Limit for minimum r_p value for pairs to be included. Default value is None. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. jk_group_name : str, optional Group in output file (hdf5) where jackknife realisations are stored. Default value is \"\". ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. Returns ------- ndarrays $\\xi_{gg}$ and $\\xi_{g+}$, r bins, mu_r bins, S+D, DD, RR (if no output file is specified) \"\"\" if masks == None: positions = self.data[\"Position\"] positions_shape_sample = self.data[\"Position_shape_sample\"] axis_direction_v = self.data[\"Axis_Direction\"] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"] weight = self.data[\"weight\"] weight_shape = self.data[\"weight_shape_sample\"] else: positions = self.data[\"Position\"][masks[\"Position\"]] positions_shape_sample = self.data[\"Position_shape_sample\"][masks[\"Position_shape_sample\"]] axis_direction_v = self.data[\"Axis_Direction\"][masks[\"Axis_Direction\"]] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"][masks[\"q\"]] try: weight_mask = masks[\"weight\"] except: masks[\"weight\"] = np.ones(self.Num_position, dtype=bool) masks[\"weight\"][sum(masks[\"Position\"]):self.Num_position] = 0 try: weight_mask = masks[\"weight_shape_sample\"] except: masks[\"weight_shape_sample\"] = np.ones(self.Num_shape, dtype=bool) masks[\"weight_shape_sample\"][sum(masks[\"Position_shape_sample\"]):self.Num_shape] = 0 weight = self.data[\"weight\"][masks[\"weight\"]] weight_shape = self.data[\"weight_shape_sample\"][masks[\"weight_shape_sample\"]] Num_position = len(positions) Num_shape = len(positions_shape_sample) print( f\"There are {Num_shape} galaxies in the shape sample and {Num_position} galaxies in the position sample.\") LOS_ind = self.data[\"LOS\"] # eg 2 for z axis not_LOS = np.array([0, 1, 2])[np.isin([0, 1, 2], LOS_ind, invert=True)] # eg 0,1 for x&y if ellipticity == 'distortion': e = (1 - q ** 2) / (1 + q ** 2) # size of ellipticity elif ellipticity == 'ellipticity': e = (1 - q) / (1 + q) else: raise ValueError(\"Invalid value for ellipticity. Choose 'distortion' or 'ellipticity'.\") del q R = sum(weight_shape * (1 - e ** 2 / 2.0)) / sum(weight_shape) # R = 1 - np.mean(e ** 2) / 2.0 # responsitivity factor L3 = self.boxsize ** 3 # box volume if rp_cut == None: rp_cut = 0.0 sub_box_len_logr = (np.log10(self.r_max) - np.log10(self.r_min)) / self.num_bins_r sub_box_len_mu_r = 2.0 / self.num_bins_pi # mu_r ranges from -1 to 1. Same number of bins as pi DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_g_plus = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_gg = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) variance = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) jackknife_region_indices_pos, jackknife_region_indices_shape = self._get_jackknife_region_indices(masks, L_subboxes) num_box = L_subboxes ** 3 DD_jk = np.zeros((num_box, self.num_bins_r, self.num_bins_pi)) Splus_D_jk = np.zeros((num_box, self.num_bins_r, self.num_bins_pi)) figname_dataset_name = dataset_name if \"/\" in dataset_name: figname_dataset_name = figname_dataset_name.replace(\"/\", \"_\") if \".\" in dataset_name: figname_dataset_name = figname_dataset_name.replace(\".\", \"p\") pos_tree = KDTree(positions, boxsize=self.boxsize) for i in np.arange(0, len(positions_shape_sample), 100): i2 = min(len(positions_shape_sample), i + 100) positions_shape_sample_i = positions_shape_sample[i:i2] axis_direction_i = axis_direction[i:i2] e_i = e[i:i2] weight_shape_i = weight_shape[i:i2] jackknife_region_indices_shape_i = jackknife_region_indices_shape[i:i2] shape_tree = KDTree(positions_shape_sample_i, boxsize=self.boxsize) ind_min_i = shape_tree.query_ball_tree(pos_tree, self.r_min) ind_max_i = shape_tree.query_ball_tree(pos_tree, self.r_max) ind_rbin_i = self.setdiff2D(ind_max_i, ind_min_i) for n in np.arange(0, len(positions_shape_sample_i)): if len(ind_rbin_i[n]) > 0: # for Splus_D (calculate ellipticities around position sample) separation = positions_shape_sample_i[n] - positions[ind_rbin_i[n]] if self.periodicity: separation[separation > self.L_0p5] -= self.boxsize # account for periodicity of box separation[separation < -self.L_0p5] += self.boxsize projected_sep = separation[:, not_LOS] LOS = separation[:, LOS_ind] projected_separation_len = np.sqrt(np.sum(projected_sep ** 2, axis=1)) with np.errstate(invalid='ignore'): separation_dir = ( projected_sep.transpose() / projected_separation_len).transpose() # normalisation of rp separation_len = np.sqrt(np.sum(separation ** 2, axis=1)) del separation, projected_sep with np.errstate(invalid='ignore'): mu_r = LOS / separation_len phi = np.arccos( separation_dir[:, 0] * axis_direction_i[n, 0] + separation_dir[:, 1] * axis_direction_i[ n, 1]) # [0,pi] e_plus, e_cross = self.get_ellipticity(e_i[n], phi) del phi, LOS, separation_dir e_plus[np.isnan(e_plus)] = 0.0 mu_r[np.isnan(e_plus)] = 0.0 e_cross[np.isnan(e_cross)] = 0.0 # get the indices for the binning mask = ( (projected_separation_len > rp_cut) * (separation_len >= self.r_bins[0]) * (separation_len < self.r_bins[-1]) ) ind_r = np.floor( np.log10(separation_len[mask]) / sub_box_len_logr - np.log10( self.r_bins[0]) / sub_box_len_logr ) ind_r = np.array(ind_r, dtype=int) ind_mu_r = np.floor( mu_r[mask] / sub_box_len_mu_r - self.mu_r_bins[0] / sub_box_len_mu_r ) # need length of LOS, so only positive values ind_mu_r = np.array(ind_mu_r, dtype=int) if np.any(ind_mu_r == self.num_bins_pi): ind_mu_r[ind_mu_r >= self.num_bins_pi] -= 1 if np.any(ind_r == self.num_bins_r): ind_r[ind_r >= self.num_bins_r] -= 1 np.add.at(Splus_D, (ind_r, ind_mu_r), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_plus[mask]) / (2 * R)) np.add.at(Scross_D, (ind_r, ind_mu_r), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_cross[mask]) / (2 * R)) del separation_len np.add.at(DD, (ind_r, ind_mu_r), weight[ind_rbin_i[n]][mask] * weight_shape_i[n]) pos_mask = \\ np.where( jackknife_region_indices_pos[ind_rbin_i[n]][mask] != jackknife_region_indices_shape_i[n])[ 0] np.add.at(Splus_D_jk, (jackknife_region_indices_shape_i[n], ind_r, ind_mu_r), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_plus[ mask])) # responsivity added later np.add.at(Splus_D_jk, (jackknife_region_indices_pos[ind_rbin_i[n]][mask][pos_mask], ind_r[pos_mask], ind_mu_r[pos_mask]), (weight[ind_rbin_i[n]][mask][pos_mask] * weight_shape_i[n] * e_plus[mask][ pos_mask])) # responsivity added later del e_plus, e_cross np.add.at(DD_jk, (jackknife_region_indices_shape_i[n], ind_r, ind_mu_r), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n])) # responsivity added later np.add.at(DD_jk, (jackknife_region_indices_pos[ind_rbin_i[n]][mask][pos_mask], ind_r[pos_mask], ind_mu_r[pos_mask]), (weight[ind_rbin_i[n]][mask][pos_mask] * weight_shape_i[n])) # responsivity added later R_jk = np.zeros(num_box) for i in np.arange(num_box): jk_mask = np.where(jackknife_region_indices_shape != i) R_jk[i] = sum(weight_shape[jk_mask] * (1 - e[jk_mask] ** 2 / 2.0)) / sum(weight_shape[jk_mask]) corrtype = \"cross\" for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_g_plus[i, p] = self.get_random_pairs_r_mur( self.r_bins[i + 1], self.r_bins[i], self.mu_r_bins[p + 1], self.mu_r_bins[p], L3, \"cross\", Num_position, Num_shape) RR_gg[i, p] = self.get_random_pairs_r_mur( self.r_bins[i + 1], self.r_bins[i], self.mu_r_bins[p + 1], self.mu_r_bins[p], L3, corrtype, Num_position, Num_shape) RR_jk = np.zeros((num_box, self.num_bins_r, self.num_bins_pi)) volume_jk = L3 * (num_box - 1) / num_box for jk in np.arange(num_box): Num_position_jk, Num_shape_jk = len(np.where(jackknife_region_indices_pos != jk)[0]), len( np.where(jackknife_region_indices_shape != jk)[0]) for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_jk[jk, i, p] = self.get_random_pairs_r_mur( self.r_bins[i + 1], self.r_bins[i], self.mu_r_bins[p + 1], self.mu_r_bins[p], volume_jk, \"cross\", Num_position_jk, Num_shape_jk) correlation = Splus_D / RR_g_plus # (Splus_D - Splus_R) / RR_g_plus xi_g_cross = Scross_D / RR_g_plus # (Scross_D - Scross_R) / RR_g_plus sigsq = variance / RR_g_plus ** 2 dsep = (self.r_bins[1:] - self.r_bins[:-1]) / 2.0 separation_bins = self.r_bins[:-1] + abs(dsep) # middle of bins dmur = (self.mu_r_bins[1:] - self.mu_r_bins[:-1]) / 2.0 mu_r_bins = self.mu_r_bins[:-1] + abs(dmur) # middle of bins if (self.output_file_name != None) and (return_output == False): output_file = h5py.File(self.output_file_name, \"a\") group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_plus/\") write_dataset_hdf5(group, dataset_name, data=correlation) write_dataset_hdf5(group, dataset_name + \"_SplusD\", data=Splus_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_plus\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_sigmasq\", data=sigsq) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_plus/{jk_group_name}\") for i in np.arange(0, num_box): corr = (Splus_D * (2 * R) - Splus_D_jk[i]) / ( RR_jk[i] * 2 * R_jk[i]) # Responsivity will be different for each realisation write_dataset_hdf5(group, dataset_name + f\"_{i}\", data=corr) write_dataset_hdf5(group, dataset_name + f\"_{i}_SplusD\", data=(Splus_D * (2 * R) - Splus_D_jk[i]) / ( 2 * R_jk[i])) # Splus_D_jk[i]/(2*R_jk[i])) write_dataset_hdf5(group, dataset_name + f\"_{i}_RR\", data=RR_jk[i]) write_dataset_hdf5(group, dataset_name + f\"_{i}_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + f\"_{i}_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_cross/{jk_group_name}\") write_dataset_hdf5(group, dataset_name + \"_ScrossD\", data=Scross_D) write_dataset_hdf5(group, dataset_name, data=xi_g_cross) write_dataset_hdf5(group, dataset_name + \"_RR_g_cross\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_sigmasq\", data=sigsq) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_gg/\") write_dataset_hdf5(group, dataset_name, data=(DD / RR_gg) - 1) write_dataset_hdf5(group, dataset_name + \"_DD\", data=DD) write_dataset_hdf5(group, dataset_name + \"_RR_gg\", data=RR_gg) write_dataset_hdf5(group, dataset_name + \"_sigmasq\", data=sigsq) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_gg/{jk_group_name}\") for i in np.arange(0, num_box): write_dataset_hdf5(group, dataset_name + f\"_{i}\", data=((DD - DD_jk[i]) / RR_jk[i]) - 1) write_dataset_hdf5(group, dataset_name + f\"_{i}_DD\", data=(DD - DD_jk[i])) write_dataset_hdf5(group, dataset_name + f\"_{i}_RR\", data=RR_jk[i]) write_dataset_hdf5(group, dataset_name + f\"_{i}_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + f\"_{i}_mu_r\", data=mu_r_bins) output_file.close() return else: return correlation, (DD / RR_gg) - 1, separation_bins, mu_r_bins, Splus_D, DD, RR_g_plus def _measure_xi_r_mur_box_jk_batch(self, i): r\"\"\"Measures components of $\\xi_{gg}$ and $\\xi_{g+}$ in (r, mu_r) bins including jackknife realisations for a batch of indices from i to i+chunk_size. Support function for _measure_xi_r_mu_r_box_jk_multiprocessing(). Parameters ---------- i: int Start index of the batch. Returns ------- ndarrays S+D, SxD, DD, DD_jk, S+D_jk where the _jk versions store the necessary information of DD of S+D for each jackknife realisation. \"\"\" if i + self.chunk_size > self.Num_shape_masked: i2 = self.Num_shape_masked else: i2 = i + self.chunk_size DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) DD_jk = np.zeros((self.num_box, self.num_bins_r, self.num_bins_pi)) Splus_D_jk = np.zeros((self.num_box, self.num_bins_r, self.num_bins_pi)) positions_shape_sample_i = self.temp_data_obj_m.read_cat(\"positions_shape_sample\", [i, i2]) axis_direction_i = self.temp_data_obj_m.read_cat(\"axis_direction\", [i, i2]) weight_shape_i = self.temp_data_obj_m.read_cat(\"weight_shape\", [i, i2]) positions = self.temp_data_obj_m.read_cat(\"positions\") weight = self.temp_data_obj_m.read_cat(\"weight\") e_i = self.e[i:i2] jackknife_region_indices_shape_i = self.jackknife_region_indices_shape[i:i2] shape_tree = KDTree(positions_shape_sample_i, boxsize=self.boxsize) ind_min_i = shape_tree.query_ball_tree(self.pos_tree, self.r_min) ind_max_i = shape_tree.query_ball_tree(self.pos_tree, self.r_max) ind_rbin_i = self.setdiff2D(ind_max_i, ind_min_i) for n in np.arange(0, len(positions_shape_sample_i)): if len(ind_rbin_i[n]) > 0: # for Splus_D (calculate ellipticities around position sample) separation = positions_shape_sample_i[n] - positions[ind_rbin_i[n]] if self.periodicity: separation[separation > self.L_0p5] -= self.boxsize # account for periodicity of box separation[separation < -self.L_0p5] += self.boxsize projected_sep = separation[:, self.not_LOS] LOS = separation[:, self.LOS_ind] projected_separation_len = np.sqrt(np.sum(projected_sep ** 2, axis=1)) with np.errstate(invalid='ignore'): separation_dir = ( projected_sep.transpose() / projected_separation_len).transpose() # normalisation of rp separation_len = np.sqrt(np.sum(separation ** 2, axis=1)) del separation, projected_sep with np.errstate(invalid='ignore'): mu_r = LOS / separation_len phi = np.arccos( separation_dir[:, 0] * axis_direction_i[n, 0] + separation_dir[:, 1] * axis_direction_i[ n, 1]) # [0,pi] e_plus, e_cross = self.get_ellipticity(e_i[n], phi) del phi, LOS, separation_dir e_plus[np.isnan(e_plus)] = 0.0 mu_r[np.isnan(e_plus)] = 0.0 e_cross[np.isnan(e_cross)] = 0.0 # get the indices for the binning mask = ( (projected_separation_len > self.rp_cut) * (separation_len >= self.r_bins[0]) * (separation_len < self.r_bins[-1]) ) ind_r = np.floor( np.log10(separation_len[mask]) / self.sub_box_len_logr - np.log10( self.r_bins[0]) / self.sub_box_len_logr ) ind_r = np.array(ind_r, dtype=int) ind_mu_r = np.floor( mu_r[mask] / self.sub_box_len_mu_r - self.mu_r_bins[0] / self.sub_box_len_mu_r ) # need length of LOS, so only positive values ind_mu_r = np.array(ind_mu_r, dtype=int) if np.any(ind_mu_r == self.num_bins_pi): ind_mu_r[ind_mu_r >= self.num_bins_pi] -= 1 if np.any(ind_r == self.num_bins_r): ind_r[ind_r >= self.num_bins_r] -= 1 np.add.at(Splus_D, (ind_r, ind_mu_r), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_plus[mask]) / (2 * self.R)) np.add.at(Scross_D, (ind_r, ind_mu_r), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_cross[mask]) / (2 * self.R)) np.add.at(DD, (ind_r, ind_mu_r), weight[ind_rbin_i[n]][mask] * weight_shape_i[n]) del separation_len pos_mask = \\ np.where( self.jackknife_region_indices_pos[ind_rbin_i[n]][mask] != jackknife_region_indices_shape_i[n])[ 0] np.add.at(Splus_D_jk, (jackknife_region_indices_shape_i[n], ind_r, ind_mu_r), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_plus[ mask])) # responsivity added later np.add.at(Splus_D_jk, (self.jackknife_region_indices_pos[ind_rbin_i[n]][mask][pos_mask], ind_r[pos_mask], ind_mu_r[pos_mask]), (weight[ind_rbin_i[n]][mask][pos_mask] * weight_shape_i[n] * e_plus[mask][ pos_mask])) # responsivity added later del e_plus, e_cross np.add.at(DD_jk, (jackknife_region_indices_shape_i[n], ind_r, ind_mu_r), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n])) # responsivity added later np.add.at(DD_jk, (self.jackknife_region_indices_pos[ind_rbin_i[n]][mask][pos_mask], ind_r[pos_mask], ind_mu_r[pos_mask]), (weight[ind_rbin_i[n]][mask][pos_mask] * weight_shape_i[n])) # responsivity added later return Splus_D, Scross_D, DD, DD_jk, Splus_D_jk def _measure_xi_r_mur_box_jk_multiprocessing(self, dataset_name, L_subboxes, file_tree_path, masks=None, rp_cut=None, return_output=False, jk_group_name=\"\", chunk_size=100, num_nodes=1, ellipticity='distortion'): r\"\"\"Measures the projected correlation functions including jackknife realisations, $\\xi_{gg}$ and $\\xi_{g+}$, in (r, mu_r) bins for an object created with MeasureIABox. Uses >1 CPU. Uses KDTree for speedup. Parameters ---------- dataset_name : str Name of the dataset in the output file. L_subboxes: int Number of subboxes on one side of the box. L_subboxes^3 is the total number of jackknife realisations. temp_file_path : str or NoneType, optional Path to where the data is temporarily stored [file name generated automatically]. masks : dict or NoneType, optional Dictionary with masks for the data to select only part of the data. Uses same keywords as data dictionary. Default value = None. rp_cut : float, optional Limit for minimum r_p value for pairs to be included. Default value is None. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. jk_group_name : str, optional Group in output file (hdf5) where jackknife realisations are stored. Default value is \"\". chunk_size: int, optional Size of the chunks of data sent to each multiprocessing node. If larger, more RAM is needed per node. Default is 1000. num_nodes : int, optional Number of CPUs used in the multiprocessing. Default is 1. ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. Returns ------- ndarrays $\\xi_{gg}$ and $\\xi_{g+}$, r bins, mu_r bins, S+D, DD, RR (if no output file is specified) \"\"\" if masks == None: positions = self.data[\"Position\"] positions_shape_sample = self.data[\"Position_shape_sample\"] axis_direction_v = self.data[\"Axis_Direction\"] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"] weight = self.data[\"weight\"] weight_shape = self.data[\"weight_shape_sample\"] else: positions = self.data[\"Position\"][masks[\"Position\"]] positions_shape_sample = self.data[\"Position_shape_sample\"][masks[\"Position_shape_sample\"]] axis_direction_v = self.data[\"Axis_Direction\"][masks[\"Axis_Direction\"]] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"][masks[\"q\"]] try: weight_mask = masks[\"weight\"] except: masks[\"weight\"] = np.ones(self.Num_position, dtype=bool) masks[\"weight\"][sum(masks[\"Position\"]):self.Num_position] = 0 try: weight_mask = masks[\"weight_shape_sample\"] except: masks[\"weight_shape_sample\"] = np.ones(self.Num_shape, dtype=bool) masks[\"weight_shape_sample\"][sum(masks[\"Position_shape_sample\"]):self.Num_shape] = 0 weight = self.data[\"weight\"][masks[\"weight\"]] weight_shape = self.data[\"weight_shape_sample\"][masks[\"weight_shape_sample\"]] self.Num_position_masked = len(positions) self.Num_shape_masked = len(positions_shape_sample) print( f\"There are {self.Num_shape_masked} galaxies in the shape sample and {self.Num_position_masked} galaxies in the position sample.\") # create temp hdf5 from which data can be read. del self.data, but save it in this method to reduce RAM figname_dataset_name = dataset_name if \"/\" in dataset_name: figname_dataset_name = figname_dataset_name.replace(\"/\", \"_\") if \".\" in dataset_name: figname_dataset_name = figname_dataset_name.replace(\".\", \"p\") file_temp = h5py.File(f\"{file_tree_path}/m_{self.simname}_temp_data_{figname_dataset_name}.hdf5\", \"w\") write_dataset_hdf5(file_temp, \"positions\", positions) write_dataset_hdf5(file_temp, \"weight\", weight) write_dataset_hdf5(file_temp, \"weight_shape\", weight_shape) write_dataset_hdf5(file_temp, \"positions_shape_sample\", positions_shape_sample) write_dataset_hdf5(file_temp, \"axis_direction\", axis_direction) file_temp.close() self.temp_data_obj_m = ReadData(self.simname, f\"m_{self.simname}_temp_data_{figname_dataset_name}\", None, data_path=file_tree_path) self.LOS_ind = self.data[\"LOS\"] # eg 2 for z axis self.not_LOS = np.array([0, 1, 2])[np.isin([0, 1, 2], self.LOS_ind, invert=True)] # eg 0,1 for x&y if ellipticity == 'distortion': self.e = (1 - q ** 2) / (1 + q ** 2) # size of ellipticity elif ellipticity == 'ellipticity': self.e = (1 - q) / (1 + q) else: raise ValueError(\"Invalid value for ellipticity. Choose 'distortion' or 'ellipticity'.\") del q self.R = sum(weight_shape * (1 - self.e ** 2 / 2.0)) / sum(weight_shape) # R = 1 - np.mean(e ** 2) / 2.0 # responsitivity factor L3 = self.boxsize ** 3 # box volume if rp_cut == None: self.rp_cut = 0.0 else: self.rp_cut = rp_cut self.sub_box_len_logr = (np.log10(self.r_max) - np.log10(self.r_min)) / self.num_bins_r self.sub_box_len_mu_r = 2.0 / self.num_bins_pi # mu_r ranges from -1 to 1. Same number of bins as pi DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_g_plus = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_gg = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) self.jackknife_region_indices_pos, self.jackknife_region_indices_shape = self._get_jackknife_region_indices( masks, L_subboxes) self.num_box = L_subboxes ** 3 DD_jk = np.zeros((self.num_box, self.num_bins_r, self.num_bins_pi)) Splus_D_jk = np.zeros((self.num_box, self.num_bins_r, self.num_bins_pi)) data_temp = self.data # make sure data is not sent to every CPU self.data = None self.pos_tree = KDTree(positions, boxsize=self.boxsize) indices = np.arange(0, len(positions_shape_sample), chunk_size) self.chunk_size = chunk_size with Pool(num_nodes) as p: result = p.map(self._measure_xi_r_mur_box_jk_batch, indices) os.remove( f\"{file_tree_path}/m_{self.simname}_temp_data_{figname_dataset_name}.hdf5\") self.data = data_temp del data_temp for i in np.arange(len(result)): Splus_D += result[i][0] Scross_D += result[i][1] DD += result[i][2] DD_jk += result[i][3] Splus_D_jk += result[i][4] R_jk = np.zeros(self.num_box) for i in np.arange(self.num_box): jk_mask = np.where(self.jackknife_region_indices_shape != i) R_jk[i] = sum(weight_shape[jk_mask] * (1 - self.e[jk_mask] ** 2 / 2.0)) / sum(weight_shape[jk_mask]) corrtype = \"cross\" for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_g_plus[i, p] = self.get_random_pairs_r_mur( self.r_bins[i + 1], self.r_bins[i], self.mu_r_bins[p + 1], self.mu_r_bins[p], L3, \"cross\", self.Num_position_masked, self.Num_shape_masked) RR_gg[i, p] = self.get_random_pairs_r_mur( self.r_bins[i + 1], self.r_bins[i], self.mu_r_bins[p + 1], self.mu_r_bins[p], L3, corrtype, self.Num_position_masked, self.Num_shape_masked) RR_jk = np.zeros((self.num_box, self.num_bins_r, self.num_bins_pi)) volume_jk = L3 * (self.num_box - 1) / self.num_box for jk in np.arange(self.num_box): Num_position_jk, Num_shape_jk = len(np.where(self.jackknife_region_indices_pos != jk)[0]), len( np.where(self.jackknife_region_indices_shape != jk)[0]) for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_jk[jk, i, p] = self.get_random_pairs_r_mur( self.r_bins[i + 1], self.r_bins[i], self.mu_r_bins[p + 1], self.mu_r_bins[p], volume_jk, \"cross\", Num_position_jk, Num_shape_jk) correlation = Splus_D / RR_g_plus # (Splus_D - Splus_R) / RR_g_plus xi_g_cross = Scross_D / RR_g_plus # (Scross_D - Scross_R) / RR_g_plus dsep = (self.r_bins[1:] - self.r_bins[:-1]) / 2.0 separation_bins = self.r_bins[:-1] + abs(dsep) # middle of bins dmur = (self.mu_r_bins[1:] - self.mu_r_bins[:-1]) / 2.0 mu_r_bins = self.mu_r_bins[:-1] + abs(dmur) # middle of bins if (self.output_file_name != None) and (return_output == False): output_file = h5py.File(self.output_file_name, \"a\") group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_plus/\") write_dataset_hdf5(group, dataset_name, data=correlation) write_dataset_hdf5(group, dataset_name + \"_SplusD\", data=Splus_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_plus\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_plus/{jk_group_name}\") for i in np.arange(0, self.num_box): corr = (Splus_D * (2 * self.R) - Splus_D_jk[i]) / ( RR_jk[i] * 2 * R_jk[i]) # Responsivity will be different for each realisation write_dataset_hdf5(group, dataset_name + f\"_{i}\", data=corr) write_dataset_hdf5(group, dataset_name + f\"_{i}_SplusD\", data=(Splus_D * (2 * self.R) - Splus_D_jk[i]) / ( 2 * R_jk[i])) # Splus_D_jk[i]/(2*R_jk[i])) write_dataset_hdf5(group, dataset_name + f\"_{i}_RR\", data=RR_jk[i]) write_dataset_hdf5(group, dataset_name + f\"_{i}_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + f\"_{i}_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_g_cross/{jk_group_name}\") write_dataset_hdf5(group, dataset_name + \"_ScrossD\", data=Scross_D) write_dataset_hdf5(group, dataset_name, data=xi_g_cross) write_dataset_hdf5(group, dataset_name + \"_RR_g_cross\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_gg/\") write_dataset_hdf5(group, dataset_name, data=(DD / RR_gg) - 1) write_dataset_hdf5(group, dataset_name + \"_DD\", data=DD) write_dataset_hdf5(group, dataset_name + \"_RR_gg\", data=RR_gg) write_dataset_hdf5(group, dataset_name + \"_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_mu_r\", data=mu_r_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/multipoles/xi_gg/{jk_group_name}\") for i in np.arange(0, self.num_box): write_dataset_hdf5(group, dataset_name + f\"_{i}\", data=((DD - DD_jk[i]) / RR_jk[i]) - 1) write_dataset_hdf5(group, dataset_name + f\"_{i}_DD\", data=(DD - DD_jk[i])) write_dataset_hdf5(group, dataset_name + f\"_{i}_RR\", data=RR_jk[i]) write_dataset_hdf5(group, dataset_name + f\"_{i}_r\", data=separation_bins) write_dataset_hdf5(group, dataset_name + f\"_{i}_mu_r\", data=mu_r_bins) output_file.close() return else: return correlation, (DD / RR_gg) - 1, separation_bins, mu_r_bins, Splus_D, DD, RR_g_plus","title":"MeasureMBoxJackknife"},{"location":"api/MeasureMBoxJackknife/#measureia.MeasureMBoxJackknife.__init__","text":"The init method of the MeasureWSimulations class. Notes Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. Source code in src/measureia/measure_m_box_jk.py def __init__( self, data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True, ): \"\"\" The __init__ method of the MeasureWSimulations class. Notes ----- Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. \"\"\" super().__init__(data, output_file_name, simulation, snapshot, separation_limits, num_bins_r, num_bins_pi, pi_max, boxsize, periodicity) return handler: python options: show_source: true members_order: source show_root_heading: true heading_level: 2","title":"__init__"},{"location":"api/MeasureWBox/","text":"MeasureWBox measureia.MeasureWBox Bases: MeasureIABase , ReadData Class that contains all methods for the measurements of \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) for \\(w_{gg}\\) and \\(w_{g+}\\) with Cartesian simulation data. Methods: Name Description _measure_xi_rp_pi_box_brute Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (rp, pi) grid binning in a periodic box using 1 CPU. _measure_xi_rp_pi_box_tree Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (rp, pi) grid binning in a periodic box using 1 CPU and KDTree for extra speed. _measure_xi_rp_pi_box_batch Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (rp, pi) grid binning in a periodic box using 1 CPU for a batch of indices. Support function of _measure_xi_rp_pi_box_multiprocessing(). _measure_xi_rp_pi_box_multiprocessing Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (rp, pi) grid binning in a periodic box using >1 CPUs. Notes Inherits attributes from 'SimInfo', where 'boxsize', 'L_0p5' and 'snap_group' are used in this class. Inherits attributes from 'MeasureIABase', where 'data', 'output_file_name', 'periodicity', 'Num_position', 'Num_shape', 'r_min', 'r_max', 'num_bins_r', 'num_bins_pi', 'r_bins', 'pi_bins', 'mu_r_bins' are used. Source code in src/measureia/measure_w_box.py class MeasureWBox(MeasureIABase, ReadData): r\"\"\"Class that contains all methods for the measurements of $\\xi_{gg}$ and $\\xi_{g+}$ for $w_{gg}$ and $w_{g+}$ with Cartesian simulation data. Methods ------- _measure_xi_rp_pi_box_brute() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (rp, pi) grid binning in a periodic box using 1 CPU. _measure_xi_rp_pi_box_tree() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (rp, pi) grid binning in a periodic box using 1 CPU and KDTree for extra speed. _measure_xi_rp_pi_box_batch() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (rp, pi) grid binning in a periodic box using 1 CPU for a batch of indices. Support function of _measure_xi_rp_pi_box_multiprocessing(). _measure_xi_rp_pi_box_multiprocessing() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (rp, pi) grid binning in a periodic box using >1 CPUs. Notes ----- Inherits attributes from 'SimInfo', where 'boxsize', 'L_0p5' and 'snap_group' are used in this class. Inherits attributes from 'MeasureIABase', where 'data', 'output_file_name', 'periodicity', 'Num_position', 'Num_shape', 'r_min', 'r_max', 'num_bins_r', 'num_bins_pi', 'r_bins', 'pi_bins', 'mu_r_bins' are used. \"\"\" def __init__( self, data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True, ): \"\"\" The __init__ method of the MeasureWSimulations class. Notes ----- Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. \"\"\" super().__init__(data, output_file_name, simulation, snapshot, separation_limits, num_bins_r, num_bins_pi, pi_max, boxsize, periodicity) return def _measure_xi_rp_pi_box_brute(self, dataset_name, masks=None, return_output=False, jk_group_name=\"\", ellipticity='distortion'): r\"\"\"Measures the projected correlation functions, $\\xi_{gg}$ and $\\xi_{g+}$, in (rp, pi) bins for an object created with MeasureIABox. Uses 1 CPU. Parameters ---------- dataset_name : str Name of the dataset in the output file. masks : dict or NoneType, optional Dictionary with masks for the data to select only part of the data. Uses same keywords as data dictionary. Default value = None. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. jk_group_name : str, optional Group in output file (hdf5) where jackknife realisations are stored. Default value is \"\". ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. Returns ------- ndarrays $\\xi_{gg}$ and $\\xi_{g+}$, r_p bins, pi bins, S+D, DD, RR (if no output file is specified) \"\"\" if masks == None: positions = self.data[\"Position\"] positions_shape_sample = self.data[\"Position_shape_sample\"] axis_direction_v = self.data[\"Axis_Direction\"] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"] weight = self.data[\"weight\"] weight_shape = self.data[\"weight_shape_sample\"] else: positions = self.data[\"Position\"][masks[\"Position\"]] positions_shape_sample = self.data[\"Position_shape_sample\"][masks[\"Position_shape_sample\"]] axis_direction_v = self.data[\"Axis_Direction\"][masks[\"Axis_Direction\"]] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"][masks[\"q\"]] try: weight_mask = masks[\"weight\"] except: masks[\"weight\"] = np.ones(self.Num_position, dtype=bool) masks[\"weight\"][sum(masks[\"Position\"]):self.Num_position] = 0 try: weight_mask = masks[\"weight_shape_sample\"] except: masks[\"weight_shape_sample\"] = np.ones(self.Num_shape, dtype=bool) masks[\"weight_shape_sample\"][sum(masks[\"Position_shape_sample\"]):self.Num_shape] = 0 weight = self.data[\"weight\"][masks[\"weight\"]] weight_shape = self.data[\"weight_shape_sample\"][masks[\"weight_shape_sample\"]] Num_position = len(positions) Num_shape = len(positions_shape_sample) print( f\"There are {Num_shape} galaxies in the shape sample and {Num_position} galaxies in the position sample.\") LOS_ind = self.data[\"LOS\"] # eg 2 for z axis not_LOS = np.array([0, 1, 2])[np.isin([0, 1, 2], LOS_ind, invert=True)] # eg 0,1 for x&y if ellipticity == 'distortion': e = (1 - q ** 2) / (1 + q ** 2) # size of ellipticity elif ellipticity == 'ellipticity': e = (1 - q) / (1 + q) else: raise ValueError(\"Invalid value for ellipticity. Choose 'distortion' or 'ellipticity'.\") del q R = sum(weight_shape * (1 - e ** 2 / 2.0)) / sum(weight_shape) # R = 1 - np.mean(e ** 2) / 2.0 # responsitivity factor L3 = self.boxsize ** 3 # box volume sub_box_len_logrp = (np.log10(self.r_max) - np.log10(self.r_min)) / self.num_bins_r sub_box_len_pi = (self.pi_bins[-1] - self.pi_bins[0]) / self.num_bins_pi DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_g_plus = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_gg = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) for n in np.arange(0, len(positions)): # for Splus_D (calculate ellipticities around position sample) separation = positions_shape_sample - positions[n] if self.periodicity: separation[separation > self.L_0p5] -= self.boxsize # account for periodicity of box separation[separation < -self.L_0p5] += self.boxsize projected_sep = separation[:, not_LOS] LOS = separation[:, LOS_ind] del separation separation_len = np.sqrt(np.sum(projected_sep ** 2, axis=1)) with np.errstate(invalid='ignore'): separation_dir = (projected_sep.transpose() / separation_len).transpose() # normalisation of rp del projected_sep phi = np.arccos(self.calculate_dot_product_arrays(separation_dir, axis_direction)) # [0,pi] del separation_dir e_plus, e_cross = self.get_ellipticity(e, phi) del phi e_plus[np.isnan(e_plus)] = 0.0 e_cross[np.isnan(e_cross)] = 0.0 # get the indices for the binning mask = (separation_len >= self.r_bins[0]) * (separation_len < self.r_bins[-1]) * ( LOS >= self.pi_bins[0]) * (LOS < self.pi_bins[-1]) ind_r = np.floor( np.log10(separation_len[mask]) / sub_box_len_logrp - np.log10(self.r_bins[0]) / sub_box_len_logrp ) del separation_len ind_r = np.array(ind_r, dtype=int) ind_pi = np.floor( LOS[mask] / sub_box_len_pi - self.pi_bins[0] / sub_box_len_pi ) # need length of LOS, so only positive values del LOS ind_pi = np.array(ind_pi, dtype=int) if np.any(ind_pi == self.num_bins_pi): ind_pi[ind_pi >= self.num_bins_pi] -= 1 if np.any(ind_r == self.num_bins_r): ind_r[ind_r >= self.num_bins_r] -= 1 np.add.at(Splus_D, (ind_r, ind_pi), (weight[n] * weight_shape[mask] * e_plus[mask]) / (2 * R)) np.add.at(Scross_D, (ind_r, ind_pi), (weight[n] * weight_shape[mask] * e_cross[mask]) / (2 * R)) del e_plus, e_cross np.add.at(DD, (ind_r, ind_pi), weight[n] * weight_shape[mask]) # if Num_position == Num_shape: # corrtype = \"auto\" # DD = DD / 2.0 # auto correlation, all pairs are double # else: corrtype = \"cross\" for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_g_plus[i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], self.pi_bins[p + 1], self.pi_bins[p], L3, \"cross\", Num_position, Num_shape) RR_gg[i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], self.pi_bins[p + 1], self.pi_bins[p], L3, corrtype, Num_position, Num_shape) correlation = Splus_D / RR_g_plus # (Splus_D - Splus_R) / RR_g_plus xi_g_cross = Scross_D / RR_g_plus # (Scross_D - Scross_R) / RR_g_plus dsep = (self.r_bins[1:] - self.r_bins[:-1]) / 2.0 separation_bins = self.r_bins[:-1] + abs(dsep) # middle of bins dpi = (self.pi_bins[1:] - self.pi_bins[:-1]) / 2.0 pi_bins = self.pi_bins[:-1] + abs(dpi) # middle of bins if (self.output_file_name != None) and (return_output == False): output_file = h5py.File(self.output_file_name, \"a\") group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_g_plus/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=correlation) write_dataset_hdf5(group, dataset_name + \"_SplusD\", data=Splus_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_plus\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_g_cross/{jk_group_name}\") write_dataset_hdf5(group, dataset_name + \"_ScrossD\", data=Scross_D) write_dataset_hdf5(group, dataset_name, data=xi_g_cross) write_dataset_hdf5(group, dataset_name + \"_RR_g_cross\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_gg/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=(DD / RR_gg) - 1) write_dataset_hdf5(group, dataset_name + \"_DD\", data=DD) write_dataset_hdf5(group, dataset_name + \"_RR_gg\", data=RR_gg) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) output_file.close() return else: return correlation, (DD / RR_gg) - 1, separation_bins, pi_bins, Splus_D, DD, RR_g_plus def _measure_xi_rp_pi_box_tree(self, dataset_name, masks=None, return_output=False, jk_group_name=\"\", ellipticity='distortion'): r\"\"\"Measures the projected correlation functions, $\\xi_{gg}$ and $\\xi_{g+}$, in (rp, pi) bins for an object created with MeasureIABox. Uses 1 CPU. Uses KDTree for speedup. Parameters ---------- dataset_name : str Name of the dataset in the output file. masks : dict or NoneType, optional Dictionary with masks for the data to select only part of the data. Uses same keywords as data dictionary. Default value = None. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. jk_group_name : str, optional Group in output file (hdf5) where jackknife realisations are stored. Default value is \"\". ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. Returns ------- ndarrays $\\xi_{gg}$ and $\\xi_{g+}$, r_p bins, pi bins, S+D, DD, RR (if no output file is specified) \"\"\" if masks == None: positions = self.data[\"Position\"] positions_shape_sample = self.data[\"Position_shape_sample\"] axis_direction_v = self.data[\"Axis_Direction\"] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"] weight = self.data[\"weight\"] weight_shape = self.data[\"weight_shape_sample\"] else: positions = self.data[\"Position\"][masks[\"Position\"]] positions_shape_sample = self.data[\"Position_shape_sample\"][masks[\"Position_shape_sample\"]] axis_direction_v = self.data[\"Axis_Direction\"][masks[\"Axis_Direction\"]] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"][masks[\"q\"]] try: weight_mask = masks[\"weight\"] except: masks[\"weight\"] = np.ones(self.Num_position, dtype=bool) masks[\"weight\"][sum(masks[\"Position\"]):self.Num_position] = 0 try: weight_mask = masks[\"weight_shape_sample\"] except: masks[\"weight_shape_sample\"] = np.ones(self.Num_shape, dtype=bool) masks[\"weight_shape_sample\"][sum(masks[\"Position_shape_sample\"]):self.Num_shape] = 0 weight = self.data[\"weight\"][masks[\"weight\"]] weight_shape = self.data[\"weight_shape_sample\"][masks[\"weight_shape_sample\"]] # masking changes the number of galaxies Num_position = len(positions) # number of halos in position sample Num_shape = len(positions_shape_sample) # number of halos in shape sample LOS_ind = self.data[\"LOS\"] # eg 2 for z axis not_LOS = np.array([0, 1, 2])[np.isin([0, 1, 2], LOS_ind, invert=True)] # eg 0,1 for x&y if ellipticity == 'distortion': e = (1 - q ** 2) / (1 + q ** 2) # size of ellipticity elif ellipticity == 'ellipticity': e = (1 - q) / (1 + q) else: raise ValueError(\"Invalid value for ellipticity. Choose 'distortion' or 'ellipticity'.\") del q R = sum(weight_shape * (1 - e ** 2 / 2.0)) / sum(weight_shape) # R = 1 - np.mean(e ** 2) / 2.0 # responsitivity factor L3 = self.boxsize ** 3 # box volume sub_box_len_logrp = (np.log10(self.r_max) - np.log10(self.r_min)) / self.num_bins_r sub_box_len_pi = (self.pi_bins[-1] - self.pi_bins[0]) / self.num_bins_pi DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_g_plus = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_gg = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) print( f\"There are {Num_shape} galaxies in the shape sample and {Num_position} galaxies in the position sample.\") pos_tree = KDTree(positions[:, not_LOS], boxsize=self.boxsize) for i in np.arange(0, len(positions_shape_sample), 100): i2 = min(len(positions_shape_sample), i + 100) positions_shape_sample_i = positions_shape_sample[i:i2] axis_direction_i = axis_direction[i:i2] e_i = e[i:i2] weight_shape_i = weight_shape[i:i2] shape_tree = KDTree(positions_shape_sample_i[:, not_LOS], boxsize=self.boxsize) ind_min_i = shape_tree.query_ball_tree(pos_tree, self.r_min) ind_max_i = shape_tree.query_ball_tree(pos_tree, self.r_max) ind_rbin_i = self.setdiff2D(ind_max_i, ind_min_i) for n in np.arange(0, len(positions_shape_sample_i)): # CHANGE2: loop now over shapes, not positions if len(ind_rbin_i[n]) > 0: # for Splus_D (calculate ellipticities around position sample) separation = positions_shape_sample_i[n] - positions[ind_rbin_i[n]] # CHANGE1 & CHANGE2 if self.periodicity: separation[separation > self.L_0p5] -= self.boxsize # account for periodicity of box separation[separation < -self.L_0p5] += self.boxsize projected_sep = separation[:, not_LOS] LOS = separation[:, LOS_ind] separation_len = np.sqrt(np.sum(projected_sep ** 2, axis=1)) with np.errstate(invalid='ignore'): separation_dir = (projected_sep.transpose() / separation_len).transpose() # normalisation of rp del projected_sep, separation phi = np.arccos( separation_dir[:, 0] * axis_direction_i[n, 0] + separation_dir[:, 1] * axis_direction_i[ n, 1]) # CHANGE2 e_plus, e_cross = self.get_ellipticity(e_i[n], phi) # CHANGE2 del phi, separation_dir e_plus[np.isnan(e_plus)] = 0.0 e_cross[np.isnan(e_cross)] = 0.0 # get the indices for the binning mask = (separation_len >= self.r_bins[0]) * (separation_len < self.r_bins[-1]) * ( LOS >= self.pi_bins[0]) * (LOS < self.pi_bins[-1]) ind_r = np.floor( np.log10(separation_len[mask]) / sub_box_len_logrp - np.log10( self.r_bins[0]) / sub_box_len_logrp ) ind_r = np.array(ind_r, dtype=int) ind_pi = np.floor( LOS[mask] / sub_box_len_pi - self.pi_bins[0] / sub_box_len_pi ) # need length of LOS, so only positive values ind_pi = np.array(ind_pi, dtype=int) if np.any(ind_pi == self.num_bins_pi): ind_pi[ind_pi >= self.num_bins_pi] -= 1 if np.any(ind_r == self.num_bins_r): ind_r[ind_r >= self.num_bins_r] -= 1 np.add.at(Splus_D, (ind_r, ind_pi), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_plus[mask]) / (2 * R)) np.add.at(Scross_D, (ind_r, ind_pi), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_cross[mask]) / (2 * R)) del e_plus, e_cross, separation_len np.add.at(DD, (ind_r, ind_pi), weight[ind_rbin_i[n]][mask] * weight_shape_i[n]) # if Num_position == Num_shape: # corrtype = \"auto\" # DD = DD / 2.0 # auto correlation, all pairs are double # else: corrtype = \"cross\" for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_g_plus[i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], self.pi_bins[p + 1], self.pi_bins[p], L3, \"cross\", Num_position, Num_shape) RR_gg[i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], self.pi_bins[p + 1], self.pi_bins[p], L3, corrtype, Num_position, Num_shape) correlation = Splus_D / RR_g_plus # (Splus_D - Splus_R) / RR_g_plus xi_g_cross = Scross_D / RR_g_plus # (Scross_D - Scross_R) / RR_g_plus dsep = (self.r_bins[1:] - self.r_bins[:-1]) / 2.0 separation_bins = self.r_bins[:-1] + abs(dsep) # middle of bins dpi = (self.pi_bins[1:] - self.pi_bins[:-1]) / 2.0 pi_bins = self.pi_bins[:-1] + abs(dpi) # middle of bins if (self.output_file_name != None) & return_output == False: output_file = h5py.File(self.output_file_name, \"a\") group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_g_plus/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=correlation) write_dataset_hdf5(group, dataset_name + \"_SplusD\", data=Splus_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_plus\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_g_cross/{jk_group_name}\") write_dataset_hdf5(group, dataset_name + \"_ScrossD\", data=Scross_D) write_dataset_hdf5(group, dataset_name, data=xi_g_cross) write_dataset_hdf5(group, dataset_name + \"_RR_g_cross\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_gg/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=(DD / RR_gg) - 1) write_dataset_hdf5(group, dataset_name + \"_DD\", data=DD) write_dataset_hdf5(group, dataset_name + \"_RR_gg\", data=RR_gg) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) output_file.close() return else: return correlation, (DD / RR_gg) - 1, separation_bins, pi_bins, Splus_D, DD, RR_g_plus def _measure_xi_rp_pi_box_batch(self, i): r\"\"\"Measures components of $\\xi_{gg}$ and $\\xi_{g+}$ in (rp,pi) bins including jackknife realisations for a batch of indices from i to i+chunk_size. Support function for _measure_xi_rp_pi_box_jk_multiprocessing(). Parameters ---------- i: int Start index of the batch. Returns ------- ndarrays S+D, SxD, DD, DD_jk, S+D_jk where the _jk versions store the necessary information of DD of S+D for each jackknife realisation. \"\"\" if i + self.chunk_size > self.Num_shape_masked: i2 = self.Num_shape_masked else: i2 = i + self.chunk_size DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) positions_shape_sample_i = self.temp_data_obj.read_cat(\"positions_shape_sample\", [i, i2]) axis_direction_i = self.temp_data_obj.read_cat(\"axis_direction\", [i, i2]) weight_shape_i = self.temp_data_obj.read_cat(\"weight_shape\", [i, i2]) positions = self.temp_data_obj.read_cat(\"positions\") weight = self.temp_data_obj.read_cat(\"weight\") e_i = self.e[i:i2] shape_tree = KDTree(positions_shape_sample_i[:, self.not_LOS], boxsize=self.boxsize) ind_min_i = shape_tree.query_ball_tree(self.pos_tree, self.r_min) ind_max_i = shape_tree.query_ball_tree(self.pos_tree, self.r_max) ind_rbin_i = self.setdiff2D(ind_max_i, ind_min_i) for n in np.arange(0, len(positions_shape_sample_i)): # CHANGE2: loop now over shapes, not positions if len(ind_rbin_i[n]) > 0: # for Splus_D (calculate ellipticities around position sample) separation = positions_shape_sample_i[n] - positions[ind_rbin_i[n]] # CHANGE1 & CHANGE2 if self.periodicity: separation[separation > self.L_0p5] -= self.boxsize # account for periodicity of box separation[separation < -self.L_0p5] += self.boxsize projected_sep = separation[:, self.not_LOS] LOS = separation[:, self.LOS_ind] separation_len = np.sqrt(np.sum(projected_sep ** 2, axis=1)) with np.errstate(invalid='ignore'): separation_dir = (projected_sep.transpose() / separation_len).transpose() # normalisation of rp del projected_sep, separation phi = np.arccos( separation_dir[:, 0] * axis_direction_i[n, 0] + separation_dir[:, 1] * axis_direction_i[ n, 1]) # CHANGE2 e_plus, e_cross = self.get_ellipticity(e_i[n], phi) # CHANGE2 del phi, separation_dir e_plus[np.isnan(e_plus)] = 0.0 e_cross[np.isnan(e_cross)] = 0.0 # get the indices for the binning mask = (separation_len >= self.r_bins[0]) * (separation_len < self.r_bins[-1]) * ( LOS >= self.pi_bins[0]) * (LOS < self.pi_bins[-1]) ind_r = np.floor( np.log10(separation_len[mask]) / self.sub_box_len_logrp - np.log10( self.r_bins[0]) / self.sub_box_len_logrp ) ind_r = np.array(ind_r, dtype=int) ind_pi = np.floor( LOS[mask] / self.sub_box_len_pi - self.pi_bins[0] / self.sub_box_len_pi ) # need length of LOS, so only positive values ind_pi = np.array(ind_pi, dtype=int) if np.any(ind_pi == self.num_bins_pi): ind_pi[ind_pi >= self.num_bins_pi] -= 1 if np.any(ind_r == self.num_bins_r): ind_r[ind_r >= self.num_bins_r] -= 1 np.add.at(Splus_D, (ind_r, ind_pi), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_plus[mask]) / (2 * self.R)) np.add.at(Scross_D, (ind_r, ind_pi), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_cross[mask]) / (2 * self.R)) del separation_len, e_cross, e_plus np.add.at(DD, (ind_r, ind_pi), weight[ind_rbin_i[n]][mask] * weight_shape_i[n]) return Splus_D, Scross_D, DD def _measure_xi_rp_pi_box_multiprocessing(self, dataset_name, temp_file_path, masks=None, return_output=False, jk_group_name=\"\", num_nodes=1, chunk_size=1000, ellipticity='distortion'): r\"\"\"Measures the projected correlation functions, $\\xi_{gg}$ and $\\xi_{g+}$, in (rp, pi) bins for an object created with MeasureIABox. Uses >1 CPU. Uses KDTree for speedup. Parameters ---------- dataset_name : str Name of the dataset in the output file. temp_file_path : str or NoneType, optional Path to where the data is temporarily stored [file name generated automatically]. num_nodes : int, optional Number of CPUs used in the multiprocessing. Default is 1. masks : dict or NoneType, optional Dictionary with masks for the data to select only part of the data. Uses same keywords as data dictionary. Default value = None. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. jk_group_name : str, optional Group in output file (hdf5) where jackknife realisations are stored. Default value is \"\". ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. Returns ------- ndarrays $\\xi_{gg}$ and $\\xi_{g+}$, r_p bins, pi bins, S+D, DD, RR (if no output file is specified) \"\"\" if masks == None: positions = self.data[\"Position\"] positions_shape_sample = self.data[\"Position_shape_sample\"] axis_direction_v = self.data[\"Axis_Direction\"] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"] weight = self.data[\"weight\"] weight_shape = self.data[\"weight_shape_sample\"] else: positions = self.data[\"Position\"][masks[\"Position\"]] positions_shape_sample = self.data[\"Position_shape_sample\"][masks[\"Position_shape_sample\"]] axis_direction_v = self.data[\"Axis_Direction\"][masks[\"Axis_Direction\"]] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"][masks[\"q\"]] try: weight_mask = masks[\"weight\"] except: masks[\"weight\"] = np.ones(self.Num_position, dtype=bool) masks[\"weight\"][sum(masks[\"Position\"]):self.Num_position] = 0 try: weight_mask = masks[\"weight_shape_sample\"] except: masks[\"weight_shape_sample\"] = np.ones(self.Num_shape, dtype=bool) masks[\"weight_shape_sample\"][sum(masks[\"Position_shape_sample\"]):self.Num_shape] = 0 weight = self.data[\"weight\"][masks[\"weight\"]] weight_shape = self.data[\"weight_shape_sample\"][masks[\"weight_shape_sample\"]] # masking changes the number of galaxies self.Num_position_masked = len(positions) self.Num_shape_masked = len(positions_shape_sample) print( f\"There are {self.Num_shape_masked} galaxies in the shape sample and {self.Num_position_masked} galaxies in the position sample.\") # create temp hdf5 from which data can be read. del self.data, but save it in this method to reduce RAM figname_dataset_name = dataset_name if \"/\" in dataset_name: figname_dataset_name = figname_dataset_name.replace(\"/\", \"_\") if \".\" in dataset_name: figname_dataset_name = figname_dataset_name.replace(\".\", \"p\") file_temp = h5py.File(f\"{temp_file_path}/w_{self.simname}_temp_data_{figname_dataset_name}.hdf5\", \"w\") write_dataset_hdf5(file_temp, \"positions\", positions) write_dataset_hdf5(file_temp, \"weight\", weight) write_dataset_hdf5(file_temp, \"weight_shape\", weight_shape) write_dataset_hdf5(file_temp, \"positions_shape_sample\", positions_shape_sample) write_dataset_hdf5(file_temp, \"axis_direction\", axis_direction) file_temp.close() self.temp_data_obj = ReadData(self.simname, f\"w_{self.simname}_temp_data_{figname_dataset_name}\", None, data_path=temp_file_path) self.LOS_ind = self.data[\"LOS\"] # eg 2 for z axis self.not_LOS = np.array([0, 1, 2])[np.isin([0, 1, 2], self.LOS_ind, invert=True)] # eg 0,1 for x&y if ellipticity == 'distortion': self.e = (1 - q ** 2) / (1 + q ** 2) # size of ellipticity elif ellipticity == 'ellipticity': self.e = (1 - q) / (1 + q) else: raise ValueError(\"Invalid value for ellipticity. Choose 'distortion' or 'ellipticity'.\") self.R = sum(weight_shape * (1 - self.e ** 2 / 2.0)) / sum(weight_shape) # self.R = 1 - np.mean(self.e ** 2) / 2.0 # responsitivity factor L3 = self.boxsize ** 3 # box volume self.sub_box_len_logrp = (np.log10(self.r_max) - np.log10(self.r_min)) / self.num_bins_r self.sub_box_len_pi = (self.pi_bins[-1] - self.pi_bins[0]) / self.num_bins_pi DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_g_plus = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_gg = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) data_temp = self.data # make sure data is not sent to every CPU self.data = None self.pos_tree = KDTree(positions[:, self.not_LOS], boxsize=self.boxsize) indices = np.arange(0, len(positions_shape_sample), chunk_size) self.chunk_size = chunk_size with Pool(num_nodes) as p: result = p.map(self._measure_xi_rp_pi_box_batch, indices) os.remove( f\"{temp_file_path}/w_{self.simname}_temp_data_{figname_dataset_name}.hdf5\") self.data = data_temp del data_temp for i in np.arange(len(result)): Splus_D += result[i][0] Scross_D += result[i][1] DD += result[i][2] # if Num_position == Num_shape: # corrtype = \"auto\" # DD = DD / 2.0 # auto correlation, all pairs are double # else: corrtype = \"cross\" for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_g_plus[i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], self.pi_bins[p + 1], self.pi_bins[p], L3, \"cross\", self.Num_position_masked, self.Num_shape_masked) RR_gg[i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], self.pi_bins[p + 1], self.pi_bins[p], L3, corrtype, self.Num_position_masked, self.Num_shape_masked) correlation = Splus_D / RR_g_plus # (Splus_D - Splus_R) / RR_g_plus xi_g_cross = Scross_D / RR_g_plus # (Scross_D - Scross_R) / RR_g_plus dsep = (self.r_bins[1:] - self.r_bins[:-1]) / 2.0 separation_bins = self.r_bins[:-1] + abs(dsep) # middle of bins dpi = (self.pi_bins[1:] - self.pi_bins[:-1]) / 2.0 pi_bins = self.pi_bins[:-1] + abs(dpi) # middle of bins if (self.output_file_name != None) & return_output == False: output_file = h5py.File(self.output_file_name, \"a\") group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_g_plus/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=correlation) write_dataset_hdf5(group, dataset_name + \"_SplusD\", data=Splus_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_plus\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_g_cross/{jk_group_name}\") write_dataset_hdf5(group, dataset_name + \"_ScrossD\", data=Scross_D) write_dataset_hdf5(group, dataset_name, data=xi_g_cross) write_dataset_hdf5(group, dataset_name + \"_RR_g_cross\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_gg/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=(DD / RR_gg) - 1) write_dataset_hdf5(group, dataset_name + \"_DD\", data=DD) write_dataset_hdf5(group, dataset_name + \"_RR_gg\", data=RR_gg) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) output_file.close() return else: return correlation, (DD / RR_gg) - 1, separation_bins, pi_bins, Splus_D, DD, RR_g_plus __init__(data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True) The init method of the MeasureWSimulations class. Notes Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. Source code in src/measureia/measure_w_box.py def __init__( self, data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True, ): \"\"\" The __init__ method of the MeasureWSimulations class. Notes ----- Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. \"\"\" super().__init__(data, output_file_name, simulation, snapshot, separation_limits, num_bins_r, num_bins_pi, pi_max, boxsize, periodicity) return","title":"MeasureWBox"},{"location":"api/MeasureWBox/#measurewbox","text":"","title":"MeasureWBox"},{"location":"api/MeasureWBox/#measureia.MeasureWBox","text":"Bases: MeasureIABase , ReadData Class that contains all methods for the measurements of \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) for \\(w_{gg}\\) and \\(w_{g+}\\) with Cartesian simulation data. Methods: Name Description _measure_xi_rp_pi_box_brute Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (rp, pi) grid binning in a periodic box using 1 CPU. _measure_xi_rp_pi_box_tree Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (rp, pi) grid binning in a periodic box using 1 CPU and KDTree for extra speed. _measure_xi_rp_pi_box_batch Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (rp, pi) grid binning in a periodic box using 1 CPU for a batch of indices. Support function of _measure_xi_rp_pi_box_multiprocessing(). _measure_xi_rp_pi_box_multiprocessing Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (rp, pi) grid binning in a periodic box using >1 CPUs. Notes Inherits attributes from 'SimInfo', where 'boxsize', 'L_0p5' and 'snap_group' are used in this class. Inherits attributes from 'MeasureIABase', where 'data', 'output_file_name', 'periodicity', 'Num_position', 'Num_shape', 'r_min', 'r_max', 'num_bins_r', 'num_bins_pi', 'r_bins', 'pi_bins', 'mu_r_bins' are used. Source code in src/measureia/measure_w_box.py class MeasureWBox(MeasureIABase, ReadData): r\"\"\"Class that contains all methods for the measurements of $\\xi_{gg}$ and $\\xi_{g+}$ for $w_{gg}$ and $w_{g+}$ with Cartesian simulation data. Methods ------- _measure_xi_rp_pi_box_brute() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (rp, pi) grid binning in a periodic box using 1 CPU. _measure_xi_rp_pi_box_tree() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (rp, pi) grid binning in a periodic box using 1 CPU and KDTree for extra speed. _measure_xi_rp_pi_box_batch() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (rp, pi) grid binning in a periodic box using 1 CPU for a batch of indices. Support function of _measure_xi_rp_pi_box_multiprocessing(). _measure_xi_rp_pi_box_multiprocessing() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (rp, pi) grid binning in a periodic box using >1 CPUs. Notes ----- Inherits attributes from 'SimInfo', where 'boxsize', 'L_0p5' and 'snap_group' are used in this class. Inherits attributes from 'MeasureIABase', where 'data', 'output_file_name', 'periodicity', 'Num_position', 'Num_shape', 'r_min', 'r_max', 'num_bins_r', 'num_bins_pi', 'r_bins', 'pi_bins', 'mu_r_bins' are used. \"\"\" def __init__( self, data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True, ): \"\"\" The __init__ method of the MeasureWSimulations class. Notes ----- Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. \"\"\" super().__init__(data, output_file_name, simulation, snapshot, separation_limits, num_bins_r, num_bins_pi, pi_max, boxsize, periodicity) return def _measure_xi_rp_pi_box_brute(self, dataset_name, masks=None, return_output=False, jk_group_name=\"\", ellipticity='distortion'): r\"\"\"Measures the projected correlation functions, $\\xi_{gg}$ and $\\xi_{g+}$, in (rp, pi) bins for an object created with MeasureIABox. Uses 1 CPU. Parameters ---------- dataset_name : str Name of the dataset in the output file. masks : dict or NoneType, optional Dictionary with masks for the data to select only part of the data. Uses same keywords as data dictionary. Default value = None. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. jk_group_name : str, optional Group in output file (hdf5) where jackknife realisations are stored. Default value is \"\". ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. Returns ------- ndarrays $\\xi_{gg}$ and $\\xi_{g+}$, r_p bins, pi bins, S+D, DD, RR (if no output file is specified) \"\"\" if masks == None: positions = self.data[\"Position\"] positions_shape_sample = self.data[\"Position_shape_sample\"] axis_direction_v = self.data[\"Axis_Direction\"] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"] weight = self.data[\"weight\"] weight_shape = self.data[\"weight_shape_sample\"] else: positions = self.data[\"Position\"][masks[\"Position\"]] positions_shape_sample = self.data[\"Position_shape_sample\"][masks[\"Position_shape_sample\"]] axis_direction_v = self.data[\"Axis_Direction\"][masks[\"Axis_Direction\"]] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"][masks[\"q\"]] try: weight_mask = masks[\"weight\"] except: masks[\"weight\"] = np.ones(self.Num_position, dtype=bool) masks[\"weight\"][sum(masks[\"Position\"]):self.Num_position] = 0 try: weight_mask = masks[\"weight_shape_sample\"] except: masks[\"weight_shape_sample\"] = np.ones(self.Num_shape, dtype=bool) masks[\"weight_shape_sample\"][sum(masks[\"Position_shape_sample\"]):self.Num_shape] = 0 weight = self.data[\"weight\"][masks[\"weight\"]] weight_shape = self.data[\"weight_shape_sample\"][masks[\"weight_shape_sample\"]] Num_position = len(positions) Num_shape = len(positions_shape_sample) print( f\"There are {Num_shape} galaxies in the shape sample and {Num_position} galaxies in the position sample.\") LOS_ind = self.data[\"LOS\"] # eg 2 for z axis not_LOS = np.array([0, 1, 2])[np.isin([0, 1, 2], LOS_ind, invert=True)] # eg 0,1 for x&y if ellipticity == 'distortion': e = (1 - q ** 2) / (1 + q ** 2) # size of ellipticity elif ellipticity == 'ellipticity': e = (1 - q) / (1 + q) else: raise ValueError(\"Invalid value for ellipticity. Choose 'distortion' or 'ellipticity'.\") del q R = sum(weight_shape * (1 - e ** 2 / 2.0)) / sum(weight_shape) # R = 1 - np.mean(e ** 2) / 2.0 # responsitivity factor L3 = self.boxsize ** 3 # box volume sub_box_len_logrp = (np.log10(self.r_max) - np.log10(self.r_min)) / self.num_bins_r sub_box_len_pi = (self.pi_bins[-1] - self.pi_bins[0]) / self.num_bins_pi DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_g_plus = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_gg = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) for n in np.arange(0, len(positions)): # for Splus_D (calculate ellipticities around position sample) separation = positions_shape_sample - positions[n] if self.periodicity: separation[separation > self.L_0p5] -= self.boxsize # account for periodicity of box separation[separation < -self.L_0p5] += self.boxsize projected_sep = separation[:, not_LOS] LOS = separation[:, LOS_ind] del separation separation_len = np.sqrt(np.sum(projected_sep ** 2, axis=1)) with np.errstate(invalid='ignore'): separation_dir = (projected_sep.transpose() / separation_len).transpose() # normalisation of rp del projected_sep phi = np.arccos(self.calculate_dot_product_arrays(separation_dir, axis_direction)) # [0,pi] del separation_dir e_plus, e_cross = self.get_ellipticity(e, phi) del phi e_plus[np.isnan(e_plus)] = 0.0 e_cross[np.isnan(e_cross)] = 0.0 # get the indices for the binning mask = (separation_len >= self.r_bins[0]) * (separation_len < self.r_bins[-1]) * ( LOS >= self.pi_bins[0]) * (LOS < self.pi_bins[-1]) ind_r = np.floor( np.log10(separation_len[mask]) / sub_box_len_logrp - np.log10(self.r_bins[0]) / sub_box_len_logrp ) del separation_len ind_r = np.array(ind_r, dtype=int) ind_pi = np.floor( LOS[mask] / sub_box_len_pi - self.pi_bins[0] / sub_box_len_pi ) # need length of LOS, so only positive values del LOS ind_pi = np.array(ind_pi, dtype=int) if np.any(ind_pi == self.num_bins_pi): ind_pi[ind_pi >= self.num_bins_pi] -= 1 if np.any(ind_r == self.num_bins_r): ind_r[ind_r >= self.num_bins_r] -= 1 np.add.at(Splus_D, (ind_r, ind_pi), (weight[n] * weight_shape[mask] * e_plus[mask]) / (2 * R)) np.add.at(Scross_D, (ind_r, ind_pi), (weight[n] * weight_shape[mask] * e_cross[mask]) / (2 * R)) del e_plus, e_cross np.add.at(DD, (ind_r, ind_pi), weight[n] * weight_shape[mask]) # if Num_position == Num_shape: # corrtype = \"auto\" # DD = DD / 2.0 # auto correlation, all pairs are double # else: corrtype = \"cross\" for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_g_plus[i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], self.pi_bins[p + 1], self.pi_bins[p], L3, \"cross\", Num_position, Num_shape) RR_gg[i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], self.pi_bins[p + 1], self.pi_bins[p], L3, corrtype, Num_position, Num_shape) correlation = Splus_D / RR_g_plus # (Splus_D - Splus_R) / RR_g_plus xi_g_cross = Scross_D / RR_g_plus # (Scross_D - Scross_R) / RR_g_plus dsep = (self.r_bins[1:] - self.r_bins[:-1]) / 2.0 separation_bins = self.r_bins[:-1] + abs(dsep) # middle of bins dpi = (self.pi_bins[1:] - self.pi_bins[:-1]) / 2.0 pi_bins = self.pi_bins[:-1] + abs(dpi) # middle of bins if (self.output_file_name != None) and (return_output == False): output_file = h5py.File(self.output_file_name, \"a\") group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_g_plus/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=correlation) write_dataset_hdf5(group, dataset_name + \"_SplusD\", data=Splus_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_plus\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_g_cross/{jk_group_name}\") write_dataset_hdf5(group, dataset_name + \"_ScrossD\", data=Scross_D) write_dataset_hdf5(group, dataset_name, data=xi_g_cross) write_dataset_hdf5(group, dataset_name + \"_RR_g_cross\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_gg/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=(DD / RR_gg) - 1) write_dataset_hdf5(group, dataset_name + \"_DD\", data=DD) write_dataset_hdf5(group, dataset_name + \"_RR_gg\", data=RR_gg) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) output_file.close() return else: return correlation, (DD / RR_gg) - 1, separation_bins, pi_bins, Splus_D, DD, RR_g_plus def _measure_xi_rp_pi_box_tree(self, dataset_name, masks=None, return_output=False, jk_group_name=\"\", ellipticity='distortion'): r\"\"\"Measures the projected correlation functions, $\\xi_{gg}$ and $\\xi_{g+}$, in (rp, pi) bins for an object created with MeasureIABox. Uses 1 CPU. Uses KDTree for speedup. Parameters ---------- dataset_name : str Name of the dataset in the output file. masks : dict or NoneType, optional Dictionary with masks for the data to select only part of the data. Uses same keywords as data dictionary. Default value = None. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. jk_group_name : str, optional Group in output file (hdf5) where jackknife realisations are stored. Default value is \"\". ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. Returns ------- ndarrays $\\xi_{gg}$ and $\\xi_{g+}$, r_p bins, pi bins, S+D, DD, RR (if no output file is specified) \"\"\" if masks == None: positions = self.data[\"Position\"] positions_shape_sample = self.data[\"Position_shape_sample\"] axis_direction_v = self.data[\"Axis_Direction\"] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"] weight = self.data[\"weight\"] weight_shape = self.data[\"weight_shape_sample\"] else: positions = self.data[\"Position\"][masks[\"Position\"]] positions_shape_sample = self.data[\"Position_shape_sample\"][masks[\"Position_shape_sample\"]] axis_direction_v = self.data[\"Axis_Direction\"][masks[\"Axis_Direction\"]] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"][masks[\"q\"]] try: weight_mask = masks[\"weight\"] except: masks[\"weight\"] = np.ones(self.Num_position, dtype=bool) masks[\"weight\"][sum(masks[\"Position\"]):self.Num_position] = 0 try: weight_mask = masks[\"weight_shape_sample\"] except: masks[\"weight_shape_sample\"] = np.ones(self.Num_shape, dtype=bool) masks[\"weight_shape_sample\"][sum(masks[\"Position_shape_sample\"]):self.Num_shape] = 0 weight = self.data[\"weight\"][masks[\"weight\"]] weight_shape = self.data[\"weight_shape_sample\"][masks[\"weight_shape_sample\"]] # masking changes the number of galaxies Num_position = len(positions) # number of halos in position sample Num_shape = len(positions_shape_sample) # number of halos in shape sample LOS_ind = self.data[\"LOS\"] # eg 2 for z axis not_LOS = np.array([0, 1, 2])[np.isin([0, 1, 2], LOS_ind, invert=True)] # eg 0,1 for x&y if ellipticity == 'distortion': e = (1 - q ** 2) / (1 + q ** 2) # size of ellipticity elif ellipticity == 'ellipticity': e = (1 - q) / (1 + q) else: raise ValueError(\"Invalid value for ellipticity. Choose 'distortion' or 'ellipticity'.\") del q R = sum(weight_shape * (1 - e ** 2 / 2.0)) / sum(weight_shape) # R = 1 - np.mean(e ** 2) / 2.0 # responsitivity factor L3 = self.boxsize ** 3 # box volume sub_box_len_logrp = (np.log10(self.r_max) - np.log10(self.r_min)) / self.num_bins_r sub_box_len_pi = (self.pi_bins[-1] - self.pi_bins[0]) / self.num_bins_pi DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_g_plus = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_gg = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) print( f\"There are {Num_shape} galaxies in the shape sample and {Num_position} galaxies in the position sample.\") pos_tree = KDTree(positions[:, not_LOS], boxsize=self.boxsize) for i in np.arange(0, len(positions_shape_sample), 100): i2 = min(len(positions_shape_sample), i + 100) positions_shape_sample_i = positions_shape_sample[i:i2] axis_direction_i = axis_direction[i:i2] e_i = e[i:i2] weight_shape_i = weight_shape[i:i2] shape_tree = KDTree(positions_shape_sample_i[:, not_LOS], boxsize=self.boxsize) ind_min_i = shape_tree.query_ball_tree(pos_tree, self.r_min) ind_max_i = shape_tree.query_ball_tree(pos_tree, self.r_max) ind_rbin_i = self.setdiff2D(ind_max_i, ind_min_i) for n in np.arange(0, len(positions_shape_sample_i)): # CHANGE2: loop now over shapes, not positions if len(ind_rbin_i[n]) > 0: # for Splus_D (calculate ellipticities around position sample) separation = positions_shape_sample_i[n] - positions[ind_rbin_i[n]] # CHANGE1 & CHANGE2 if self.periodicity: separation[separation > self.L_0p5] -= self.boxsize # account for periodicity of box separation[separation < -self.L_0p5] += self.boxsize projected_sep = separation[:, not_LOS] LOS = separation[:, LOS_ind] separation_len = np.sqrt(np.sum(projected_sep ** 2, axis=1)) with np.errstate(invalid='ignore'): separation_dir = (projected_sep.transpose() / separation_len).transpose() # normalisation of rp del projected_sep, separation phi = np.arccos( separation_dir[:, 0] * axis_direction_i[n, 0] + separation_dir[:, 1] * axis_direction_i[ n, 1]) # CHANGE2 e_plus, e_cross = self.get_ellipticity(e_i[n], phi) # CHANGE2 del phi, separation_dir e_plus[np.isnan(e_plus)] = 0.0 e_cross[np.isnan(e_cross)] = 0.0 # get the indices for the binning mask = (separation_len >= self.r_bins[0]) * (separation_len < self.r_bins[-1]) * ( LOS >= self.pi_bins[0]) * (LOS < self.pi_bins[-1]) ind_r = np.floor( np.log10(separation_len[mask]) / sub_box_len_logrp - np.log10( self.r_bins[0]) / sub_box_len_logrp ) ind_r = np.array(ind_r, dtype=int) ind_pi = np.floor( LOS[mask] / sub_box_len_pi - self.pi_bins[0] / sub_box_len_pi ) # need length of LOS, so only positive values ind_pi = np.array(ind_pi, dtype=int) if np.any(ind_pi == self.num_bins_pi): ind_pi[ind_pi >= self.num_bins_pi] -= 1 if np.any(ind_r == self.num_bins_r): ind_r[ind_r >= self.num_bins_r] -= 1 np.add.at(Splus_D, (ind_r, ind_pi), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_plus[mask]) / (2 * R)) np.add.at(Scross_D, (ind_r, ind_pi), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_cross[mask]) / (2 * R)) del e_plus, e_cross, separation_len np.add.at(DD, (ind_r, ind_pi), weight[ind_rbin_i[n]][mask] * weight_shape_i[n]) # if Num_position == Num_shape: # corrtype = \"auto\" # DD = DD / 2.0 # auto correlation, all pairs are double # else: corrtype = \"cross\" for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_g_plus[i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], self.pi_bins[p + 1], self.pi_bins[p], L3, \"cross\", Num_position, Num_shape) RR_gg[i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], self.pi_bins[p + 1], self.pi_bins[p], L3, corrtype, Num_position, Num_shape) correlation = Splus_D / RR_g_plus # (Splus_D - Splus_R) / RR_g_plus xi_g_cross = Scross_D / RR_g_plus # (Scross_D - Scross_R) / RR_g_plus dsep = (self.r_bins[1:] - self.r_bins[:-1]) / 2.0 separation_bins = self.r_bins[:-1] + abs(dsep) # middle of bins dpi = (self.pi_bins[1:] - self.pi_bins[:-1]) / 2.0 pi_bins = self.pi_bins[:-1] + abs(dpi) # middle of bins if (self.output_file_name != None) & return_output == False: output_file = h5py.File(self.output_file_name, \"a\") group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_g_plus/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=correlation) write_dataset_hdf5(group, dataset_name + \"_SplusD\", data=Splus_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_plus\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_g_cross/{jk_group_name}\") write_dataset_hdf5(group, dataset_name + \"_ScrossD\", data=Scross_D) write_dataset_hdf5(group, dataset_name, data=xi_g_cross) write_dataset_hdf5(group, dataset_name + \"_RR_g_cross\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_gg/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=(DD / RR_gg) - 1) write_dataset_hdf5(group, dataset_name + \"_DD\", data=DD) write_dataset_hdf5(group, dataset_name + \"_RR_gg\", data=RR_gg) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) output_file.close() return else: return correlation, (DD / RR_gg) - 1, separation_bins, pi_bins, Splus_D, DD, RR_g_plus def _measure_xi_rp_pi_box_batch(self, i): r\"\"\"Measures components of $\\xi_{gg}$ and $\\xi_{g+}$ in (rp,pi) bins including jackknife realisations for a batch of indices from i to i+chunk_size. Support function for _measure_xi_rp_pi_box_jk_multiprocessing(). Parameters ---------- i: int Start index of the batch. Returns ------- ndarrays S+D, SxD, DD, DD_jk, S+D_jk where the _jk versions store the necessary information of DD of S+D for each jackknife realisation. \"\"\" if i + self.chunk_size > self.Num_shape_masked: i2 = self.Num_shape_masked else: i2 = i + self.chunk_size DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) positions_shape_sample_i = self.temp_data_obj.read_cat(\"positions_shape_sample\", [i, i2]) axis_direction_i = self.temp_data_obj.read_cat(\"axis_direction\", [i, i2]) weight_shape_i = self.temp_data_obj.read_cat(\"weight_shape\", [i, i2]) positions = self.temp_data_obj.read_cat(\"positions\") weight = self.temp_data_obj.read_cat(\"weight\") e_i = self.e[i:i2] shape_tree = KDTree(positions_shape_sample_i[:, self.not_LOS], boxsize=self.boxsize) ind_min_i = shape_tree.query_ball_tree(self.pos_tree, self.r_min) ind_max_i = shape_tree.query_ball_tree(self.pos_tree, self.r_max) ind_rbin_i = self.setdiff2D(ind_max_i, ind_min_i) for n in np.arange(0, len(positions_shape_sample_i)): # CHANGE2: loop now over shapes, not positions if len(ind_rbin_i[n]) > 0: # for Splus_D (calculate ellipticities around position sample) separation = positions_shape_sample_i[n] - positions[ind_rbin_i[n]] # CHANGE1 & CHANGE2 if self.periodicity: separation[separation > self.L_0p5] -= self.boxsize # account for periodicity of box separation[separation < -self.L_0p5] += self.boxsize projected_sep = separation[:, self.not_LOS] LOS = separation[:, self.LOS_ind] separation_len = np.sqrt(np.sum(projected_sep ** 2, axis=1)) with np.errstate(invalid='ignore'): separation_dir = (projected_sep.transpose() / separation_len).transpose() # normalisation of rp del projected_sep, separation phi = np.arccos( separation_dir[:, 0] * axis_direction_i[n, 0] + separation_dir[:, 1] * axis_direction_i[ n, 1]) # CHANGE2 e_plus, e_cross = self.get_ellipticity(e_i[n], phi) # CHANGE2 del phi, separation_dir e_plus[np.isnan(e_plus)] = 0.0 e_cross[np.isnan(e_cross)] = 0.0 # get the indices for the binning mask = (separation_len >= self.r_bins[0]) * (separation_len < self.r_bins[-1]) * ( LOS >= self.pi_bins[0]) * (LOS < self.pi_bins[-1]) ind_r = np.floor( np.log10(separation_len[mask]) / self.sub_box_len_logrp - np.log10( self.r_bins[0]) / self.sub_box_len_logrp ) ind_r = np.array(ind_r, dtype=int) ind_pi = np.floor( LOS[mask] / self.sub_box_len_pi - self.pi_bins[0] / self.sub_box_len_pi ) # need length of LOS, so only positive values ind_pi = np.array(ind_pi, dtype=int) if np.any(ind_pi == self.num_bins_pi): ind_pi[ind_pi >= self.num_bins_pi] -= 1 if np.any(ind_r == self.num_bins_r): ind_r[ind_r >= self.num_bins_r] -= 1 np.add.at(Splus_D, (ind_r, ind_pi), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_plus[mask]) / (2 * self.R)) np.add.at(Scross_D, (ind_r, ind_pi), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_cross[mask]) / (2 * self.R)) del separation_len, e_cross, e_plus np.add.at(DD, (ind_r, ind_pi), weight[ind_rbin_i[n]][mask] * weight_shape_i[n]) return Splus_D, Scross_D, DD def _measure_xi_rp_pi_box_multiprocessing(self, dataset_name, temp_file_path, masks=None, return_output=False, jk_group_name=\"\", num_nodes=1, chunk_size=1000, ellipticity='distortion'): r\"\"\"Measures the projected correlation functions, $\\xi_{gg}$ and $\\xi_{g+}$, in (rp, pi) bins for an object created with MeasureIABox. Uses >1 CPU. Uses KDTree for speedup. Parameters ---------- dataset_name : str Name of the dataset in the output file. temp_file_path : str or NoneType, optional Path to where the data is temporarily stored [file name generated automatically]. num_nodes : int, optional Number of CPUs used in the multiprocessing. Default is 1. masks : dict or NoneType, optional Dictionary with masks for the data to select only part of the data. Uses same keywords as data dictionary. Default value = None. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. jk_group_name : str, optional Group in output file (hdf5) where jackknife realisations are stored. Default value is \"\". ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. Returns ------- ndarrays $\\xi_{gg}$ and $\\xi_{g+}$, r_p bins, pi bins, S+D, DD, RR (if no output file is specified) \"\"\" if masks == None: positions = self.data[\"Position\"] positions_shape_sample = self.data[\"Position_shape_sample\"] axis_direction_v = self.data[\"Axis_Direction\"] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"] weight = self.data[\"weight\"] weight_shape = self.data[\"weight_shape_sample\"] else: positions = self.data[\"Position\"][masks[\"Position\"]] positions_shape_sample = self.data[\"Position_shape_sample\"][masks[\"Position_shape_sample\"]] axis_direction_v = self.data[\"Axis_Direction\"][masks[\"Axis_Direction\"]] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"][masks[\"q\"]] try: weight_mask = masks[\"weight\"] except: masks[\"weight\"] = np.ones(self.Num_position, dtype=bool) masks[\"weight\"][sum(masks[\"Position\"]):self.Num_position] = 0 try: weight_mask = masks[\"weight_shape_sample\"] except: masks[\"weight_shape_sample\"] = np.ones(self.Num_shape, dtype=bool) masks[\"weight_shape_sample\"][sum(masks[\"Position_shape_sample\"]):self.Num_shape] = 0 weight = self.data[\"weight\"][masks[\"weight\"]] weight_shape = self.data[\"weight_shape_sample\"][masks[\"weight_shape_sample\"]] # masking changes the number of galaxies self.Num_position_masked = len(positions) self.Num_shape_masked = len(positions_shape_sample) print( f\"There are {self.Num_shape_masked} galaxies in the shape sample and {self.Num_position_masked} galaxies in the position sample.\") # create temp hdf5 from which data can be read. del self.data, but save it in this method to reduce RAM figname_dataset_name = dataset_name if \"/\" in dataset_name: figname_dataset_name = figname_dataset_name.replace(\"/\", \"_\") if \".\" in dataset_name: figname_dataset_name = figname_dataset_name.replace(\".\", \"p\") file_temp = h5py.File(f\"{temp_file_path}/w_{self.simname}_temp_data_{figname_dataset_name}.hdf5\", \"w\") write_dataset_hdf5(file_temp, \"positions\", positions) write_dataset_hdf5(file_temp, \"weight\", weight) write_dataset_hdf5(file_temp, \"weight_shape\", weight_shape) write_dataset_hdf5(file_temp, \"positions_shape_sample\", positions_shape_sample) write_dataset_hdf5(file_temp, \"axis_direction\", axis_direction) file_temp.close() self.temp_data_obj = ReadData(self.simname, f\"w_{self.simname}_temp_data_{figname_dataset_name}\", None, data_path=temp_file_path) self.LOS_ind = self.data[\"LOS\"] # eg 2 for z axis self.not_LOS = np.array([0, 1, 2])[np.isin([0, 1, 2], self.LOS_ind, invert=True)] # eg 0,1 for x&y if ellipticity == 'distortion': self.e = (1 - q ** 2) / (1 + q ** 2) # size of ellipticity elif ellipticity == 'ellipticity': self.e = (1 - q) / (1 + q) else: raise ValueError(\"Invalid value for ellipticity. Choose 'distortion' or 'ellipticity'.\") self.R = sum(weight_shape * (1 - self.e ** 2 / 2.0)) / sum(weight_shape) # self.R = 1 - np.mean(self.e ** 2) / 2.0 # responsitivity factor L3 = self.boxsize ** 3 # box volume self.sub_box_len_logrp = (np.log10(self.r_max) - np.log10(self.r_min)) / self.num_bins_r self.sub_box_len_pi = (self.pi_bins[-1] - self.pi_bins[0]) / self.num_bins_pi DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_g_plus = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_gg = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) data_temp = self.data # make sure data is not sent to every CPU self.data = None self.pos_tree = KDTree(positions[:, self.not_LOS], boxsize=self.boxsize) indices = np.arange(0, len(positions_shape_sample), chunk_size) self.chunk_size = chunk_size with Pool(num_nodes) as p: result = p.map(self._measure_xi_rp_pi_box_batch, indices) os.remove( f\"{temp_file_path}/w_{self.simname}_temp_data_{figname_dataset_name}.hdf5\") self.data = data_temp del data_temp for i in np.arange(len(result)): Splus_D += result[i][0] Scross_D += result[i][1] DD += result[i][2] # if Num_position == Num_shape: # corrtype = \"auto\" # DD = DD / 2.0 # auto correlation, all pairs are double # else: corrtype = \"cross\" for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_g_plus[i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], self.pi_bins[p + 1], self.pi_bins[p], L3, \"cross\", self.Num_position_masked, self.Num_shape_masked) RR_gg[i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], self.pi_bins[p + 1], self.pi_bins[p], L3, corrtype, self.Num_position_masked, self.Num_shape_masked) correlation = Splus_D / RR_g_plus # (Splus_D - Splus_R) / RR_g_plus xi_g_cross = Scross_D / RR_g_plus # (Scross_D - Scross_R) / RR_g_plus dsep = (self.r_bins[1:] - self.r_bins[:-1]) / 2.0 separation_bins = self.r_bins[:-1] + abs(dsep) # middle of bins dpi = (self.pi_bins[1:] - self.pi_bins[:-1]) / 2.0 pi_bins = self.pi_bins[:-1] + abs(dpi) # middle of bins if (self.output_file_name != None) & return_output == False: output_file = h5py.File(self.output_file_name, \"a\") group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_g_plus/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=correlation) write_dataset_hdf5(group, dataset_name + \"_SplusD\", data=Splus_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_plus\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_g_cross/{jk_group_name}\") write_dataset_hdf5(group, dataset_name + \"_ScrossD\", data=Scross_D) write_dataset_hdf5(group, dataset_name, data=xi_g_cross) write_dataset_hdf5(group, dataset_name + \"_RR_g_cross\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_gg/{jk_group_name}\") write_dataset_hdf5(group, dataset_name, data=(DD / RR_gg) - 1) write_dataset_hdf5(group, dataset_name + \"_DD\", data=DD) write_dataset_hdf5(group, dataset_name + \"_RR_gg\", data=RR_gg) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) output_file.close() return else: return correlation, (DD / RR_gg) - 1, separation_bins, pi_bins, Splus_D, DD, RR_g_plus","title":"MeasureWBox"},{"location":"api/MeasureWBox/#measureia.MeasureWBox.__init__","text":"The init method of the MeasureWSimulations class. Notes Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. Source code in src/measureia/measure_w_box.py def __init__( self, data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True, ): \"\"\" The __init__ method of the MeasureWSimulations class. Notes ----- Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. \"\"\" super().__init__(data, output_file_name, simulation, snapshot, separation_limits, num_bins_r, num_bins_pi, pi_max, boxsize, periodicity) return","title":"__init__"},{"location":"api/MeasureWBoxJackknife/","text":"MeasureWBoxJackknife Bases: MeasureIABase , ReadData Class that contains all methods for the measurements of \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) for \\(w_{gg}\\) and \\(w_{g+}\\) including the jackknife realisations needed for the covariance estimation with Cartesian simulation data. Methods: Name Description _measure_xi_rp_pi_box_jk_brute Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (rp, pi) grid binning including jackknife realisations in a periodic box using 1 CPU. _measure_xi_rp_pi_box_jk_tree Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (rp, pi) grid binning including jackknife realisations in a periodic box using 1 CPU and KDTree for extra speed. _measure_xi_rp_pi_box_jk_batch Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (rp, pi) grid binning including jackknife realisations in a periodic box using 1 CPU for a batch of indices. Support function of _measure_xi_rp_pi_box_jk_multiprocessing(). _measure_xi_rp_pi_box_jk_multiprocessing Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (rp, pi) grid binning including jackknife realisations in a periodic box using >1 CPUs. Notes Inherits attributes from 'SimInfo', where 'boxsize', 'L_0p5' and 'snap_group' are used in this class. Inherits attributes from 'MeasureIABase', where 'data', 'output_file_name', 'periodicity', 'Num_position', 'Num_shape', 'r_min', 'r_max', 'num_bins_r', 'num_bins_pi', 'r_bins', 'pi_bins', 'mu_r_bins' are used. Source code in src/measureia/measure_w_box_jk.py class MeasureWBoxJackknife(MeasureIABase, ReadData): r\"\"\"Class that contains all methods for the measurements of $\\xi_{gg}$ and $\\xi_{g+}$ for $w_{gg}$ and $w_{g+}$ including the jackknife realisations needed for the covariance estimation with Cartesian simulation data. Methods ------- _measure_xi_rp_pi_box_jk_brute() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (rp, pi) grid binning including jackknife realisations in a periodic box using 1 CPU. _measure_xi_rp_pi_box_jk_tree() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (rp, pi) grid binning including jackknife realisations in a periodic box using 1 CPU and KDTree for extra speed. _measure_xi_rp_pi_box_jk_batch() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (rp, pi) grid binning including jackknife realisations in a periodic box using 1 CPU for a batch of indices. Support function of _measure_xi_rp_pi_box_jk_multiprocessing(). _measure_xi_rp_pi_box_jk_multiprocessing() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (rp, pi) grid binning including jackknife realisations in a periodic box using >1 CPUs. Notes ----- Inherits attributes from 'SimInfo', where 'boxsize', 'L_0p5' and 'snap_group' are used in this class. Inherits attributes from 'MeasureIABase', where 'data', 'output_file_name', 'periodicity', 'Num_position', 'Num_shape', 'r_min', 'r_max', 'num_bins_r', 'num_bins_pi', 'r_bins', 'pi_bins', 'mu_r_bins' are used. \"\"\" def __init__( self, data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True, ): \"\"\" The __init__ method of the MeasureWSimulations class. Notes ----- Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. \"\"\" super().__init__(data, output_file_name, simulation, snapshot, separation_limits, num_bins_r, num_bins_pi, pi_max, boxsize, periodicity) return def _measure_xi_rp_pi_box_jk_brute(self, dataset_name, L_subboxes, masks=None, return_output=False, jk_group_name=\"\", ellipticity='distortion'): r\"\"\"Measures the projected correlation functions including jackknife realisations, $\\xi_{gg}$ and $\\xi_{g+}$, in (rp, pi) bins for an object created with MeasureIABox. Uses 1 CPU. Parameters ---------- dataset_name : str Name of the dataset in the output file. L_subboxes: int Number of subboxes on one side of the box. L_subboxes^3 is the total number of jackknife realisations. masks : dict or NoneType, optional Dictionary with masks for the data to select only part of the data. Uses same keywords as data dictionary. Default value = None. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. jk_group_name : str, optional Group in output file (hdf5) where jackknife realisations are stored. Default value is \"\". ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. Returns ------- ndarrays $\\xi_{gg}$ and $\\xi_{g+}$, r_p bins, pi bins, S+D, DD, RR (if no output file is specified) \"\"\" if masks == None: positions = self.data[\"Position\"] positions_shape_sample = self.data[\"Position_shape_sample\"] axis_direction_v = self.data[\"Axis_Direction\"] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"] weight = self.data[\"weight\"] weight_shape = self.data[\"weight_shape_sample\"] else: positions = self.data[\"Position\"][masks[\"Position\"]] positions_shape_sample = self.data[\"Position_shape_sample\"][masks[\"Position_shape_sample\"]] axis_direction_v = self.data[\"Axis_Direction\"][masks[\"Axis_Direction\"]] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"][masks[\"q\"]] try: weight_mask = masks[\"weight\"] except: masks[\"weight\"] = np.ones(self.Num_position, dtype=bool) masks[\"weight\"][sum(masks[\"Position\"]):self.Num_position] = 0 try: weight_mask = masks[\"weight_shape_sample\"] except: masks[\"weight_shape_sample\"] = np.ones(self.Num_shape, dtype=bool) masks[\"weight_shape_sample\"][sum(masks[\"Position_shape_sample\"]):self.Num_shape] = 0 weight = self.data[\"weight\"][masks[\"weight\"]] weight_shape = self.data[\"weight_shape_sample\"][masks[\"weight_shape_sample\"]] Num_position = len(positions) Num_shape = len(positions_shape_sample) print( f\"There are {Num_shape} galaxies in the shape sample and {Num_position} galaxies in the position sample.\") LOS_ind = self.data[\"LOS\"] # eg 2 for z axis not_LOS = np.array([0, 1, 2])[np.isin([0, 1, 2], LOS_ind, invert=True)] # eg 0,1 for x&y if ellipticity == 'distortion': e = (1 - q ** 2) / (1 + q ** 2) # size of ellipticity elif ellipticity == 'ellipticity': e = (1 - q) / (1 + q) else: raise ValueError(\"Invalid value for ellipticity. Choose 'distortion' or 'ellipticity'.\") del q R = sum(weight_shape * (1 - e ** 2 / 2.0)) / sum(weight_shape) # R = 1 - np.mean(e ** 2) / 2.0 # responsitivity factor L3 = self.boxsize ** 3 # box volume sub_box_len_logrp = (np.log10(self.r_max) - np.log10(self.r_min)) / self.num_bins_r sub_box_len_pi = (self.pi_bins[-1] - self.pi_bins[0]) / self.num_bins_pi DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_g_plus = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_gg = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) variance = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) jackknife_region_indices_pos, jackknife_region_indices_shape = self._get_jackknife_region_indices(masks, L_subboxes) num_box = L_subboxes ** 3 DD_jk = np.zeros((num_box, self.num_bins_r, self.num_bins_pi)) Splus_D_jk = np.zeros((num_box, self.num_bins_r, self.num_bins_pi)) for n in np.arange(0, len(positions)): # for Splus_D (calculate ellipticities around position sample) separation = positions_shape_sample - positions[n] if self.periodicity: separation[separation > self.L_0p5] -= self.boxsize # account for periodicity of box separation[separation < -self.L_0p5] += self.boxsize projected_sep = separation[:, not_LOS] LOS = separation[:, LOS_ind] del separation separation_len = np.sqrt(np.sum(projected_sep ** 2, axis=1)) with np.errstate(invalid='ignore'): separation_dir = (projected_sep.transpose() / separation_len).transpose() # normalisation of rp del projected_sep phi = np.arccos(self.calculate_dot_product_arrays(separation_dir, axis_direction)) # [0,pi] del separation_dir e_plus, e_cross = self.get_ellipticity(e, phi) del phi e_plus[np.isnan(e_plus)] = 0.0 e_cross[np.isnan(e_cross)] = 0.0 # get the indices for the binning mask = (separation_len >= self.r_bins[0]) * (separation_len < self.r_bins[-1]) * ( LOS >= self.pi_bins[0]) * (LOS < self.pi_bins[-1]) ind_r = np.floor( np.log10(separation_len[mask]) / sub_box_len_logrp - np.log10(self.r_bins[0]) / sub_box_len_logrp ) del separation_len ind_r = np.array(ind_r, dtype=int) ind_pi = np.floor( LOS[mask] / sub_box_len_pi - self.pi_bins[0] / sub_box_len_pi ) # need length of LOS, so only positive values del LOS ind_pi = np.array(ind_pi, dtype=int) if np.any(ind_pi == self.num_bins_pi): ind_pi[ind_pi >= self.num_bins_pi] -= 1 if np.any(ind_r == self.num_bins_r): ind_r[ind_r >= self.num_bins_r] -= 1 np.add.at(Splus_D, (ind_r, ind_pi), (weight[n] * weight_shape[mask] * e_plus[mask]) / (2 * R)) np.add.at(Scross_D, (ind_r, ind_pi), (weight[n] * weight_shape[mask] * e_cross[mask]) / (2 * R)) np.add.at(variance, (ind_r, ind_pi), ((weight[n] * weight_shape[mask] * e_plus[mask]) / (2 * R)) ** 2) shape_mask = np.where(jackknife_region_indices_shape[mask] != jackknife_region_indices_pos[n])[0] np.add.at(Splus_D_jk, (jackknife_region_indices_pos[n], ind_r, ind_pi), (weight[n] * weight_shape[mask] * e_plus[mask])) # responsivity added later np.add.at(Splus_D_jk, (jackknife_region_indices_shape[mask][shape_mask], ind_r[shape_mask], ind_pi[shape_mask]), (weight[n] * weight_shape[mask][shape_mask] * e_plus[mask][ shape_mask])) # responsivity added later del e_plus, e_cross np.add.at(DD, (ind_r, ind_pi), weight[n] * weight_shape[mask]) np.add.at(DD_jk, (jackknife_region_indices_pos[n], ind_r, ind_pi), (weight[n] * weight_shape[mask])) np.add.at(DD_jk, (jackknife_region_indices_shape[mask][shape_mask], ind_r[shape_mask], ind_pi[shape_mask]), (weight[n] * weight_shape[mask][shape_mask])) R_jk = np.zeros(num_box) for i in np.arange(num_box): jk_mask = np.where(jackknife_region_indices_shape != i) R_jk[i] = sum(weight_shape[jk_mask] * (1 - e[jk_mask] ** 2 / 2.0)) / sum(weight_shape[jk_mask]) corrtype = \"cross\" for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_g_plus[i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], self.pi_bins[p + 1], self.pi_bins[p], L3, \"cross\", Num_position, Num_shape) RR_gg[i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], self.pi_bins[p + 1], self.pi_bins[p], L3, corrtype, Num_position, Num_shape) RR_jk = np.zeros((num_box, self.num_bins_r, self.num_bins_pi)) volume_jk = L3 * (num_box - 1) / (num_box) for jk in np.arange(num_box): Num_position_jk, Num_shape_jk = len(np.where(jackknife_region_indices_pos != jk)[0]), len( np.where(jackknife_region_indices_shape != jk)[0]) for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_jk[jk, i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], self.pi_bins[p + 1], self.pi_bins[p], volume_jk, \"cross\", Num_position_jk, Num_shape_jk) correlation = Splus_D / RR_g_plus # (Splus_D - Splus_R) / RR_g_plus xi_g_cross = Scross_D / RR_g_plus # (Scross_D - Scross_R) / RR_g_plus sigsq = variance / RR_g_plus ** 2 dsep = (self.r_bins[1:] - self.r_bins[:-1]) / 2.0 separation_bins = self.r_bins[:-1] + abs(dsep) # middle of bins dpi = (self.pi_bins[1:] - self.pi_bins[:-1]) / 2.0 pi_bins = self.pi_bins[:-1] + abs(dpi) # middle of bins if (self.output_file_name != None) and (return_output == False): output_file = h5py.File(self.output_file_name, \"a\") group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_g_plus/\") write_dataset_hdf5(group, dataset_name, data=correlation) write_dataset_hdf5(group, dataset_name + \"_SplusD\", data=Splus_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_plus\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_sigmasq\", data=sigsq) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_g_plus/{jk_group_name}\") for i in np.arange(0, num_box): corr = (Splus_D * (2 * R) - Splus_D_jk[i]) / ( RR_jk[i] * 2 * R_jk[i]) # Responsivity will be different for each realisation write_dataset_hdf5(group, dataset_name + f\"_{i}\", data=corr) write_dataset_hdf5(group, dataset_name + f\"_{i}_SplusD\", data=(Splus_D * (2 * R) - Splus_D_jk[i]) / ( 2 * R_jk[i])) # Splus_D_jk[i]/(2*R_jk[i])) write_dataset_hdf5(group, dataset_name + f\"_{i}_RR\", data=RR_jk[i]) write_dataset_hdf5(group, dataset_name + f\"_{i}_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + f\"_{i}_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_g_cross/{jk_group_name}\") write_dataset_hdf5(group, dataset_name + \"_ScrossD\", data=Scross_D) write_dataset_hdf5(group, dataset_name, data=xi_g_cross) write_dataset_hdf5(group, dataset_name + \"_RR_g_cross\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_sigmasq\", data=sigsq) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_gg/\") write_dataset_hdf5(group, dataset_name, data=(DD / RR_gg) - 1) write_dataset_hdf5(group, dataset_name + \"_DD\", data=DD) write_dataset_hdf5(group, dataset_name + \"_RR_gg\", data=RR_gg) write_dataset_hdf5(group, dataset_name + \"_sigmasq\", data=sigsq) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_gg/{jk_group_name}\") for i in np.arange(0, num_box): write_dataset_hdf5(group, dataset_name + f\"_{i}\", data=((DD - DD_jk[i]) / RR_jk[i]) - 1) write_dataset_hdf5(group, dataset_name + f\"_{i}_DD\", data=(DD - DD_jk[i])) write_dataset_hdf5(group, dataset_name + f\"_{i}_RR\", data=RR_jk[i]) write_dataset_hdf5(group, dataset_name + f\"_{i}_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + f\"_{i}_pi\", data=pi_bins) output_file.close() return else: return correlation, (DD / RR_gg) - 1, separation_bins, pi_bins, Splus_D, DD, RR_g_plus def _measure_xi_rp_pi_box_jk_tree(self, dataset_name, L_subboxes, masks=None, return_output=False, jk_group_name=\"\", ellipticity='distortion'): r\"\"\"Measures the projected correlation functions including jackknife realisations, $\\xi_{gg}$ and $\\xi_{g+}$, in (rp, pi) bins for an object created with MeasureIABox. Uses 1 CPU. Uses KDTree for speedup. Parameters ---------- dataset_name : str Name of the dataset in the output file. L_subboxes: int Number of subboxes on one side of the box. L_subboxes^3 is the total number of jackknife realisations. masks : dict or NoneType, optional Dictionary with masks for the data to select only part of the data. Uses same keywords as data dictionary. Default value = None. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. jk_group_name : str, optional Group in output file (hdf5) where jackknife realisations are stored. Default value is \"\". ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. Returns ------- ndarrays $\\xi_{gg}$ and $\\xi_{g+}$, r_p bins, pi bins, S+D, DD, RR (if no output file is specified) \"\"\" if masks == None: positions = self.data[\"Position\"] positions_shape_sample = self.data[\"Position_shape_sample\"] axis_direction_v = self.data[\"Axis_Direction\"] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"] weight = self.data[\"weight\"] weight_shape = self.data[\"weight_shape_sample\"] else: positions = self.data[\"Position\"][masks[\"Position\"]] positions_shape_sample = self.data[\"Position_shape_sample\"][masks[\"Position_shape_sample\"]] axis_direction_v = self.data[\"Axis_Direction\"][masks[\"Axis_Direction\"]] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"][masks[\"q\"]] try: weight_mask = masks[\"weight\"] except: masks[\"weight\"] = np.ones(self.Num_position, dtype=bool) masks[\"weight\"][sum(masks[\"Position\"]):self.Num_position] = 0 try: weight_mask = masks[\"weight_shape_sample\"] except: masks[\"weight_shape_sample\"] = np.ones(self.Num_shape, dtype=bool) masks[\"weight_shape_sample\"][sum(masks[\"Position_shape_sample\"]):self.Num_shape] = 0 weight = self.data[\"weight\"][masks[\"weight\"]] weight_shape = self.data[\"weight_shape_sample\"][masks[\"weight_shape_sample\"]] Num_position = len(positions) Num_shape = len(positions_shape_sample) print( f\"There are {Num_shape} galaxies in the shape sample and {Num_position} galaxies in the position sample.\") LOS_ind = self.data[\"LOS\"] # eg 2 for z axis not_LOS = np.array([0, 1, 2])[np.isin([0, 1, 2], LOS_ind, invert=True)] # eg 0,1 for x&y if ellipticity == 'distortion': e = (1 - q ** 2) / (1 + q ** 2) # size of ellipticity elif ellipticity == 'ellipticity': e = (1 - q) / (1 + q) else: raise ValueError(\"Invalid value for ellipticity. Choose 'distortion' or 'ellipticity'.\") del q R = sum(weight_shape * (1 - e ** 2 / 2.0)) / sum(weight_shape) # R = 1 - np.mean(e ** 2) / 2.0 # responsitivity factor L3 = self.boxsize ** 3 # box volume sub_box_len_logrp = (np.log10(self.r_max) - np.log10(self.r_min)) / self.num_bins_r sub_box_len_pi = (self.pi_bins[-1] - self.pi_bins[0]) / self.num_bins_pi DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_g_plus = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_gg = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) variance = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) jackknife_region_indices_pos, jackknife_region_indices_shape = self._get_jackknife_region_indices(masks, L_subboxes) num_box = L_subboxes ** 3 DD_jk = np.zeros((num_box, self.num_bins_r, self.num_bins_pi)) Splus_D_jk = np.zeros((num_box, self.num_bins_r, self.num_bins_pi)) figname_dataset_name = dataset_name if \"/\" in dataset_name: figname_dataset_name = figname_dataset_name.replace(\"/\", \"_\") if \".\" in dataset_name: figname_dataset_name = figname_dataset_name.replace(\".\", \"p\") pos_tree = KDTree(positions[:, not_LOS], boxsize=self.boxsize) for i in np.arange(0, len(positions_shape_sample), 100): i2 = min(len(positions_shape_sample), i + 100) positions_shape_sample_i = positions_shape_sample[i:i2] axis_direction_i = axis_direction[i:i2] e_i = e[i:i2] weight_shape_i = weight_shape[i:i2] jackknife_region_indices_shape_i = jackknife_region_indices_shape[i:i2] shape_tree = KDTree(positions_shape_sample_i[:, not_LOS], boxsize=self.boxsize) ind_min_i = shape_tree.query_ball_tree(pos_tree, self.r_min) ind_max_i = shape_tree.query_ball_tree(pos_tree, self.r_max) ind_rbin_i = self.setdiff2D(ind_max_i, ind_min_i) for n in np.arange(0, len(positions_shape_sample_i)): # CHANGE2: loop now over shapes, not positions if len(ind_rbin_i[n]) > 0: # for Splus_D (calculate ellipticities around position sample) separation = positions_shape_sample_i[n] - positions[ind_rbin_i[n]] # CHANGE1 & CHANGE2 if self.periodicity: separation[separation > self.L_0p5] -= self.boxsize # account for periodicity of box separation[separation < -self.L_0p5] += self.boxsize projected_sep = separation[:, not_LOS] LOS = separation[:, LOS_ind] separation_len = np.sqrt(np.sum(projected_sep ** 2, axis=1)) with np.errstate(invalid='ignore'): separation_dir = (projected_sep.transpose() / separation_len).transpose() # normalisation of rp del projected_sep, separation phi = np.arccos( separation_dir[:, 0] * axis_direction_i[n, 0] + separation_dir[:, 1] * axis_direction_i[ n, 1]) # CHANGE2 e_plus, e_cross = self.get_ellipticity(e_i[n], phi) # CHANGE2 del phi, separation_dir e_plus[np.isnan(e_plus)] = 0.0 e_cross[np.isnan(e_cross)] = 0.0 # get the indices for the binning mask = (separation_len >= self.r_bins[0]) * (separation_len < self.r_bins[-1]) * ( LOS >= self.pi_bins[0]) * (LOS < self.pi_bins[-1]) ind_r = np.floor( np.log10(separation_len[mask]) / sub_box_len_logrp - np.log10( self.r_bins[0]) / sub_box_len_logrp ) ind_r = np.array(ind_r, dtype=int) ind_pi = np.floor( LOS[mask] / sub_box_len_pi - self.pi_bins[0] / sub_box_len_pi ) # need length of LOS, so only positive values ind_pi = np.array(ind_pi, dtype=int) if np.any(ind_pi == self.num_bins_pi): ind_pi[ind_pi >= self.num_bins_pi] -= 1 if np.any(ind_r == self.num_bins_r): ind_r[ind_r >= self.num_bins_r] -= 1 np.add.at(Splus_D, (ind_r, ind_pi), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_plus[mask]) / (2 * R)) np.add.at(Scross_D, (ind_r, ind_pi), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_cross[mask]) / (2 * R)) del separation_len np.add.at(DD, (ind_r, ind_pi), weight[ind_rbin_i[n]][mask] * weight_shape_i[n]) pos_mask = \\ np.where( jackknife_region_indices_pos[ind_rbin_i[n]][mask] != jackknife_region_indices_shape_i[n])[ 0] np.add.at(Splus_D_jk, (jackknife_region_indices_shape_i[n], ind_r, ind_pi), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_plus[ mask])) # responsivity added later np.add.at(Splus_D_jk, (jackknife_region_indices_pos[ind_rbin_i[n]][mask][pos_mask], ind_r[pos_mask], ind_pi[pos_mask]), (weight[ind_rbin_i[n]][mask][pos_mask] * weight_shape_i[n] * e_plus[mask][ pos_mask])) # responsivity added later del e_plus, e_cross np.add.at(DD_jk, (jackknife_region_indices_shape_i[n], ind_r, ind_pi), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n])) # responsivity added later np.add.at(DD_jk, (jackknife_region_indices_pos[ind_rbin_i[n]][mask][pos_mask], ind_r[pos_mask], ind_pi[pos_mask]), (weight[ind_rbin_i[n]][mask][pos_mask] * weight_shape_i[n])) # responsivity added later R_jk = np.zeros(num_box) for i in np.arange(num_box): jk_mask = np.where(jackknife_region_indices_shape != i) R_jk[i] = sum(weight_shape[jk_mask] * (1 - e[jk_mask] ** 2 / 2.0)) / sum(weight_shape[jk_mask]) corrtype = \"cross\" for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_g_plus[i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], self.pi_bins[p + 1], self.pi_bins[p], L3, \"cross\", Num_position, Num_shape) RR_gg[i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], self.pi_bins[p + 1], self.pi_bins[p], L3, corrtype, Num_position, Num_shape) RR_jk = np.zeros((num_box, self.num_bins_r, self.num_bins_pi)) volume_jk = L3 * (num_box - 1) / num_box for jk in np.arange(num_box): Num_position_jk, Num_shape_jk = len(np.where(jackknife_region_indices_pos != jk)[0]), len( np.where(jackknife_region_indices_shape != jk)[0]) for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_jk[jk, i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], self.pi_bins[p + 1], self.pi_bins[p], volume_jk, \"cross\", Num_position_jk, Num_shape_jk) correlation = Splus_D / RR_g_plus # (Splus_D - Splus_R) / RR_g_plus xi_g_cross = Scross_D / RR_g_plus # (Scross_D - Scross_R) / RR_g_plus sigsq = variance / RR_g_plus ** 2 dsep = (self.r_bins[1:] - self.r_bins[:-1]) / 2.0 separation_bins = self.r_bins[:-1] + abs(dsep) # middle of bins dpi = (self.pi_bins[1:] - self.pi_bins[:-1]) / 2.0 pi_bins = self.pi_bins[:-1] + abs(dpi) # middle of bins if (self.output_file_name != None) and (return_output == False): output_file = h5py.File(self.output_file_name, \"a\") group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_g_plus/\") write_dataset_hdf5(group, dataset_name, data=correlation) write_dataset_hdf5(group, dataset_name + \"_SplusD\", data=Splus_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_plus\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_sigmasq\", data=sigsq) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_g_plus/{jk_group_name}\") for i in np.arange(0, num_box): corr = (Splus_D * (2 * R) - Splus_D_jk[i]) / ( RR_jk[i] * 2 * R_jk[i]) # Responsivity will be different for each realisation write_dataset_hdf5(group, dataset_name + f\"_{i}\", data=corr) write_dataset_hdf5(group, dataset_name + f\"_{i}_SplusD\", data=(Splus_D * (2 * R) - Splus_D_jk[i]) / ( 2 * R_jk[i])) # Splus_D_jk[i]/(2*R_jk[i])) write_dataset_hdf5(group, dataset_name + f\"_{i}_RR\", data=RR_jk[i]) write_dataset_hdf5(group, dataset_name + f\"_{i}_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + f\"_{i}_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_g_cross/{jk_group_name}\") write_dataset_hdf5(group, dataset_name + \"_ScrossD\", data=Scross_D) write_dataset_hdf5(group, dataset_name, data=xi_g_cross) write_dataset_hdf5(group, dataset_name + \"_RR_g_cross\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_sigmasq\", data=sigsq) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_gg/\") write_dataset_hdf5(group, dataset_name, data=(DD / RR_gg) - 1) write_dataset_hdf5(group, dataset_name + \"_DD\", data=DD) write_dataset_hdf5(group, dataset_name + \"_RR_gg\", data=RR_gg) write_dataset_hdf5(group, dataset_name + \"_sigmasq\", data=sigsq) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_gg/{jk_group_name}\") for i in np.arange(0, num_box): write_dataset_hdf5(group, dataset_name + f\"_{i}\", data=((DD - DD_jk[i]) / RR_jk[i]) - 1) write_dataset_hdf5(group, dataset_name + f\"_{i}_DD\", data=(DD - DD_jk[i])) write_dataset_hdf5(group, dataset_name + f\"_{i}_RR\", data=RR_jk[i]) write_dataset_hdf5(group, dataset_name + f\"_{i}_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + f\"_{i}_pi\", data=pi_bins) output_file.close() return else: return correlation, (DD / RR_gg) - 1, separation_bins, pi_bins, Splus_D, DD, RR_g_plus def _measure_xi_rp_pi_box_jk_batch(self, i): r\"\"\"Measures components of $\\xi_{gg}$ and $\\xi_{g+}$ in (rp,pi) bins including jackknife realisations for a batch of indices from i to i+chunk_size. Support function for _measure_xi_rp_pi_box_jk_multiprocessing(). Parameters ---------- i: int Start index of the batch. Returns ------- ndarrays S+D, SxD, DD, DD_jk, S+D_jk where the _jk versions store the necessary information of DD of S+D for each jackknife realisation. \"\"\" if i + self.chunk_size > self.Num_shape_masked: i2 = self.Num_shape_masked else: i2 = i + self.chunk_size DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) DD_jk = np.zeros((self.num_box, self.num_bins_r, self.num_bins_pi)) Splus_D_jk = np.zeros((self.num_box, self.num_bins_r, self.num_bins_pi)) positions_shape_sample_i = self.temp_data_obj.read_cat(\"positions_shape_sample\", [i, i2]) axis_direction_i = self.temp_data_obj.read_cat(\"axis_direction\", [i, i2]) weight_shape_i = self.temp_data_obj.read_cat(\"weight_shape\", [i, i2]) positions = self.temp_data_obj.read_cat(\"positions\") weight = self.temp_data_obj.read_cat(\"weight\") e_i = self.e[i:i2] jackknife_region_indices_shape_i = self.jackknife_region_indices_shape[i:i2] shape_tree = KDTree(positions_shape_sample_i[:, self.not_LOS], boxsize=self.boxsize) ind_min_i = shape_tree.query_ball_tree(self.pos_tree, self.r_min) ind_max_i = shape_tree.query_ball_tree(self.pos_tree, self.r_max) ind_rbin_i = self.setdiff2D(ind_max_i, ind_min_i) for n in np.arange(0, len(positions_shape_sample_i)): # CHANGE2: loop now over shapes, not positions if len(ind_rbin_i[n]) > 0: # for Splus_D (calculate ellipticities around position sample) separation = positions_shape_sample_i[n] - positions[ind_rbin_i[n]] # CHANGE1 & CHANGE2 if self.periodicity: separation[separation > self.L_0p5] -= self.boxsize # account for periodicity of box separation[separation < -self.L_0p5] += self.boxsize projected_sep = separation[:, self.not_LOS] LOS = separation[:, self.LOS_ind] separation_len = np.sqrt(np.sum(projected_sep ** 2, axis=1)) with np.errstate(invalid='ignore'): separation_dir = (projected_sep.transpose() / separation_len).transpose() # normalisation of rp del projected_sep, separation phi = np.arccos( separation_dir[:, 0] * axis_direction_i[n, 0] + separation_dir[:, 1] * axis_direction_i[ n, 1]) # CHANGE2 e_plus, e_cross = self.get_ellipticity(e_i[n], phi) # CHANGE2 del phi, separation_dir e_plus[np.isnan(e_plus)] = 0.0 e_cross[np.isnan(e_cross)] = 0.0 # get the indices for the binning mask = (separation_len >= self.r_bins[0]) * (separation_len < self.r_bins[-1]) * ( LOS >= self.pi_bins[0]) * (LOS < self.pi_bins[-1]) ind_r = np.floor( np.log10(separation_len[mask]) / self.sub_box_len_logrp - np.log10( self.r_bins[0]) / self.sub_box_len_logrp ) ind_r = np.array(ind_r, dtype=int) ind_pi = np.floor( LOS[mask] / self.sub_box_len_pi - self.pi_bins[0] / self.sub_box_len_pi ) # need length of LOS, so only positive values ind_pi = np.array(ind_pi, dtype=int) if np.any(ind_pi == self.num_bins_pi): ind_pi[ind_pi >= self.num_bins_pi] -= 1 if np.any(ind_r == self.num_bins_r): ind_r[ind_r >= self.num_bins_r] -= 1 np.add.at(Splus_D, (ind_r, ind_pi), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_plus[mask]) / (2 * self.R)) np.add.at(Scross_D, (ind_r, ind_pi), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_cross[mask]) / (2 * self.R)) del separation_len np.add.at(DD, (ind_r, ind_pi), weight[ind_rbin_i[n]][mask] * weight_shape_i[n]) pos_mask = \\ np.where( self.jackknife_region_indices_pos[ind_rbin_i[n]][mask] != jackknife_region_indices_shape_i[n])[ 0] np.add.at(Splus_D_jk, (jackknife_region_indices_shape_i[n], ind_r, ind_pi), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_plus[ mask])) # responsivity added later np.add.at(Splus_D_jk, (self.jackknife_region_indices_pos[ind_rbin_i[n]][mask][pos_mask], ind_r[pos_mask], ind_pi[pos_mask]), (weight[ind_rbin_i[n]][mask][pos_mask] * weight_shape_i[n] * e_plus[mask][ pos_mask])) # responsivity added later del e_plus, e_cross np.add.at(DD_jk, (jackknife_region_indices_shape_i[n], ind_r, ind_pi), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n])) # responsivity added later np.add.at(DD_jk, (self.jackknife_region_indices_pos[ind_rbin_i[n]][mask][pos_mask], ind_r[pos_mask], ind_pi[pos_mask]), (weight[ind_rbin_i[n]][mask][pos_mask] * weight_shape_i[n])) # responsivity added later return Splus_D, Scross_D, DD, DD_jk, Splus_D_jk def _measure_xi_rp_pi_box_jk_multiprocessing(self, dataset_name, L_subboxes, temp_file_path, masks=None, return_output=False, jk_group_name=\"\", chunk_size=1000, num_nodes=1, ellipticity='distortion' ): r\"\"\"Measures the projected correlation functions including jackknife realisations, $\\xi_{gg}$ and $\\xi_{g+}$, in (rp, pi) bins for an object created with MeasureIABox. Uses >1 CPU. Uses KDTree for speedup. Parameters ---------- dataset_name : str Name of the dataset in the output file. L_subboxes: int Number of subboxes on one side of the box. L_subboxes^3 is the total number of jackknife realisations. temp_file_path : str or NoneType, optional Path to where the data is temporarily stored [file name generated automatically]. masks : dict or NoneType, optional Dictionary with masks for the data to select only part of the data. Uses same keywords as data dictionary. Default value = None. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. jk_group_name : str, optional Group in output file (hdf5) where jackknife realisations are stored. Default value is \"\". chunk_size: int, optional Size of the chunks of data sent to each multiprocessing node. If larger, more RAM is needed per node. Default is 1000. num_nodes : int, optional Number of CPUs used in the multiprocessing. Default is 1. ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. Returns ------- ndarrays $\\xi_{gg}$ and $\\xi_{g+}$, r_p bins, pi bins, S+D, DD, RR (if no output file is specified) \"\"\" if masks == None: positions = self.data[\"Position\"] positions_shape_sample = self.data[\"Position_shape_sample\"] axis_direction_v = self.data[\"Axis_Direction\"] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"] weight = self.data[\"weight\"] weight_shape = self.data[\"weight_shape_sample\"] else: positions = self.data[\"Position\"][masks[\"Position\"]] positions_shape_sample = self.data[\"Position_shape_sample\"][masks[\"Position_shape_sample\"]] axis_direction_v = self.data[\"Axis_Direction\"][masks[\"Axis_Direction\"]] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"][masks[\"q\"]] try: weight_mask = masks[\"weight\"] except: masks[\"weight\"] = np.ones(self.Num_position, dtype=bool) masks[\"weight\"][sum(masks[\"Position\"]):self.Num_position] = 0 try: weight_mask = masks[\"weight_shape_sample\"] except: masks[\"weight_shape_sample\"] = np.ones(self.Num_shape, dtype=bool) masks[\"weight_shape_sample\"][sum(masks[\"Position_shape_sample\"]):self.Num_shape] = 0 weight = self.data[\"weight\"][masks[\"weight\"]] weight_shape = self.data[\"weight_shape_sample\"][masks[\"weight_shape_sample\"]] self.Num_position_masked = len(positions) self.Num_shape_masked = len(positions_shape_sample) # create temp hdf5 from which data can be read. del self.data, but save it in this method to reduce RAM figname_dataset_name = dataset_name if \"/\" in dataset_name: figname_dataset_name = figname_dataset_name.replace(\"/\", \"_\") if \".\" in dataset_name: figname_dataset_name = figname_dataset_name.replace(\".\", \"p\") file_temp = h5py.File(f\"{temp_file_path}/w_{self.simname}_temp_data_{figname_dataset_name}.hdf5\", \"w\") write_dataset_hdf5(file_temp, \"positions\", positions) write_dataset_hdf5(file_temp, \"weight\", weight) write_dataset_hdf5(file_temp, \"weight_shape\", weight_shape) write_dataset_hdf5(file_temp, \"positions_shape_sample\", positions_shape_sample) write_dataset_hdf5(file_temp, \"axis_direction\", axis_direction) file_temp.close() self.temp_data_obj = ReadData(self.simname, f\"w_{self.simname}_temp_data_{figname_dataset_name}\", None, data_path=temp_file_path) print( f\"There are {self.Num_shape_masked} galaxies in the shape sample and {self.Num_position_masked} galaxies in the position sample.\") self.LOS_ind = self.data[\"LOS\"] # eg 2 for z axis self.not_LOS = np.array([0, 1, 2])[np.isin([0, 1, 2], self.LOS_ind, invert=True)] # eg 0,1 for x&y if ellipticity == 'distortion': self.e = (1 - q ** 2) / (1 + q ** 2) # size of ellipticity elif ellipticity == 'ellipticity': self.e = (1 - q) / (1 + q) else: raise ValueError(\"Invalid value for ellipticity. Choose 'distortion' or 'ellipticity'.\") del q self.R = sum(weight_shape * (1 - self.e ** 2 / 2.0)) / sum(weight_shape) # R = 1 - np.mean(e ** 2) / 2.0 # responsitivity factor L3 = self.boxsize ** 3 # box volume self.sub_box_len_logrp = (np.log10(self.r_max) - np.log10(self.r_min)) / self.num_bins_r self.sub_box_len_pi = (self.pi_bins[-1] - self.pi_bins[0]) / self.num_bins_pi DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_g_plus = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_gg = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) self.jackknife_region_indices_pos, self.jackknife_region_indices_shape = self._get_jackknife_region_indices( masks, L_subboxes) self.num_box = L_subboxes ** 3 DD_jk = np.zeros((self.num_box, self.num_bins_r, self.num_bins_pi)) Splus_D_jk = np.zeros((self.num_box, self.num_bins_r, self.num_bins_pi)) data_temp = self.data # make sure data is not sent to every CPU self.data = None self.pos_tree = KDTree(positions[:, self.not_LOS], boxsize=self.boxsize) indices = np.arange(0, len(positions_shape_sample), chunk_size) self.chunk_size = chunk_size with Pool(num_nodes) as p: result = p.map(self._measure_xi_rp_pi_box_jk_batch, indices) os.remove( f\"{temp_file_path}/w_{self.simname}_temp_data_{figname_dataset_name}.hdf5\") self.data = data_temp del data_temp for i in np.arange(len(result)): Splus_D += result[i][0] Scross_D += result[i][1] DD += result[i][2] DD_jk += result[i][3] Splus_D_jk += result[i][4] R_jk = np.zeros(self.num_box) for i in np.arange(self.num_box): jk_mask = np.where(self.jackknife_region_indices_shape != i) R_jk[i] = sum(weight_shape[jk_mask] * (1 - self.e[jk_mask] ** 2 / 2.0)) / sum(weight_shape[jk_mask]) corrtype = \"cross\" for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_g_plus[i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], self.pi_bins[p + 1], self.pi_bins[p], L3, \"cross\", self.Num_position_masked, self.Num_shape_masked) RR_gg[i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], self.pi_bins[p + 1], self.pi_bins[p], L3, corrtype, self.Num_position_masked, self.Num_shape_masked) RR_jk = np.zeros((self.num_box, self.num_bins_r, self.num_bins_pi)) volume_jk = L3 * (self.num_box - 1) / self.num_box for jk in np.arange(self.num_box): Num_position_jk, Num_shape_jk = len(np.where(self.jackknife_region_indices_pos != jk)[0]), len( np.where(self.jackknife_region_indices_shape != jk)[0]) for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_jk[jk, i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], self.pi_bins[p + 1], self.pi_bins[p], volume_jk, \"cross\", Num_position_jk, Num_shape_jk) correlation = Splus_D / RR_g_plus # (Splus_D - Splus_R) / RR_g_plus xi_g_cross = Scross_D / RR_g_plus # (Scross_D - Scross_R) / RR_g_plus dsep = (self.r_bins[1:] - self.r_bins[:-1]) / 2.0 separation_bins = self.r_bins[:-1] + abs(dsep) # middle of bins dpi = (self.pi_bins[1:] - self.pi_bins[:-1]) / 2.0 pi_bins = self.pi_bins[:-1] + abs(dpi) # middle of bins if (self.output_file_name != None) and (return_output == False): output_file = h5py.File(self.output_file_name, \"a\") group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_g_plus/\") write_dataset_hdf5(group, dataset_name, data=correlation) write_dataset_hdf5(group, dataset_name + \"_SplusD\", data=Splus_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_plus\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_g_plus/{jk_group_name}\") for i in np.arange(0, self.num_box): corr = (Splus_D * (2 * self.R) - Splus_D_jk[i]) / ( RR_jk[i] * 2 * R_jk[i]) # Responsivity will be different for each realisation write_dataset_hdf5(group, dataset_name + f\"_{i}\", data=corr) write_dataset_hdf5(group, dataset_name + f\"_{i}_SplusD\", data=(Splus_D * (2 * self.R) - Splus_D_jk[i]) / ( 2 * R_jk[i])) # Splus_D_jk[i]/(2*R_jk[i])) write_dataset_hdf5(group, dataset_name + f\"_{i}_RR\", data=RR_jk[i]) write_dataset_hdf5(group, dataset_name + f\"_{i}_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + f\"_{i}_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_g_cross/{jk_group_name}\") write_dataset_hdf5(group, dataset_name + \"_ScrossD\", data=Scross_D) write_dataset_hdf5(group, dataset_name, data=xi_g_cross) write_dataset_hdf5(group, dataset_name + \"_RR_g_cross\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_gg/\") write_dataset_hdf5(group, dataset_name, data=(DD / RR_gg) - 1) write_dataset_hdf5(group, dataset_name + \"_DD\", data=DD) write_dataset_hdf5(group, dataset_name + \"_RR_gg\", data=RR_gg) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_gg/{jk_group_name}\") for i in np.arange(0, self.num_box): write_dataset_hdf5(group, dataset_name + f\"_{i}\", data=((DD - DD_jk[i]) / RR_jk[i]) - 1) write_dataset_hdf5(group, dataset_name + f\"_{i}_DD\", data=(DD - DD_jk[i])) write_dataset_hdf5(group, dataset_name + f\"_{i}_RR\", data=RR_jk[i]) write_dataset_hdf5(group, dataset_name + f\"_{i}_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + f\"_{i}_pi\", data=pi_bins) output_file.close() return else: return correlation, (DD / RR_gg) - 1, separation_bins, pi_bins, Splus_D, DD, RR_g_plus __init__(data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True) The init method of the MeasureWSimulations class. Notes Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. Source code in src/measureia/measure_w_box_jk.py def __init__( self, data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True, ): \"\"\" The __init__ method of the MeasureWSimulations class. Notes ----- Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. \"\"\" super().__init__(data, output_file_name, simulation, snapshot, separation_limits, num_bins_r, num_bins_pi, pi_max, boxsize, periodicity) return handler: python options: show_source: true members_order: source show_root_heading: true heading_level: 2","title":"MeasureWBoxJackknife"},{"location":"api/MeasureWBoxJackknife/#measurewboxjackknife","text":"Bases: MeasureIABase , ReadData Class that contains all methods for the measurements of \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) for \\(w_{gg}\\) and \\(w_{g+}\\) including the jackknife realisations needed for the covariance estimation with Cartesian simulation data. Methods: Name Description _measure_xi_rp_pi_box_jk_brute Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (rp, pi) grid binning including jackknife realisations in a periodic box using 1 CPU. _measure_xi_rp_pi_box_jk_tree Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (rp, pi) grid binning including jackknife realisations in a periodic box using 1 CPU and KDTree for extra speed. _measure_xi_rp_pi_box_jk_batch Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (rp, pi) grid binning including jackknife realisations in a periodic box using 1 CPU for a batch of indices. Support function of _measure_xi_rp_pi_box_jk_multiprocessing(). _measure_xi_rp_pi_box_jk_multiprocessing Measure \\(\\xi_{gg}\\) and \\(\\xi_{g+}\\) in (rp, pi) grid binning including jackknife realisations in a periodic box using >1 CPUs. Notes Inherits attributes from 'SimInfo', where 'boxsize', 'L_0p5' and 'snap_group' are used in this class. Inherits attributes from 'MeasureIABase', where 'data', 'output_file_name', 'periodicity', 'Num_position', 'Num_shape', 'r_min', 'r_max', 'num_bins_r', 'num_bins_pi', 'r_bins', 'pi_bins', 'mu_r_bins' are used. Source code in src/measureia/measure_w_box_jk.py class MeasureWBoxJackknife(MeasureIABase, ReadData): r\"\"\"Class that contains all methods for the measurements of $\\xi_{gg}$ and $\\xi_{g+}$ for $w_{gg}$ and $w_{g+}$ including the jackknife realisations needed for the covariance estimation with Cartesian simulation data. Methods ------- _measure_xi_rp_pi_box_jk_brute() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (rp, pi) grid binning including jackknife realisations in a periodic box using 1 CPU. _measure_xi_rp_pi_box_jk_tree() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (rp, pi) grid binning including jackknife realisations in a periodic box using 1 CPU and KDTree for extra speed. _measure_xi_rp_pi_box_jk_batch() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (rp, pi) grid binning including jackknife realisations in a periodic box using 1 CPU for a batch of indices. Support function of _measure_xi_rp_pi_box_jk_multiprocessing(). _measure_xi_rp_pi_box_jk_multiprocessing() Measure $\\xi_{gg}$ and $\\xi_{g+}$ in (rp, pi) grid binning including jackknife realisations in a periodic box using >1 CPUs. Notes ----- Inherits attributes from 'SimInfo', where 'boxsize', 'L_0p5' and 'snap_group' are used in this class. Inherits attributes from 'MeasureIABase', where 'data', 'output_file_name', 'periodicity', 'Num_position', 'Num_shape', 'r_min', 'r_max', 'num_bins_r', 'num_bins_pi', 'r_bins', 'pi_bins', 'mu_r_bins' are used. \"\"\" def __init__( self, data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True, ): \"\"\" The __init__ method of the MeasureWSimulations class. Notes ----- Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. \"\"\" super().__init__(data, output_file_name, simulation, snapshot, separation_limits, num_bins_r, num_bins_pi, pi_max, boxsize, periodicity) return def _measure_xi_rp_pi_box_jk_brute(self, dataset_name, L_subboxes, masks=None, return_output=False, jk_group_name=\"\", ellipticity='distortion'): r\"\"\"Measures the projected correlation functions including jackknife realisations, $\\xi_{gg}$ and $\\xi_{g+}$, in (rp, pi) bins for an object created with MeasureIABox. Uses 1 CPU. Parameters ---------- dataset_name : str Name of the dataset in the output file. L_subboxes: int Number of subboxes on one side of the box. L_subboxes^3 is the total number of jackknife realisations. masks : dict or NoneType, optional Dictionary with masks for the data to select only part of the data. Uses same keywords as data dictionary. Default value = None. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. jk_group_name : str, optional Group in output file (hdf5) where jackknife realisations are stored. Default value is \"\". ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. Returns ------- ndarrays $\\xi_{gg}$ and $\\xi_{g+}$, r_p bins, pi bins, S+D, DD, RR (if no output file is specified) \"\"\" if masks == None: positions = self.data[\"Position\"] positions_shape_sample = self.data[\"Position_shape_sample\"] axis_direction_v = self.data[\"Axis_Direction\"] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"] weight = self.data[\"weight\"] weight_shape = self.data[\"weight_shape_sample\"] else: positions = self.data[\"Position\"][masks[\"Position\"]] positions_shape_sample = self.data[\"Position_shape_sample\"][masks[\"Position_shape_sample\"]] axis_direction_v = self.data[\"Axis_Direction\"][masks[\"Axis_Direction\"]] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"][masks[\"q\"]] try: weight_mask = masks[\"weight\"] except: masks[\"weight\"] = np.ones(self.Num_position, dtype=bool) masks[\"weight\"][sum(masks[\"Position\"]):self.Num_position] = 0 try: weight_mask = masks[\"weight_shape_sample\"] except: masks[\"weight_shape_sample\"] = np.ones(self.Num_shape, dtype=bool) masks[\"weight_shape_sample\"][sum(masks[\"Position_shape_sample\"]):self.Num_shape] = 0 weight = self.data[\"weight\"][masks[\"weight\"]] weight_shape = self.data[\"weight_shape_sample\"][masks[\"weight_shape_sample\"]] Num_position = len(positions) Num_shape = len(positions_shape_sample) print( f\"There are {Num_shape} galaxies in the shape sample and {Num_position} galaxies in the position sample.\") LOS_ind = self.data[\"LOS\"] # eg 2 for z axis not_LOS = np.array([0, 1, 2])[np.isin([0, 1, 2], LOS_ind, invert=True)] # eg 0,1 for x&y if ellipticity == 'distortion': e = (1 - q ** 2) / (1 + q ** 2) # size of ellipticity elif ellipticity == 'ellipticity': e = (1 - q) / (1 + q) else: raise ValueError(\"Invalid value for ellipticity. Choose 'distortion' or 'ellipticity'.\") del q R = sum(weight_shape * (1 - e ** 2 / 2.0)) / sum(weight_shape) # R = 1 - np.mean(e ** 2) / 2.0 # responsitivity factor L3 = self.boxsize ** 3 # box volume sub_box_len_logrp = (np.log10(self.r_max) - np.log10(self.r_min)) / self.num_bins_r sub_box_len_pi = (self.pi_bins[-1] - self.pi_bins[0]) / self.num_bins_pi DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_g_plus = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_gg = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) variance = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) jackknife_region_indices_pos, jackknife_region_indices_shape = self._get_jackknife_region_indices(masks, L_subboxes) num_box = L_subboxes ** 3 DD_jk = np.zeros((num_box, self.num_bins_r, self.num_bins_pi)) Splus_D_jk = np.zeros((num_box, self.num_bins_r, self.num_bins_pi)) for n in np.arange(0, len(positions)): # for Splus_D (calculate ellipticities around position sample) separation = positions_shape_sample - positions[n] if self.periodicity: separation[separation > self.L_0p5] -= self.boxsize # account for periodicity of box separation[separation < -self.L_0p5] += self.boxsize projected_sep = separation[:, not_LOS] LOS = separation[:, LOS_ind] del separation separation_len = np.sqrt(np.sum(projected_sep ** 2, axis=1)) with np.errstate(invalid='ignore'): separation_dir = (projected_sep.transpose() / separation_len).transpose() # normalisation of rp del projected_sep phi = np.arccos(self.calculate_dot_product_arrays(separation_dir, axis_direction)) # [0,pi] del separation_dir e_plus, e_cross = self.get_ellipticity(e, phi) del phi e_plus[np.isnan(e_plus)] = 0.0 e_cross[np.isnan(e_cross)] = 0.0 # get the indices for the binning mask = (separation_len >= self.r_bins[0]) * (separation_len < self.r_bins[-1]) * ( LOS >= self.pi_bins[0]) * (LOS < self.pi_bins[-1]) ind_r = np.floor( np.log10(separation_len[mask]) / sub_box_len_logrp - np.log10(self.r_bins[0]) / sub_box_len_logrp ) del separation_len ind_r = np.array(ind_r, dtype=int) ind_pi = np.floor( LOS[mask] / sub_box_len_pi - self.pi_bins[0] / sub_box_len_pi ) # need length of LOS, so only positive values del LOS ind_pi = np.array(ind_pi, dtype=int) if np.any(ind_pi == self.num_bins_pi): ind_pi[ind_pi >= self.num_bins_pi] -= 1 if np.any(ind_r == self.num_bins_r): ind_r[ind_r >= self.num_bins_r] -= 1 np.add.at(Splus_D, (ind_r, ind_pi), (weight[n] * weight_shape[mask] * e_plus[mask]) / (2 * R)) np.add.at(Scross_D, (ind_r, ind_pi), (weight[n] * weight_shape[mask] * e_cross[mask]) / (2 * R)) np.add.at(variance, (ind_r, ind_pi), ((weight[n] * weight_shape[mask] * e_plus[mask]) / (2 * R)) ** 2) shape_mask = np.where(jackknife_region_indices_shape[mask] != jackknife_region_indices_pos[n])[0] np.add.at(Splus_D_jk, (jackknife_region_indices_pos[n], ind_r, ind_pi), (weight[n] * weight_shape[mask] * e_plus[mask])) # responsivity added later np.add.at(Splus_D_jk, (jackknife_region_indices_shape[mask][shape_mask], ind_r[shape_mask], ind_pi[shape_mask]), (weight[n] * weight_shape[mask][shape_mask] * e_plus[mask][ shape_mask])) # responsivity added later del e_plus, e_cross np.add.at(DD, (ind_r, ind_pi), weight[n] * weight_shape[mask]) np.add.at(DD_jk, (jackknife_region_indices_pos[n], ind_r, ind_pi), (weight[n] * weight_shape[mask])) np.add.at(DD_jk, (jackknife_region_indices_shape[mask][shape_mask], ind_r[shape_mask], ind_pi[shape_mask]), (weight[n] * weight_shape[mask][shape_mask])) R_jk = np.zeros(num_box) for i in np.arange(num_box): jk_mask = np.where(jackknife_region_indices_shape != i) R_jk[i] = sum(weight_shape[jk_mask] * (1 - e[jk_mask] ** 2 / 2.0)) / sum(weight_shape[jk_mask]) corrtype = \"cross\" for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_g_plus[i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], self.pi_bins[p + 1], self.pi_bins[p], L3, \"cross\", Num_position, Num_shape) RR_gg[i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], self.pi_bins[p + 1], self.pi_bins[p], L3, corrtype, Num_position, Num_shape) RR_jk = np.zeros((num_box, self.num_bins_r, self.num_bins_pi)) volume_jk = L3 * (num_box - 1) / (num_box) for jk in np.arange(num_box): Num_position_jk, Num_shape_jk = len(np.where(jackknife_region_indices_pos != jk)[0]), len( np.where(jackknife_region_indices_shape != jk)[0]) for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_jk[jk, i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], self.pi_bins[p + 1], self.pi_bins[p], volume_jk, \"cross\", Num_position_jk, Num_shape_jk) correlation = Splus_D / RR_g_plus # (Splus_D - Splus_R) / RR_g_plus xi_g_cross = Scross_D / RR_g_plus # (Scross_D - Scross_R) / RR_g_plus sigsq = variance / RR_g_plus ** 2 dsep = (self.r_bins[1:] - self.r_bins[:-1]) / 2.0 separation_bins = self.r_bins[:-1] + abs(dsep) # middle of bins dpi = (self.pi_bins[1:] - self.pi_bins[:-1]) / 2.0 pi_bins = self.pi_bins[:-1] + abs(dpi) # middle of bins if (self.output_file_name != None) and (return_output == False): output_file = h5py.File(self.output_file_name, \"a\") group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_g_plus/\") write_dataset_hdf5(group, dataset_name, data=correlation) write_dataset_hdf5(group, dataset_name + \"_SplusD\", data=Splus_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_plus\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_sigmasq\", data=sigsq) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_g_plus/{jk_group_name}\") for i in np.arange(0, num_box): corr = (Splus_D * (2 * R) - Splus_D_jk[i]) / ( RR_jk[i] * 2 * R_jk[i]) # Responsivity will be different for each realisation write_dataset_hdf5(group, dataset_name + f\"_{i}\", data=corr) write_dataset_hdf5(group, dataset_name + f\"_{i}_SplusD\", data=(Splus_D * (2 * R) - Splus_D_jk[i]) / ( 2 * R_jk[i])) # Splus_D_jk[i]/(2*R_jk[i])) write_dataset_hdf5(group, dataset_name + f\"_{i}_RR\", data=RR_jk[i]) write_dataset_hdf5(group, dataset_name + f\"_{i}_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + f\"_{i}_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_g_cross/{jk_group_name}\") write_dataset_hdf5(group, dataset_name + \"_ScrossD\", data=Scross_D) write_dataset_hdf5(group, dataset_name, data=xi_g_cross) write_dataset_hdf5(group, dataset_name + \"_RR_g_cross\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_sigmasq\", data=sigsq) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_gg/\") write_dataset_hdf5(group, dataset_name, data=(DD / RR_gg) - 1) write_dataset_hdf5(group, dataset_name + \"_DD\", data=DD) write_dataset_hdf5(group, dataset_name + \"_RR_gg\", data=RR_gg) write_dataset_hdf5(group, dataset_name + \"_sigmasq\", data=sigsq) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_gg/{jk_group_name}\") for i in np.arange(0, num_box): write_dataset_hdf5(group, dataset_name + f\"_{i}\", data=((DD - DD_jk[i]) / RR_jk[i]) - 1) write_dataset_hdf5(group, dataset_name + f\"_{i}_DD\", data=(DD - DD_jk[i])) write_dataset_hdf5(group, dataset_name + f\"_{i}_RR\", data=RR_jk[i]) write_dataset_hdf5(group, dataset_name + f\"_{i}_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + f\"_{i}_pi\", data=pi_bins) output_file.close() return else: return correlation, (DD / RR_gg) - 1, separation_bins, pi_bins, Splus_D, DD, RR_g_plus def _measure_xi_rp_pi_box_jk_tree(self, dataset_name, L_subboxes, masks=None, return_output=False, jk_group_name=\"\", ellipticity='distortion'): r\"\"\"Measures the projected correlation functions including jackknife realisations, $\\xi_{gg}$ and $\\xi_{g+}$, in (rp, pi) bins for an object created with MeasureIABox. Uses 1 CPU. Uses KDTree for speedup. Parameters ---------- dataset_name : str Name of the dataset in the output file. L_subboxes: int Number of subboxes on one side of the box. L_subboxes^3 is the total number of jackknife realisations. masks : dict or NoneType, optional Dictionary with masks for the data to select only part of the data. Uses same keywords as data dictionary. Default value = None. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. jk_group_name : str, optional Group in output file (hdf5) where jackknife realisations are stored. Default value is \"\". ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. Returns ------- ndarrays $\\xi_{gg}$ and $\\xi_{g+}$, r_p bins, pi bins, S+D, DD, RR (if no output file is specified) \"\"\" if masks == None: positions = self.data[\"Position\"] positions_shape_sample = self.data[\"Position_shape_sample\"] axis_direction_v = self.data[\"Axis_Direction\"] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"] weight = self.data[\"weight\"] weight_shape = self.data[\"weight_shape_sample\"] else: positions = self.data[\"Position\"][masks[\"Position\"]] positions_shape_sample = self.data[\"Position_shape_sample\"][masks[\"Position_shape_sample\"]] axis_direction_v = self.data[\"Axis_Direction\"][masks[\"Axis_Direction\"]] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"][masks[\"q\"]] try: weight_mask = masks[\"weight\"] except: masks[\"weight\"] = np.ones(self.Num_position, dtype=bool) masks[\"weight\"][sum(masks[\"Position\"]):self.Num_position] = 0 try: weight_mask = masks[\"weight_shape_sample\"] except: masks[\"weight_shape_sample\"] = np.ones(self.Num_shape, dtype=bool) masks[\"weight_shape_sample\"][sum(masks[\"Position_shape_sample\"]):self.Num_shape] = 0 weight = self.data[\"weight\"][masks[\"weight\"]] weight_shape = self.data[\"weight_shape_sample\"][masks[\"weight_shape_sample\"]] Num_position = len(positions) Num_shape = len(positions_shape_sample) print( f\"There are {Num_shape} galaxies in the shape sample and {Num_position} galaxies in the position sample.\") LOS_ind = self.data[\"LOS\"] # eg 2 for z axis not_LOS = np.array([0, 1, 2])[np.isin([0, 1, 2], LOS_ind, invert=True)] # eg 0,1 for x&y if ellipticity == 'distortion': e = (1 - q ** 2) / (1 + q ** 2) # size of ellipticity elif ellipticity == 'ellipticity': e = (1 - q) / (1 + q) else: raise ValueError(\"Invalid value for ellipticity. Choose 'distortion' or 'ellipticity'.\") del q R = sum(weight_shape * (1 - e ** 2 / 2.0)) / sum(weight_shape) # R = 1 - np.mean(e ** 2) / 2.0 # responsitivity factor L3 = self.boxsize ** 3 # box volume sub_box_len_logrp = (np.log10(self.r_max) - np.log10(self.r_min)) / self.num_bins_r sub_box_len_pi = (self.pi_bins[-1] - self.pi_bins[0]) / self.num_bins_pi DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_g_plus = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_gg = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) variance = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) jackknife_region_indices_pos, jackknife_region_indices_shape = self._get_jackknife_region_indices(masks, L_subboxes) num_box = L_subboxes ** 3 DD_jk = np.zeros((num_box, self.num_bins_r, self.num_bins_pi)) Splus_D_jk = np.zeros((num_box, self.num_bins_r, self.num_bins_pi)) figname_dataset_name = dataset_name if \"/\" in dataset_name: figname_dataset_name = figname_dataset_name.replace(\"/\", \"_\") if \".\" in dataset_name: figname_dataset_name = figname_dataset_name.replace(\".\", \"p\") pos_tree = KDTree(positions[:, not_LOS], boxsize=self.boxsize) for i in np.arange(0, len(positions_shape_sample), 100): i2 = min(len(positions_shape_sample), i + 100) positions_shape_sample_i = positions_shape_sample[i:i2] axis_direction_i = axis_direction[i:i2] e_i = e[i:i2] weight_shape_i = weight_shape[i:i2] jackknife_region_indices_shape_i = jackknife_region_indices_shape[i:i2] shape_tree = KDTree(positions_shape_sample_i[:, not_LOS], boxsize=self.boxsize) ind_min_i = shape_tree.query_ball_tree(pos_tree, self.r_min) ind_max_i = shape_tree.query_ball_tree(pos_tree, self.r_max) ind_rbin_i = self.setdiff2D(ind_max_i, ind_min_i) for n in np.arange(0, len(positions_shape_sample_i)): # CHANGE2: loop now over shapes, not positions if len(ind_rbin_i[n]) > 0: # for Splus_D (calculate ellipticities around position sample) separation = positions_shape_sample_i[n] - positions[ind_rbin_i[n]] # CHANGE1 & CHANGE2 if self.periodicity: separation[separation > self.L_0p5] -= self.boxsize # account for periodicity of box separation[separation < -self.L_0p5] += self.boxsize projected_sep = separation[:, not_LOS] LOS = separation[:, LOS_ind] separation_len = np.sqrt(np.sum(projected_sep ** 2, axis=1)) with np.errstate(invalid='ignore'): separation_dir = (projected_sep.transpose() / separation_len).transpose() # normalisation of rp del projected_sep, separation phi = np.arccos( separation_dir[:, 0] * axis_direction_i[n, 0] + separation_dir[:, 1] * axis_direction_i[ n, 1]) # CHANGE2 e_plus, e_cross = self.get_ellipticity(e_i[n], phi) # CHANGE2 del phi, separation_dir e_plus[np.isnan(e_plus)] = 0.0 e_cross[np.isnan(e_cross)] = 0.0 # get the indices for the binning mask = (separation_len >= self.r_bins[0]) * (separation_len < self.r_bins[-1]) * ( LOS >= self.pi_bins[0]) * (LOS < self.pi_bins[-1]) ind_r = np.floor( np.log10(separation_len[mask]) / sub_box_len_logrp - np.log10( self.r_bins[0]) / sub_box_len_logrp ) ind_r = np.array(ind_r, dtype=int) ind_pi = np.floor( LOS[mask] / sub_box_len_pi - self.pi_bins[0] / sub_box_len_pi ) # need length of LOS, so only positive values ind_pi = np.array(ind_pi, dtype=int) if np.any(ind_pi == self.num_bins_pi): ind_pi[ind_pi >= self.num_bins_pi] -= 1 if np.any(ind_r == self.num_bins_r): ind_r[ind_r >= self.num_bins_r] -= 1 np.add.at(Splus_D, (ind_r, ind_pi), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_plus[mask]) / (2 * R)) np.add.at(Scross_D, (ind_r, ind_pi), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_cross[mask]) / (2 * R)) del separation_len np.add.at(DD, (ind_r, ind_pi), weight[ind_rbin_i[n]][mask] * weight_shape_i[n]) pos_mask = \\ np.where( jackknife_region_indices_pos[ind_rbin_i[n]][mask] != jackknife_region_indices_shape_i[n])[ 0] np.add.at(Splus_D_jk, (jackknife_region_indices_shape_i[n], ind_r, ind_pi), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_plus[ mask])) # responsivity added later np.add.at(Splus_D_jk, (jackknife_region_indices_pos[ind_rbin_i[n]][mask][pos_mask], ind_r[pos_mask], ind_pi[pos_mask]), (weight[ind_rbin_i[n]][mask][pos_mask] * weight_shape_i[n] * e_plus[mask][ pos_mask])) # responsivity added later del e_plus, e_cross np.add.at(DD_jk, (jackknife_region_indices_shape_i[n], ind_r, ind_pi), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n])) # responsivity added later np.add.at(DD_jk, (jackknife_region_indices_pos[ind_rbin_i[n]][mask][pos_mask], ind_r[pos_mask], ind_pi[pos_mask]), (weight[ind_rbin_i[n]][mask][pos_mask] * weight_shape_i[n])) # responsivity added later R_jk = np.zeros(num_box) for i in np.arange(num_box): jk_mask = np.where(jackknife_region_indices_shape != i) R_jk[i] = sum(weight_shape[jk_mask] * (1 - e[jk_mask] ** 2 / 2.0)) / sum(weight_shape[jk_mask]) corrtype = \"cross\" for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_g_plus[i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], self.pi_bins[p + 1], self.pi_bins[p], L3, \"cross\", Num_position, Num_shape) RR_gg[i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], self.pi_bins[p + 1], self.pi_bins[p], L3, corrtype, Num_position, Num_shape) RR_jk = np.zeros((num_box, self.num_bins_r, self.num_bins_pi)) volume_jk = L3 * (num_box - 1) / num_box for jk in np.arange(num_box): Num_position_jk, Num_shape_jk = len(np.where(jackknife_region_indices_pos != jk)[0]), len( np.where(jackknife_region_indices_shape != jk)[0]) for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_jk[jk, i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], self.pi_bins[p + 1], self.pi_bins[p], volume_jk, \"cross\", Num_position_jk, Num_shape_jk) correlation = Splus_D / RR_g_plus # (Splus_D - Splus_R) / RR_g_plus xi_g_cross = Scross_D / RR_g_plus # (Scross_D - Scross_R) / RR_g_plus sigsq = variance / RR_g_plus ** 2 dsep = (self.r_bins[1:] - self.r_bins[:-1]) / 2.0 separation_bins = self.r_bins[:-1] + abs(dsep) # middle of bins dpi = (self.pi_bins[1:] - self.pi_bins[:-1]) / 2.0 pi_bins = self.pi_bins[:-1] + abs(dpi) # middle of bins if (self.output_file_name != None) and (return_output == False): output_file = h5py.File(self.output_file_name, \"a\") group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_g_plus/\") write_dataset_hdf5(group, dataset_name, data=correlation) write_dataset_hdf5(group, dataset_name + \"_SplusD\", data=Splus_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_plus\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_sigmasq\", data=sigsq) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_g_plus/{jk_group_name}\") for i in np.arange(0, num_box): corr = (Splus_D * (2 * R) - Splus_D_jk[i]) / ( RR_jk[i] * 2 * R_jk[i]) # Responsivity will be different for each realisation write_dataset_hdf5(group, dataset_name + f\"_{i}\", data=corr) write_dataset_hdf5(group, dataset_name + f\"_{i}_SplusD\", data=(Splus_D * (2 * R) - Splus_D_jk[i]) / ( 2 * R_jk[i])) # Splus_D_jk[i]/(2*R_jk[i])) write_dataset_hdf5(group, dataset_name + f\"_{i}_RR\", data=RR_jk[i]) write_dataset_hdf5(group, dataset_name + f\"_{i}_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + f\"_{i}_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_g_cross/{jk_group_name}\") write_dataset_hdf5(group, dataset_name + \"_ScrossD\", data=Scross_D) write_dataset_hdf5(group, dataset_name, data=xi_g_cross) write_dataset_hdf5(group, dataset_name + \"_RR_g_cross\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_sigmasq\", data=sigsq) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_gg/\") write_dataset_hdf5(group, dataset_name, data=(DD / RR_gg) - 1) write_dataset_hdf5(group, dataset_name + \"_DD\", data=DD) write_dataset_hdf5(group, dataset_name + \"_RR_gg\", data=RR_gg) write_dataset_hdf5(group, dataset_name + \"_sigmasq\", data=sigsq) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_gg/{jk_group_name}\") for i in np.arange(0, num_box): write_dataset_hdf5(group, dataset_name + f\"_{i}\", data=((DD - DD_jk[i]) / RR_jk[i]) - 1) write_dataset_hdf5(group, dataset_name + f\"_{i}_DD\", data=(DD - DD_jk[i])) write_dataset_hdf5(group, dataset_name + f\"_{i}_RR\", data=RR_jk[i]) write_dataset_hdf5(group, dataset_name + f\"_{i}_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + f\"_{i}_pi\", data=pi_bins) output_file.close() return else: return correlation, (DD / RR_gg) - 1, separation_bins, pi_bins, Splus_D, DD, RR_g_plus def _measure_xi_rp_pi_box_jk_batch(self, i): r\"\"\"Measures components of $\\xi_{gg}$ and $\\xi_{g+}$ in (rp,pi) bins including jackknife realisations for a batch of indices from i to i+chunk_size. Support function for _measure_xi_rp_pi_box_jk_multiprocessing(). Parameters ---------- i: int Start index of the batch. Returns ------- ndarrays S+D, SxD, DD, DD_jk, S+D_jk where the _jk versions store the necessary information of DD of S+D for each jackknife realisation. \"\"\" if i + self.chunk_size > self.Num_shape_masked: i2 = self.Num_shape_masked else: i2 = i + self.chunk_size DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) DD_jk = np.zeros((self.num_box, self.num_bins_r, self.num_bins_pi)) Splus_D_jk = np.zeros((self.num_box, self.num_bins_r, self.num_bins_pi)) positions_shape_sample_i = self.temp_data_obj.read_cat(\"positions_shape_sample\", [i, i2]) axis_direction_i = self.temp_data_obj.read_cat(\"axis_direction\", [i, i2]) weight_shape_i = self.temp_data_obj.read_cat(\"weight_shape\", [i, i2]) positions = self.temp_data_obj.read_cat(\"positions\") weight = self.temp_data_obj.read_cat(\"weight\") e_i = self.e[i:i2] jackknife_region_indices_shape_i = self.jackknife_region_indices_shape[i:i2] shape_tree = KDTree(positions_shape_sample_i[:, self.not_LOS], boxsize=self.boxsize) ind_min_i = shape_tree.query_ball_tree(self.pos_tree, self.r_min) ind_max_i = shape_tree.query_ball_tree(self.pos_tree, self.r_max) ind_rbin_i = self.setdiff2D(ind_max_i, ind_min_i) for n in np.arange(0, len(positions_shape_sample_i)): # CHANGE2: loop now over shapes, not positions if len(ind_rbin_i[n]) > 0: # for Splus_D (calculate ellipticities around position sample) separation = positions_shape_sample_i[n] - positions[ind_rbin_i[n]] # CHANGE1 & CHANGE2 if self.periodicity: separation[separation > self.L_0p5] -= self.boxsize # account for periodicity of box separation[separation < -self.L_0p5] += self.boxsize projected_sep = separation[:, self.not_LOS] LOS = separation[:, self.LOS_ind] separation_len = np.sqrt(np.sum(projected_sep ** 2, axis=1)) with np.errstate(invalid='ignore'): separation_dir = (projected_sep.transpose() / separation_len).transpose() # normalisation of rp del projected_sep, separation phi = np.arccos( separation_dir[:, 0] * axis_direction_i[n, 0] + separation_dir[:, 1] * axis_direction_i[ n, 1]) # CHANGE2 e_plus, e_cross = self.get_ellipticity(e_i[n], phi) # CHANGE2 del phi, separation_dir e_plus[np.isnan(e_plus)] = 0.0 e_cross[np.isnan(e_cross)] = 0.0 # get the indices for the binning mask = (separation_len >= self.r_bins[0]) * (separation_len < self.r_bins[-1]) * ( LOS >= self.pi_bins[0]) * (LOS < self.pi_bins[-1]) ind_r = np.floor( np.log10(separation_len[mask]) / self.sub_box_len_logrp - np.log10( self.r_bins[0]) / self.sub_box_len_logrp ) ind_r = np.array(ind_r, dtype=int) ind_pi = np.floor( LOS[mask] / self.sub_box_len_pi - self.pi_bins[0] / self.sub_box_len_pi ) # need length of LOS, so only positive values ind_pi = np.array(ind_pi, dtype=int) if np.any(ind_pi == self.num_bins_pi): ind_pi[ind_pi >= self.num_bins_pi] -= 1 if np.any(ind_r == self.num_bins_r): ind_r[ind_r >= self.num_bins_r] -= 1 np.add.at(Splus_D, (ind_r, ind_pi), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_plus[mask]) / (2 * self.R)) np.add.at(Scross_D, (ind_r, ind_pi), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_cross[mask]) / (2 * self.R)) del separation_len np.add.at(DD, (ind_r, ind_pi), weight[ind_rbin_i[n]][mask] * weight_shape_i[n]) pos_mask = \\ np.where( self.jackknife_region_indices_pos[ind_rbin_i[n]][mask] != jackknife_region_indices_shape_i[n])[ 0] np.add.at(Splus_D_jk, (jackknife_region_indices_shape_i[n], ind_r, ind_pi), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n] * e_plus[ mask])) # responsivity added later np.add.at(Splus_D_jk, (self.jackknife_region_indices_pos[ind_rbin_i[n]][mask][pos_mask], ind_r[pos_mask], ind_pi[pos_mask]), (weight[ind_rbin_i[n]][mask][pos_mask] * weight_shape_i[n] * e_plus[mask][ pos_mask])) # responsivity added later del e_plus, e_cross np.add.at(DD_jk, (jackknife_region_indices_shape_i[n], ind_r, ind_pi), (weight[ind_rbin_i[n]][mask] * weight_shape_i[n])) # responsivity added later np.add.at(DD_jk, (self.jackknife_region_indices_pos[ind_rbin_i[n]][mask][pos_mask], ind_r[pos_mask], ind_pi[pos_mask]), (weight[ind_rbin_i[n]][mask][pos_mask] * weight_shape_i[n])) # responsivity added later return Splus_D, Scross_D, DD, DD_jk, Splus_D_jk def _measure_xi_rp_pi_box_jk_multiprocessing(self, dataset_name, L_subboxes, temp_file_path, masks=None, return_output=False, jk_group_name=\"\", chunk_size=1000, num_nodes=1, ellipticity='distortion' ): r\"\"\"Measures the projected correlation functions including jackknife realisations, $\\xi_{gg}$ and $\\xi_{g+}$, in (rp, pi) bins for an object created with MeasureIABox. Uses >1 CPU. Uses KDTree for speedup. Parameters ---------- dataset_name : str Name of the dataset in the output file. L_subboxes: int Number of subboxes on one side of the box. L_subboxes^3 is the total number of jackknife realisations. temp_file_path : str or NoneType, optional Path to where the data is temporarily stored [file name generated automatically]. masks : dict or NoneType, optional Dictionary with masks for the data to select only part of the data. Uses same keywords as data dictionary. Default value = None. return_output : bool, optional If True, the output will be returned instead of written to a file. Default value is False. jk_group_name : str, optional Group in output file (hdf5) where jackknife realisations are stored. Default value is \"\". chunk_size: int, optional Size of the chunks of data sent to each multiprocessing node. If larger, more RAM is needed per node. Default is 1000. num_nodes : int, optional Number of CPUs used in the multiprocessing. Default is 1. ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. Returns ------- ndarrays $\\xi_{gg}$ and $\\xi_{g+}$, r_p bins, pi bins, S+D, DD, RR (if no output file is specified) \"\"\" if masks == None: positions = self.data[\"Position\"] positions_shape_sample = self.data[\"Position_shape_sample\"] axis_direction_v = self.data[\"Axis_Direction\"] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"] weight = self.data[\"weight\"] weight_shape = self.data[\"weight_shape_sample\"] else: positions = self.data[\"Position\"][masks[\"Position\"]] positions_shape_sample = self.data[\"Position_shape_sample\"][masks[\"Position_shape_sample\"]] axis_direction_v = self.data[\"Axis_Direction\"][masks[\"Axis_Direction\"]] axis_direction_len = np.sqrt(np.sum(axis_direction_v ** 2, axis=1)) axis_direction = (axis_direction_v.transpose() / axis_direction_len).transpose() q = self.data[\"q\"][masks[\"q\"]] try: weight_mask = masks[\"weight\"] except: masks[\"weight\"] = np.ones(self.Num_position, dtype=bool) masks[\"weight\"][sum(masks[\"Position\"]):self.Num_position] = 0 try: weight_mask = masks[\"weight_shape_sample\"] except: masks[\"weight_shape_sample\"] = np.ones(self.Num_shape, dtype=bool) masks[\"weight_shape_sample\"][sum(masks[\"Position_shape_sample\"]):self.Num_shape] = 0 weight = self.data[\"weight\"][masks[\"weight\"]] weight_shape = self.data[\"weight_shape_sample\"][masks[\"weight_shape_sample\"]] self.Num_position_masked = len(positions) self.Num_shape_masked = len(positions_shape_sample) # create temp hdf5 from which data can be read. del self.data, but save it in this method to reduce RAM figname_dataset_name = dataset_name if \"/\" in dataset_name: figname_dataset_name = figname_dataset_name.replace(\"/\", \"_\") if \".\" in dataset_name: figname_dataset_name = figname_dataset_name.replace(\".\", \"p\") file_temp = h5py.File(f\"{temp_file_path}/w_{self.simname}_temp_data_{figname_dataset_name}.hdf5\", \"w\") write_dataset_hdf5(file_temp, \"positions\", positions) write_dataset_hdf5(file_temp, \"weight\", weight) write_dataset_hdf5(file_temp, \"weight_shape\", weight_shape) write_dataset_hdf5(file_temp, \"positions_shape_sample\", positions_shape_sample) write_dataset_hdf5(file_temp, \"axis_direction\", axis_direction) file_temp.close() self.temp_data_obj = ReadData(self.simname, f\"w_{self.simname}_temp_data_{figname_dataset_name}\", None, data_path=temp_file_path) print( f\"There are {self.Num_shape_masked} galaxies in the shape sample and {self.Num_position_masked} galaxies in the position sample.\") self.LOS_ind = self.data[\"LOS\"] # eg 2 for z axis self.not_LOS = np.array([0, 1, 2])[np.isin([0, 1, 2], self.LOS_ind, invert=True)] # eg 0,1 for x&y if ellipticity == 'distortion': self.e = (1 - q ** 2) / (1 + q ** 2) # size of ellipticity elif ellipticity == 'ellipticity': self.e = (1 - q) / (1 + q) else: raise ValueError(\"Invalid value for ellipticity. Choose 'distortion' or 'ellipticity'.\") del q self.R = sum(weight_shape * (1 - self.e ** 2 / 2.0)) / sum(weight_shape) # R = 1 - np.mean(e ** 2) / 2.0 # responsitivity factor L3 = self.boxsize ** 3 # box volume self.sub_box_len_logrp = (np.log10(self.r_max) - np.log10(self.r_min)) / self.num_bins_r self.sub_box_len_pi = (self.pi_bins[-1] - self.pi_bins[0]) / self.num_bins_pi DD = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Splus_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) Scross_D = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_g_plus = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) RR_gg = np.array([[0.0] * self.num_bins_pi] * self.num_bins_r) self.jackknife_region_indices_pos, self.jackknife_region_indices_shape = self._get_jackknife_region_indices( masks, L_subboxes) self.num_box = L_subboxes ** 3 DD_jk = np.zeros((self.num_box, self.num_bins_r, self.num_bins_pi)) Splus_D_jk = np.zeros((self.num_box, self.num_bins_r, self.num_bins_pi)) data_temp = self.data # make sure data is not sent to every CPU self.data = None self.pos_tree = KDTree(positions[:, self.not_LOS], boxsize=self.boxsize) indices = np.arange(0, len(positions_shape_sample), chunk_size) self.chunk_size = chunk_size with Pool(num_nodes) as p: result = p.map(self._measure_xi_rp_pi_box_jk_batch, indices) os.remove( f\"{temp_file_path}/w_{self.simname}_temp_data_{figname_dataset_name}.hdf5\") self.data = data_temp del data_temp for i in np.arange(len(result)): Splus_D += result[i][0] Scross_D += result[i][1] DD += result[i][2] DD_jk += result[i][3] Splus_D_jk += result[i][4] R_jk = np.zeros(self.num_box) for i in np.arange(self.num_box): jk_mask = np.where(self.jackknife_region_indices_shape != i) R_jk[i] = sum(weight_shape[jk_mask] * (1 - self.e[jk_mask] ** 2 / 2.0)) / sum(weight_shape[jk_mask]) corrtype = \"cross\" for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_g_plus[i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], self.pi_bins[p + 1], self.pi_bins[p], L3, \"cross\", self.Num_position_masked, self.Num_shape_masked) RR_gg[i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], self.pi_bins[p + 1], self.pi_bins[p], L3, corrtype, self.Num_position_masked, self.Num_shape_masked) RR_jk = np.zeros((self.num_box, self.num_bins_r, self.num_bins_pi)) volume_jk = L3 * (self.num_box - 1) / self.num_box for jk in np.arange(self.num_box): Num_position_jk, Num_shape_jk = len(np.where(self.jackknife_region_indices_pos != jk)[0]), len( np.where(self.jackknife_region_indices_shape != jk)[0]) for i in np.arange(0, self.num_bins_r): for p in np.arange(0, self.num_bins_pi): RR_jk[jk, i, p] = self.get_random_pairs( self.r_bins[i + 1], self.r_bins[i], self.pi_bins[p + 1], self.pi_bins[p], volume_jk, \"cross\", Num_position_jk, Num_shape_jk) correlation = Splus_D / RR_g_plus # (Splus_D - Splus_R) / RR_g_plus xi_g_cross = Scross_D / RR_g_plus # (Scross_D - Scross_R) / RR_g_plus dsep = (self.r_bins[1:] - self.r_bins[:-1]) / 2.0 separation_bins = self.r_bins[:-1] + abs(dsep) # middle of bins dpi = (self.pi_bins[1:] - self.pi_bins[:-1]) / 2.0 pi_bins = self.pi_bins[:-1] + abs(dpi) # middle of bins if (self.output_file_name != None) and (return_output == False): output_file = h5py.File(self.output_file_name, \"a\") group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_g_plus/\") write_dataset_hdf5(group, dataset_name, data=correlation) write_dataset_hdf5(group, dataset_name + \"_SplusD\", data=Splus_D) write_dataset_hdf5(group, dataset_name + \"_RR_g_plus\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_g_plus/{jk_group_name}\") for i in np.arange(0, self.num_box): corr = (Splus_D * (2 * self.R) - Splus_D_jk[i]) / ( RR_jk[i] * 2 * R_jk[i]) # Responsivity will be different for each realisation write_dataset_hdf5(group, dataset_name + f\"_{i}\", data=corr) write_dataset_hdf5(group, dataset_name + f\"_{i}_SplusD\", data=(Splus_D * (2 * self.R) - Splus_D_jk[i]) / ( 2 * R_jk[i])) # Splus_D_jk[i]/(2*R_jk[i])) write_dataset_hdf5(group, dataset_name + f\"_{i}_RR\", data=RR_jk[i]) write_dataset_hdf5(group, dataset_name + f\"_{i}_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + f\"_{i}_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_g_cross/{jk_group_name}\") write_dataset_hdf5(group, dataset_name + \"_ScrossD\", data=Scross_D) write_dataset_hdf5(group, dataset_name, data=xi_g_cross) write_dataset_hdf5(group, dataset_name + \"_RR_g_cross\", data=RR_g_plus) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_gg/\") write_dataset_hdf5(group, dataset_name, data=(DD / RR_gg) - 1) write_dataset_hdf5(group, dataset_name + \"_DD\", data=DD) write_dataset_hdf5(group, dataset_name + \"_RR_gg\", data=RR_gg) write_dataset_hdf5(group, dataset_name + \"_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + \"_pi\", data=pi_bins) group = create_group_hdf5(output_file, f\"{self.snap_group}/w/xi_gg/{jk_group_name}\") for i in np.arange(0, self.num_box): write_dataset_hdf5(group, dataset_name + f\"_{i}\", data=((DD - DD_jk[i]) / RR_jk[i]) - 1) write_dataset_hdf5(group, dataset_name + f\"_{i}_DD\", data=(DD - DD_jk[i])) write_dataset_hdf5(group, dataset_name + f\"_{i}_RR\", data=RR_jk[i]) write_dataset_hdf5(group, dataset_name + f\"_{i}_rp\", data=separation_bins) write_dataset_hdf5(group, dataset_name + f\"_{i}_pi\", data=pi_bins) output_file.close() return else: return correlation, (DD / RR_gg) - 1, separation_bins, pi_bins, Splus_D, DD, RR_g_plus","title":"MeasureWBoxJackknife"},{"location":"api/MeasureWBoxJackknife/#measureia.MeasureWBoxJackknife.__init__","text":"The init method of the MeasureWSimulations class. Notes Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. Source code in src/measureia/measure_w_box_jk.py def __init__( self, data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True, ): \"\"\" The __init__ method of the MeasureWSimulations class. Notes ----- Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. \"\"\" super().__init__(data, output_file_name, simulation, snapshot, separation_limits, num_bins_r, num_bins_pi, pi_max, boxsize, periodicity) return handler: python options: show_source: true members_order: source show_root_heading: true heading_level: 2","title":"__init__"},{"location":"api/ReadData/","text":"ReadData measureia.ReadData Bases: SimInfo Class to read different hdf5 data files. Assumes underlying file structures used in MeasureIA and MeasureSnapshotVariables classes. Attributes: catalogue ( str ) \u2013 Catalogue name that contains the data. sub_group ( ( str , optional ) ) \u2013 Name of group(s)/structure within snap_group where dataset is found. Default is empty str. output_file_name ( ( str , optional ) ) \u2013 Name where output should be stored. data_path ( ( str , optional ) ) \u2013 The path to where the data is saved. Default='./data/raw/ Methods: Name Description read_cat Reads the data from the specified catalogue. read_subhalo Read the data from the subhalo files. read_snapshot Read the data from the snapshot files and optionally write to output file. read_snapshot_multiple Read multiple datasets from the snapshot files for a specified shapshot number. Notes Inherits attributes from 'SimInfo', where 'snap_group', 'snap_folder' and 'fof_folder' are used in this class. Source code in src/measureia/read_data.py class ReadData(SimInfo): \"\"\" Class to read different hdf5 data files. Assumes underlying file structures used in MeasureIA and MeasureSnapshotVariables classes. Attributes ---------- catalogue : str Catalogue name that contains the data. sub_group : str, optional Name of group(s)/structure within snap_group where dataset is found. Default is empty str. output_file_name : str, optional Name where output should be stored. data_path : str, optional The path to where the data is saved. Default='./data/raw/ Methods ------- read_cat() Reads the data from the specified catalogue. read_subhalo() Read the data from the subhalo files. read_snapshot() Read the data from the snapshot files and optionally write to output file. read_snapshot_multiple() Read multiple datasets from the snapshot files for a specified shapshot number. Notes ----- Inherits attributes from 'SimInfo', where 'snap_group', 'snap_folder' and 'fof_folder' are used in this class. \"\"\" def __init__( self, simulation, catalogue, snapshot, sub_group=\"\", output_file_name=None, data_path=\"./data/raw/\" ): \"\"\" The __init__ method of the ReadData class. Parameters ---------- simulation : str Identifier of the simulation, allowing for correct information to be obtained. Choose from [TNG100, TNG100_2, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1, FLAMINGO_L2p8]. catalogue : str Catalogue name that contains the data. If groupcat file: 'Subhalo' (then use read_subhalo). If snapshot file: enter 'PartTypeX' where X is the particle type number (then use read_snapshot). snapshot : int or str or NoneType Number of the snapshot. sub_group : str, optional Name of group(s)/structure within snap_group where dataset is found. Default is empty str. output_file_name : str, optional Name where output should be stored. data_path : str, optional The path to where the data is saved. Default='./data/raw/ \"\"\" SimInfo.__init__(self, simulation, snapshot, boxsize=None, file_info=True) self.catalogue = catalogue self.sub_group = sub_group + \"/\" self.data_path = data_path + \"/\" self.output_file_name = output_file_name self.r = None self.rp = None self.w_gg = None self.w_gp = None self.multipoles_gg = None self.multipoles_gp = None self.cov_multipoles_gg = None self.errors_multipoles_gg = None self.cov_multipoles_gp = None self.errors_multipoles_gp = None self.cov_w_gg = None self.errors_w_gg = None self.cov_w_gp = None self.errors_w_gp = None return def read_cat(self, dataset_name, cut=None): \"\"\"Reads the data from the specified catalogue. Parameters ---------- dataset_name : the dataset name for the requested data cut : iterable with 2 entries Read dataset slice [cut[0]:cut[1]]. Default value = None Returns ------- ndarray The requested dataset (slice) Raises ------ KeyError If catalogue=Subhalo or Snapshot. \"\"\" if self.catalogue == \"Subhalo\": raise KeyError(\"Use read_subhalo method\") elif self.catalogue == \"Snapshot\": raise KeyError(\"Use read_snapshot method\") file = h5py.File(f\"{self.data_path}{self.catalogue}.hdf5\", \"r\") if cut == None: data = file[self.snap_group + self.sub_group + dataset_name][:] else: data = file[self.snap_group + self.sub_group + dataset_name][cut[0]: cut[1]] file.close() return data def read_subhalo(self, dataset_name, Nfiles=0): \"\"\"Read the data from the subhalo files. Parameters ---------- dataset_name : The dataset name for the requested data Nfiles : int, optional Number of files to read from. Default=0, in which case the number from SimInfo object is used. Returns ------- ndarray The requested dataset \"\"\" subhalo_file = h5py.File(f\"{self.data_path}{self.fof_folder}.0.hdf5\", \"r\") Subhalo = subhalo_file[self.catalogue] try: data = Subhalo[dataset_name][:] except KeyError: print(\"Variable not found in Subhalo files. Choose from \", Subhalo.keys()) if len(np.shape(data)) > 1: stack = True else: stack = False subhalo_file.close() if Nfiles == 0: Nfiles = self.N_files for n in np.arange(1, Nfiles): subhalo_file = h5py.File(f\"{self.data_path}{self.fof_folder}.{n}.hdf5\", \"r\") try: Subhalo = subhalo_file[self.catalogue] data_n = Subhalo[dataset_name][:] # get data single file except KeyError: print(\"problem at file \", n) subhalo_file.close() continue if stack: data = np.vstack((data, data_n)) else: data = np.append(data, data_n) subhalo_file.close() return data def read_snapshot(self, dataset_name): \"\"\"Read the data from the snapshot files and optionally write to output file. Parameters ---------- dataset_name : The dataset name for the requested data Returns ------- ndarray The requested dataset or nothing if output_file_name is specified \"\"\" if self.output_file_name != None: output_file = h5py.File(self.output_file_name, \"a\") group_out = create_group_hdf5(output_file, self.snap_group) write_output = True else: write_output = False print(dataset_name) snap_file = h5py.File(f\"{self.data_path}{self.snap_folder}.0.hdf5\", \"r\") Snap_data = snap_file[self.catalogue] try: data = Snap_data[dataset_name][:] except KeyError: print(f\"Variable not found in Snapshot files: {dataset_name}. Choose from \", Snap_data.keys()) if len(np.shape(data)) > 1: stack = True else: stack = False if write_output: try: dataset = group_out[dataset_name] del group_out[dataset_name] except: pass if stack: group_out.create_dataset(dataset_name, data=data, maxshape=(None, np.shape(data)[1]), chunks=True) else: group_out.create_dataset(dataset_name, data=data, maxshape=(None,), chunks=True) snap_file.close() for n in np.arange(1, self.N_files): snap_file = h5py.File(f\"{self.data_path}{self.snap_folder}.{n}.hdf5\", \"r\") try: Snap_data = snap_file[self.catalogue] data_n = Snap_data[dataset_name][:] # get data single file except KeyError: print(\"problem at file \", n) snap_file.close() continue if write_output: group_out[dataset_name].resize((group_out[dataset_name].shape[0] + data_n.shape[0]), axis=0) group_out[dataset_name][-data_n.shape[0]:] = data_n else: if stack: data = np.vstack((data, data_n)) else: data = np.append(data, data_n) snap_file.close() if write_output: output_file.close() return else: return data def read_snapshot_multiple(self, dataset_name): \"\"\"Read multiple datasets from the snapshot files for a specified shapshot number. Parameters ---------- dataset_name : list or str The dataset names for the requested data Returns ------- ndarray The requested datasets or nothing if output_file_name is specified \"\"\" if self.output_file_name != None: output_file = h5py.File(self.output_file_name, \"a\") group_out = create_group_hdf5(output_file, self.snap_group) write_output = True else: write_output = False snap_file = h5py.File(f\"{self.data_path}{self.snap_folder}.0.hdf5\", \"r\") Snap_data = snap_file[self.catalogue] stack = [] for i, variable in enumerate(dataset_name): try: data = Snap_data[dataset_name[i]][:] except KeyError: print(f\"Variable not found in Snapshot files {variable}. Choose from \", Snap_data.keys()) if len(np.shape(data)) > 1: stack.append(True) else: stack.append(False) if write_output: try: dataset = group_out[variable] del group_out[variable] except: pass if stack[i]: group_out.create_dataset(variable, data=data, maxshape=(None, np.shape(data)[1]), chunks=True) else: group_out.create_dataset(variable, data=data, maxshape=(None,), chunks=True) snap_file.close() for n in np.arange(1, self.N_files): snap_file = h5py.File(f\"{self.data_path}{self.snap_folder}.{n}.hdf5\", \"r\") for i, variable in enumerate(dataset_name): try: Snap_data = snap_file[self.catalogue] data_n = Snap_data[variable][:] # get data single file except KeyError: print(\"problem at file \", n) snap_file.close() continue if write_output: group_out[variable].resize((group_out[variable].shape[0] + data_n.shape[0]), axis=0) group_out[variable][-data_n.shape[0]:] = data_n else: if stack[i]: data = np.vstack((data, data_n)) else: data = np.append(data, data_n) snap_file.close() if write_output: output_file.close() return else: return data def read_MeasureIA_output(self, dataset_name, num_jk): \"\"\" Fills in the available w_gg, w_gp, multipoles_gg, multipoles_gp, r, rp, and associated cov and errors attributes for a given dataset and num_jk from the output file of MeasureIA. Parameters ---------- dataset_name: str Name of the dataset in the output file of MeasureIA. num_jk: int or str or NoneType Number of jackknife patches to be generated internally. If None, the covariance will not be read. Returns ------- \"\"\" # reset parameters (if same object is used for multiple datasets) self.r = None self.rp = None self.w_gg = None self.w_gp = None self.multipoles_gg = None self.multipoles_gp = None self.cov_multipoles_gg = None self.errors_multipoles_gg = None self.cov_multipoles_gp = None self.errors_multipoles_gp = None self.cov_w_gg = None self.errors_w_gg = None self.cov_w_gp = None self.errors_w_gp = None file = h5py.File(f\"{self.data_path}{self.catalogue}.hdf5\", \"r\") if self.snap_group != \"\": data_group = file[self.snap_group] else: data_group = file try: self.multipoles_gg = data_group[f\"multipoles_gg/{dataset_name}\"][:] self.r = data_group[f\"multipoles_gg/{dataset_name}_r\"][:] if num_jk != None: self.cov_multipoles_gg = data_group[f\"multipoles_gg/{dataset_name}_jackknife_cov_{num_jk}\"][:] self.errors_multipoles_gg = data_group[f\"multipoles_gg/{dataset_name}_jackknife_{num_jk}\"][:] except KeyError: pass try: self.multipoles_gp = data_group[f\"multipoles_g_plus/{dataset_name}\"][:] self.r = data_group[f\"multipoles_g_plus/{dataset_name}_r\"][:] if num_jk != None: self.cov_multipoles_gp = data_group[f\"multipoles_g_plus/{dataset_name}_jackknife_cov_{num_jk}\"][:] self.errors_multipoles_gp = data_group[f\"multipoles_g_plus/{dataset_name}_jackknife_{num_jk}\"][:] except KeyError: pass try: self.w_gg = data_group[f\"w_gg/{dataset_name}\"][:] self.rp = data_group[f\"w_gg/{dataset_name}_rp\"][:] if num_jk != None: self.cov_w_gg = data_group[f\"w_gg/{dataset_name}_jackknife_cov_{num_jk}\"][:] self.errors_w_gg = data_group[f\"w_gg/{dataset_name}_jackknife_{num_jk}\"][:] except KeyError: pass try: self.w_gp = data_group[f\"w_g_plus/{dataset_name}\"][:] self.rp = data_group[f\"w_g_plus/{dataset_name}_rp\"][:] if num_jk != None: self.cov_w_gp = data_group[f\"w_g_plus/{dataset_name}_jackknife_cov_{num_jk}\"][:] self.errors_w_gp = data_group[f\"w_g_plus/{dataset_name}_jackknife_{num_jk}\"][:] except KeyError: pass file.close() return __init__(simulation, catalogue, snapshot, sub_group='', output_file_name=None, data_path='./data/raw/') The init method of the ReadData class. Parameters: simulation ( str ) \u2013 Identifier of the simulation, allowing for correct information to be obtained. Choose from [TNG100, TNG100_2, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1, FLAMINGO_L2p8]. catalogue ( str ) \u2013 Catalogue name that contains the data. If groupcat file: 'Subhalo' (then use read_subhalo). If snapshot file: enter 'PartTypeX' where X is the particle type number (then use read_snapshot). snapshot ( int or str or NoneType ) \u2013 Number of the snapshot. sub_group ( str , default: '' ) \u2013 Name of group(s)/structure within snap_group where dataset is found. Default is empty str. output_file_name ( str , default: None ) \u2013 Name where output should be stored. data_path ( str , default: './data/raw/' ) \u2013 The path to where the data is saved. Default='./data/raw/ Source code in src/measureia/read_data.py def __init__( self, simulation, catalogue, snapshot, sub_group=\"\", output_file_name=None, data_path=\"./data/raw/\" ): \"\"\" The __init__ method of the ReadData class. Parameters ---------- simulation : str Identifier of the simulation, allowing for correct information to be obtained. Choose from [TNG100, TNG100_2, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1, FLAMINGO_L2p8]. catalogue : str Catalogue name that contains the data. If groupcat file: 'Subhalo' (then use read_subhalo). If snapshot file: enter 'PartTypeX' where X is the particle type number (then use read_snapshot). snapshot : int or str or NoneType Number of the snapshot. sub_group : str, optional Name of group(s)/structure within snap_group where dataset is found. Default is empty str. output_file_name : str, optional Name where output should be stored. data_path : str, optional The path to where the data is saved. Default='./data/raw/ \"\"\" SimInfo.__init__(self, simulation, snapshot, boxsize=None, file_info=True) self.catalogue = catalogue self.sub_group = sub_group + \"/\" self.data_path = data_path + \"/\" self.output_file_name = output_file_name self.r = None self.rp = None self.w_gg = None self.w_gp = None self.multipoles_gg = None self.multipoles_gp = None self.cov_multipoles_gg = None self.errors_multipoles_gg = None self.cov_multipoles_gp = None self.errors_multipoles_gp = None self.cov_w_gg = None self.errors_w_gg = None self.cov_w_gp = None self.errors_w_gp = None return read_cat(dataset_name, cut=None) Reads the data from the specified catalogue. Parameters: dataset_name \u2013 the dataset name for the requested data cut ( iterable with 2 entries , default: None ) \u2013 Read dataset slice [cut[0]:cut[1]]. Default value = None Returns: ndarray \u2013 The requested dataset (slice) Raises: KeyError \u2013 If catalogue=Subhalo or Snapshot. Source code in src/measureia/read_data.py def read_cat(self, dataset_name, cut=None): \"\"\"Reads the data from the specified catalogue. Parameters ---------- dataset_name : the dataset name for the requested data cut : iterable with 2 entries Read dataset slice [cut[0]:cut[1]]. Default value = None Returns ------- ndarray The requested dataset (slice) Raises ------ KeyError If catalogue=Subhalo or Snapshot. \"\"\" if self.catalogue == \"Subhalo\": raise KeyError(\"Use read_subhalo method\") elif self.catalogue == \"Snapshot\": raise KeyError(\"Use read_snapshot method\") file = h5py.File(f\"{self.data_path}{self.catalogue}.hdf5\", \"r\") if cut == None: data = file[self.snap_group + self.sub_group + dataset_name][:] else: data = file[self.snap_group + self.sub_group + dataset_name][cut[0]: cut[1]] file.close() return data read_subhalo(dataset_name, Nfiles=0) Read the data from the subhalo files. Parameters: dataset_name \u2013 The dataset name for the requested data Nfiles ( int , default: 0 ) \u2013 Number of files to read from. Default=0, in which case the number from SimInfo object is used. Returns: ndarray \u2013 The requested dataset Source code in src/measureia/read_data.py def read_subhalo(self, dataset_name, Nfiles=0): \"\"\"Read the data from the subhalo files. Parameters ---------- dataset_name : The dataset name for the requested data Nfiles : int, optional Number of files to read from. Default=0, in which case the number from SimInfo object is used. Returns ------- ndarray The requested dataset \"\"\" subhalo_file = h5py.File(f\"{self.data_path}{self.fof_folder}.0.hdf5\", \"r\") Subhalo = subhalo_file[self.catalogue] try: data = Subhalo[dataset_name][:] except KeyError: print(\"Variable not found in Subhalo files. Choose from \", Subhalo.keys()) if len(np.shape(data)) > 1: stack = True else: stack = False subhalo_file.close() if Nfiles == 0: Nfiles = self.N_files for n in np.arange(1, Nfiles): subhalo_file = h5py.File(f\"{self.data_path}{self.fof_folder}.{n}.hdf5\", \"r\") try: Subhalo = subhalo_file[self.catalogue] data_n = Subhalo[dataset_name][:] # get data single file except KeyError: print(\"problem at file \", n) subhalo_file.close() continue if stack: data = np.vstack((data, data_n)) else: data = np.append(data, data_n) subhalo_file.close() return data read_snapshot(dataset_name) Read the data from the snapshot files and optionally write to output file. Parameters: dataset_name \u2013 The dataset name for the requested data Returns: ndarray \u2013 The requested dataset or nothing if output_file_name is specified Source code in src/measureia/read_data.py def read_snapshot(self, dataset_name): \"\"\"Read the data from the snapshot files and optionally write to output file. Parameters ---------- dataset_name : The dataset name for the requested data Returns ------- ndarray The requested dataset or nothing if output_file_name is specified \"\"\" if self.output_file_name != None: output_file = h5py.File(self.output_file_name, \"a\") group_out = create_group_hdf5(output_file, self.snap_group) write_output = True else: write_output = False print(dataset_name) snap_file = h5py.File(f\"{self.data_path}{self.snap_folder}.0.hdf5\", \"r\") Snap_data = snap_file[self.catalogue] try: data = Snap_data[dataset_name][:] except KeyError: print(f\"Variable not found in Snapshot files: {dataset_name}. Choose from \", Snap_data.keys()) if len(np.shape(data)) > 1: stack = True else: stack = False if write_output: try: dataset = group_out[dataset_name] del group_out[dataset_name] except: pass if stack: group_out.create_dataset(dataset_name, data=data, maxshape=(None, np.shape(data)[1]), chunks=True) else: group_out.create_dataset(dataset_name, data=data, maxshape=(None,), chunks=True) snap_file.close() for n in np.arange(1, self.N_files): snap_file = h5py.File(f\"{self.data_path}{self.snap_folder}.{n}.hdf5\", \"r\") try: Snap_data = snap_file[self.catalogue] data_n = Snap_data[dataset_name][:] # get data single file except KeyError: print(\"problem at file \", n) snap_file.close() continue if write_output: group_out[dataset_name].resize((group_out[dataset_name].shape[0] + data_n.shape[0]), axis=0) group_out[dataset_name][-data_n.shape[0]:] = data_n else: if stack: data = np.vstack((data, data_n)) else: data = np.append(data, data_n) snap_file.close() if write_output: output_file.close() return else: return data read_snapshot_multiple(dataset_name) Read multiple datasets from the snapshot files for a specified shapshot number. Parameters: dataset_name ( list or str ) \u2013 The dataset names for the requested data Returns: ndarray \u2013 The requested datasets or nothing if output_file_name is specified Source code in src/measureia/read_data.py def read_snapshot_multiple(self, dataset_name): \"\"\"Read multiple datasets from the snapshot files for a specified shapshot number. Parameters ---------- dataset_name : list or str The dataset names for the requested data Returns ------- ndarray The requested datasets or nothing if output_file_name is specified \"\"\" if self.output_file_name != None: output_file = h5py.File(self.output_file_name, \"a\") group_out = create_group_hdf5(output_file, self.snap_group) write_output = True else: write_output = False snap_file = h5py.File(f\"{self.data_path}{self.snap_folder}.0.hdf5\", \"r\") Snap_data = snap_file[self.catalogue] stack = [] for i, variable in enumerate(dataset_name): try: data = Snap_data[dataset_name[i]][:] except KeyError: print(f\"Variable not found in Snapshot files {variable}. Choose from \", Snap_data.keys()) if len(np.shape(data)) > 1: stack.append(True) else: stack.append(False) if write_output: try: dataset = group_out[variable] del group_out[variable] except: pass if stack[i]: group_out.create_dataset(variable, data=data, maxshape=(None, np.shape(data)[1]), chunks=True) else: group_out.create_dataset(variable, data=data, maxshape=(None,), chunks=True) snap_file.close() for n in np.arange(1, self.N_files): snap_file = h5py.File(f\"{self.data_path}{self.snap_folder}.{n}.hdf5\", \"r\") for i, variable in enumerate(dataset_name): try: Snap_data = snap_file[self.catalogue] data_n = Snap_data[variable][:] # get data single file except KeyError: print(\"problem at file \", n) snap_file.close() continue if write_output: group_out[variable].resize((group_out[variable].shape[0] + data_n.shape[0]), axis=0) group_out[variable][-data_n.shape[0]:] = data_n else: if stack[i]: data = np.vstack((data, data_n)) else: data = np.append(data, data_n) snap_file.close() if write_output: output_file.close() return else: return data read_MeasureIA_output(dataset_name, num_jk) Fills in the available w_gg, w_gp, multipoles_gg, multipoles_gp, r, rp, and associated cov and errors attributes for a given dataset and num_jk from the output file of MeasureIA. Parameters: dataset_name \u2013 Name of the dataset in the output file of MeasureIA. num_jk \u2013 Number of jackknife patches to be generated internally. If None, the covariance will not be read. Source code in src/measureia/read_data.py def read_MeasureIA_output(self, dataset_name, num_jk): \"\"\" Fills in the available w_gg, w_gp, multipoles_gg, multipoles_gp, r, rp, and associated cov and errors attributes for a given dataset and num_jk from the output file of MeasureIA. Parameters ---------- dataset_name: str Name of the dataset in the output file of MeasureIA. num_jk: int or str or NoneType Number of jackknife patches to be generated internally. If None, the covariance will not be read. Returns ------- \"\"\" # reset parameters (if same object is used for multiple datasets) self.r = None self.rp = None self.w_gg = None self.w_gp = None self.multipoles_gg = None self.multipoles_gp = None self.cov_multipoles_gg = None self.errors_multipoles_gg = None self.cov_multipoles_gp = None self.errors_multipoles_gp = None self.cov_w_gg = None self.errors_w_gg = None self.cov_w_gp = None self.errors_w_gp = None file = h5py.File(f\"{self.data_path}{self.catalogue}.hdf5\", \"r\") if self.snap_group != \"\": data_group = file[self.snap_group] else: data_group = file try: self.multipoles_gg = data_group[f\"multipoles_gg/{dataset_name}\"][:] self.r = data_group[f\"multipoles_gg/{dataset_name}_r\"][:] if num_jk != None: self.cov_multipoles_gg = data_group[f\"multipoles_gg/{dataset_name}_jackknife_cov_{num_jk}\"][:] self.errors_multipoles_gg = data_group[f\"multipoles_gg/{dataset_name}_jackknife_{num_jk}\"][:] except KeyError: pass try: self.multipoles_gp = data_group[f\"multipoles_g_plus/{dataset_name}\"][:] self.r = data_group[f\"multipoles_g_plus/{dataset_name}_r\"][:] if num_jk != None: self.cov_multipoles_gp = data_group[f\"multipoles_g_plus/{dataset_name}_jackknife_cov_{num_jk}\"][:] self.errors_multipoles_gp = data_group[f\"multipoles_g_plus/{dataset_name}_jackknife_{num_jk}\"][:] except KeyError: pass try: self.w_gg = data_group[f\"w_gg/{dataset_name}\"][:] self.rp = data_group[f\"w_gg/{dataset_name}_rp\"][:] if num_jk != None: self.cov_w_gg = data_group[f\"w_gg/{dataset_name}_jackknife_cov_{num_jk}\"][:] self.errors_w_gg = data_group[f\"w_gg/{dataset_name}_jackknife_{num_jk}\"][:] except KeyError: pass try: self.w_gp = data_group[f\"w_g_plus/{dataset_name}\"][:] self.rp = data_group[f\"w_g_plus/{dataset_name}_rp\"][:] if num_jk != None: self.cov_w_gp = data_group[f\"w_g_plus/{dataset_name}_jackknife_cov_{num_jk}\"][:] self.errors_w_gp = data_group[f\"w_g_plus/{dataset_name}_jackknife_{num_jk}\"][:] except KeyError: pass file.close() return","title":"ReadData"},{"location":"api/ReadData/#readdata","text":"","title":"ReadData"},{"location":"api/ReadData/#measureia.ReadData","text":"Bases: SimInfo Class to read different hdf5 data files. Assumes underlying file structures used in MeasureIA and MeasureSnapshotVariables classes. Attributes: catalogue ( str ) \u2013 Catalogue name that contains the data. sub_group ( ( str , optional ) ) \u2013 Name of group(s)/structure within snap_group where dataset is found. Default is empty str. output_file_name ( ( str , optional ) ) \u2013 Name where output should be stored. data_path ( ( str , optional ) ) \u2013 The path to where the data is saved. Default='./data/raw/ Methods: Name Description read_cat Reads the data from the specified catalogue. read_subhalo Read the data from the subhalo files. read_snapshot Read the data from the snapshot files and optionally write to output file. read_snapshot_multiple Read multiple datasets from the snapshot files for a specified shapshot number. Notes Inherits attributes from 'SimInfo', where 'snap_group', 'snap_folder' and 'fof_folder' are used in this class. Source code in src/measureia/read_data.py class ReadData(SimInfo): \"\"\" Class to read different hdf5 data files. Assumes underlying file structures used in MeasureIA and MeasureSnapshotVariables classes. Attributes ---------- catalogue : str Catalogue name that contains the data. sub_group : str, optional Name of group(s)/structure within snap_group where dataset is found. Default is empty str. output_file_name : str, optional Name where output should be stored. data_path : str, optional The path to where the data is saved. Default='./data/raw/ Methods ------- read_cat() Reads the data from the specified catalogue. read_subhalo() Read the data from the subhalo files. read_snapshot() Read the data from the snapshot files and optionally write to output file. read_snapshot_multiple() Read multiple datasets from the snapshot files for a specified shapshot number. Notes ----- Inherits attributes from 'SimInfo', where 'snap_group', 'snap_folder' and 'fof_folder' are used in this class. \"\"\" def __init__( self, simulation, catalogue, snapshot, sub_group=\"\", output_file_name=None, data_path=\"./data/raw/\" ): \"\"\" The __init__ method of the ReadData class. Parameters ---------- simulation : str Identifier of the simulation, allowing for correct information to be obtained. Choose from [TNG100, TNG100_2, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1, FLAMINGO_L2p8]. catalogue : str Catalogue name that contains the data. If groupcat file: 'Subhalo' (then use read_subhalo). If snapshot file: enter 'PartTypeX' where X is the particle type number (then use read_snapshot). snapshot : int or str or NoneType Number of the snapshot. sub_group : str, optional Name of group(s)/structure within snap_group where dataset is found. Default is empty str. output_file_name : str, optional Name where output should be stored. data_path : str, optional The path to where the data is saved. Default='./data/raw/ \"\"\" SimInfo.__init__(self, simulation, snapshot, boxsize=None, file_info=True) self.catalogue = catalogue self.sub_group = sub_group + \"/\" self.data_path = data_path + \"/\" self.output_file_name = output_file_name self.r = None self.rp = None self.w_gg = None self.w_gp = None self.multipoles_gg = None self.multipoles_gp = None self.cov_multipoles_gg = None self.errors_multipoles_gg = None self.cov_multipoles_gp = None self.errors_multipoles_gp = None self.cov_w_gg = None self.errors_w_gg = None self.cov_w_gp = None self.errors_w_gp = None return def read_cat(self, dataset_name, cut=None): \"\"\"Reads the data from the specified catalogue. Parameters ---------- dataset_name : the dataset name for the requested data cut : iterable with 2 entries Read dataset slice [cut[0]:cut[1]]. Default value = None Returns ------- ndarray The requested dataset (slice) Raises ------ KeyError If catalogue=Subhalo or Snapshot. \"\"\" if self.catalogue == \"Subhalo\": raise KeyError(\"Use read_subhalo method\") elif self.catalogue == \"Snapshot\": raise KeyError(\"Use read_snapshot method\") file = h5py.File(f\"{self.data_path}{self.catalogue}.hdf5\", \"r\") if cut == None: data = file[self.snap_group + self.sub_group + dataset_name][:] else: data = file[self.snap_group + self.sub_group + dataset_name][cut[0]: cut[1]] file.close() return data def read_subhalo(self, dataset_name, Nfiles=0): \"\"\"Read the data from the subhalo files. Parameters ---------- dataset_name : The dataset name for the requested data Nfiles : int, optional Number of files to read from. Default=0, in which case the number from SimInfo object is used. Returns ------- ndarray The requested dataset \"\"\" subhalo_file = h5py.File(f\"{self.data_path}{self.fof_folder}.0.hdf5\", \"r\") Subhalo = subhalo_file[self.catalogue] try: data = Subhalo[dataset_name][:] except KeyError: print(\"Variable not found in Subhalo files. Choose from \", Subhalo.keys()) if len(np.shape(data)) > 1: stack = True else: stack = False subhalo_file.close() if Nfiles == 0: Nfiles = self.N_files for n in np.arange(1, Nfiles): subhalo_file = h5py.File(f\"{self.data_path}{self.fof_folder}.{n}.hdf5\", \"r\") try: Subhalo = subhalo_file[self.catalogue] data_n = Subhalo[dataset_name][:] # get data single file except KeyError: print(\"problem at file \", n) subhalo_file.close() continue if stack: data = np.vstack((data, data_n)) else: data = np.append(data, data_n) subhalo_file.close() return data def read_snapshot(self, dataset_name): \"\"\"Read the data from the snapshot files and optionally write to output file. Parameters ---------- dataset_name : The dataset name for the requested data Returns ------- ndarray The requested dataset or nothing if output_file_name is specified \"\"\" if self.output_file_name != None: output_file = h5py.File(self.output_file_name, \"a\") group_out = create_group_hdf5(output_file, self.snap_group) write_output = True else: write_output = False print(dataset_name) snap_file = h5py.File(f\"{self.data_path}{self.snap_folder}.0.hdf5\", \"r\") Snap_data = snap_file[self.catalogue] try: data = Snap_data[dataset_name][:] except KeyError: print(f\"Variable not found in Snapshot files: {dataset_name}. Choose from \", Snap_data.keys()) if len(np.shape(data)) > 1: stack = True else: stack = False if write_output: try: dataset = group_out[dataset_name] del group_out[dataset_name] except: pass if stack: group_out.create_dataset(dataset_name, data=data, maxshape=(None, np.shape(data)[1]), chunks=True) else: group_out.create_dataset(dataset_name, data=data, maxshape=(None,), chunks=True) snap_file.close() for n in np.arange(1, self.N_files): snap_file = h5py.File(f\"{self.data_path}{self.snap_folder}.{n}.hdf5\", \"r\") try: Snap_data = snap_file[self.catalogue] data_n = Snap_data[dataset_name][:] # get data single file except KeyError: print(\"problem at file \", n) snap_file.close() continue if write_output: group_out[dataset_name].resize((group_out[dataset_name].shape[0] + data_n.shape[0]), axis=0) group_out[dataset_name][-data_n.shape[0]:] = data_n else: if stack: data = np.vstack((data, data_n)) else: data = np.append(data, data_n) snap_file.close() if write_output: output_file.close() return else: return data def read_snapshot_multiple(self, dataset_name): \"\"\"Read multiple datasets from the snapshot files for a specified shapshot number. Parameters ---------- dataset_name : list or str The dataset names for the requested data Returns ------- ndarray The requested datasets or nothing if output_file_name is specified \"\"\" if self.output_file_name != None: output_file = h5py.File(self.output_file_name, \"a\") group_out = create_group_hdf5(output_file, self.snap_group) write_output = True else: write_output = False snap_file = h5py.File(f\"{self.data_path}{self.snap_folder}.0.hdf5\", \"r\") Snap_data = snap_file[self.catalogue] stack = [] for i, variable in enumerate(dataset_name): try: data = Snap_data[dataset_name[i]][:] except KeyError: print(f\"Variable not found in Snapshot files {variable}. Choose from \", Snap_data.keys()) if len(np.shape(data)) > 1: stack.append(True) else: stack.append(False) if write_output: try: dataset = group_out[variable] del group_out[variable] except: pass if stack[i]: group_out.create_dataset(variable, data=data, maxshape=(None, np.shape(data)[1]), chunks=True) else: group_out.create_dataset(variable, data=data, maxshape=(None,), chunks=True) snap_file.close() for n in np.arange(1, self.N_files): snap_file = h5py.File(f\"{self.data_path}{self.snap_folder}.{n}.hdf5\", \"r\") for i, variable in enumerate(dataset_name): try: Snap_data = snap_file[self.catalogue] data_n = Snap_data[variable][:] # get data single file except KeyError: print(\"problem at file \", n) snap_file.close() continue if write_output: group_out[variable].resize((group_out[variable].shape[0] + data_n.shape[0]), axis=0) group_out[variable][-data_n.shape[0]:] = data_n else: if stack[i]: data = np.vstack((data, data_n)) else: data = np.append(data, data_n) snap_file.close() if write_output: output_file.close() return else: return data def read_MeasureIA_output(self, dataset_name, num_jk): \"\"\" Fills in the available w_gg, w_gp, multipoles_gg, multipoles_gp, r, rp, and associated cov and errors attributes for a given dataset and num_jk from the output file of MeasureIA. Parameters ---------- dataset_name: str Name of the dataset in the output file of MeasureIA. num_jk: int or str or NoneType Number of jackknife patches to be generated internally. If None, the covariance will not be read. Returns ------- \"\"\" # reset parameters (if same object is used for multiple datasets) self.r = None self.rp = None self.w_gg = None self.w_gp = None self.multipoles_gg = None self.multipoles_gp = None self.cov_multipoles_gg = None self.errors_multipoles_gg = None self.cov_multipoles_gp = None self.errors_multipoles_gp = None self.cov_w_gg = None self.errors_w_gg = None self.cov_w_gp = None self.errors_w_gp = None file = h5py.File(f\"{self.data_path}{self.catalogue}.hdf5\", \"r\") if self.snap_group != \"\": data_group = file[self.snap_group] else: data_group = file try: self.multipoles_gg = data_group[f\"multipoles_gg/{dataset_name}\"][:] self.r = data_group[f\"multipoles_gg/{dataset_name}_r\"][:] if num_jk != None: self.cov_multipoles_gg = data_group[f\"multipoles_gg/{dataset_name}_jackknife_cov_{num_jk}\"][:] self.errors_multipoles_gg = data_group[f\"multipoles_gg/{dataset_name}_jackknife_{num_jk}\"][:] except KeyError: pass try: self.multipoles_gp = data_group[f\"multipoles_g_plus/{dataset_name}\"][:] self.r = data_group[f\"multipoles_g_plus/{dataset_name}_r\"][:] if num_jk != None: self.cov_multipoles_gp = data_group[f\"multipoles_g_plus/{dataset_name}_jackknife_cov_{num_jk}\"][:] self.errors_multipoles_gp = data_group[f\"multipoles_g_plus/{dataset_name}_jackknife_{num_jk}\"][:] except KeyError: pass try: self.w_gg = data_group[f\"w_gg/{dataset_name}\"][:] self.rp = data_group[f\"w_gg/{dataset_name}_rp\"][:] if num_jk != None: self.cov_w_gg = data_group[f\"w_gg/{dataset_name}_jackknife_cov_{num_jk}\"][:] self.errors_w_gg = data_group[f\"w_gg/{dataset_name}_jackknife_{num_jk}\"][:] except KeyError: pass try: self.w_gp = data_group[f\"w_g_plus/{dataset_name}\"][:] self.rp = data_group[f\"w_g_plus/{dataset_name}_rp\"][:] if num_jk != None: self.cov_w_gp = data_group[f\"w_g_plus/{dataset_name}_jackknife_cov_{num_jk}\"][:] self.errors_w_gp = data_group[f\"w_g_plus/{dataset_name}_jackknife_{num_jk}\"][:] except KeyError: pass file.close() return","title":"ReadData"},{"location":"api/ReadData/#measureia.ReadData.__init__","text":"The init method of the ReadData class. Parameters: simulation ( str ) \u2013 Identifier of the simulation, allowing for correct information to be obtained. Choose from [TNG100, TNG100_2, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1, FLAMINGO_L2p8]. catalogue ( str ) \u2013 Catalogue name that contains the data. If groupcat file: 'Subhalo' (then use read_subhalo). If snapshot file: enter 'PartTypeX' where X is the particle type number (then use read_snapshot). snapshot ( int or str or NoneType ) \u2013 Number of the snapshot. sub_group ( str , default: '' ) \u2013 Name of group(s)/structure within snap_group where dataset is found. Default is empty str. output_file_name ( str , default: None ) \u2013 Name where output should be stored. data_path ( str , default: './data/raw/' ) \u2013 The path to where the data is saved. Default='./data/raw/ Source code in src/measureia/read_data.py def __init__( self, simulation, catalogue, snapshot, sub_group=\"\", output_file_name=None, data_path=\"./data/raw/\" ): \"\"\" The __init__ method of the ReadData class. Parameters ---------- simulation : str Identifier of the simulation, allowing for correct information to be obtained. Choose from [TNG100, TNG100_2, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1, FLAMINGO_L2p8]. catalogue : str Catalogue name that contains the data. If groupcat file: 'Subhalo' (then use read_subhalo). If snapshot file: enter 'PartTypeX' where X is the particle type number (then use read_snapshot). snapshot : int or str or NoneType Number of the snapshot. sub_group : str, optional Name of group(s)/structure within snap_group where dataset is found. Default is empty str. output_file_name : str, optional Name where output should be stored. data_path : str, optional The path to where the data is saved. Default='./data/raw/ \"\"\" SimInfo.__init__(self, simulation, snapshot, boxsize=None, file_info=True) self.catalogue = catalogue self.sub_group = sub_group + \"/\" self.data_path = data_path + \"/\" self.output_file_name = output_file_name self.r = None self.rp = None self.w_gg = None self.w_gp = None self.multipoles_gg = None self.multipoles_gp = None self.cov_multipoles_gg = None self.errors_multipoles_gg = None self.cov_multipoles_gp = None self.errors_multipoles_gp = None self.cov_w_gg = None self.errors_w_gg = None self.cov_w_gp = None self.errors_w_gp = None return","title":"__init__"},{"location":"api/ReadData/#measureia.ReadData.read_cat","text":"Reads the data from the specified catalogue. Parameters: dataset_name \u2013 the dataset name for the requested data cut ( iterable with 2 entries , default: None ) \u2013 Read dataset slice [cut[0]:cut[1]]. Default value = None Returns: ndarray \u2013 The requested dataset (slice) Raises: KeyError \u2013 If catalogue=Subhalo or Snapshot. Source code in src/measureia/read_data.py def read_cat(self, dataset_name, cut=None): \"\"\"Reads the data from the specified catalogue. Parameters ---------- dataset_name : the dataset name for the requested data cut : iterable with 2 entries Read dataset slice [cut[0]:cut[1]]. Default value = None Returns ------- ndarray The requested dataset (slice) Raises ------ KeyError If catalogue=Subhalo or Snapshot. \"\"\" if self.catalogue == \"Subhalo\": raise KeyError(\"Use read_subhalo method\") elif self.catalogue == \"Snapshot\": raise KeyError(\"Use read_snapshot method\") file = h5py.File(f\"{self.data_path}{self.catalogue}.hdf5\", \"r\") if cut == None: data = file[self.snap_group + self.sub_group + dataset_name][:] else: data = file[self.snap_group + self.sub_group + dataset_name][cut[0]: cut[1]] file.close() return data","title":"read_cat"},{"location":"api/ReadData/#measureia.ReadData.read_subhalo","text":"Read the data from the subhalo files. Parameters: dataset_name \u2013 The dataset name for the requested data Nfiles ( int , default: 0 ) \u2013 Number of files to read from. Default=0, in which case the number from SimInfo object is used. Returns: ndarray \u2013 The requested dataset Source code in src/measureia/read_data.py def read_subhalo(self, dataset_name, Nfiles=0): \"\"\"Read the data from the subhalo files. Parameters ---------- dataset_name : The dataset name for the requested data Nfiles : int, optional Number of files to read from. Default=0, in which case the number from SimInfo object is used. Returns ------- ndarray The requested dataset \"\"\" subhalo_file = h5py.File(f\"{self.data_path}{self.fof_folder}.0.hdf5\", \"r\") Subhalo = subhalo_file[self.catalogue] try: data = Subhalo[dataset_name][:] except KeyError: print(\"Variable not found in Subhalo files. Choose from \", Subhalo.keys()) if len(np.shape(data)) > 1: stack = True else: stack = False subhalo_file.close() if Nfiles == 0: Nfiles = self.N_files for n in np.arange(1, Nfiles): subhalo_file = h5py.File(f\"{self.data_path}{self.fof_folder}.{n}.hdf5\", \"r\") try: Subhalo = subhalo_file[self.catalogue] data_n = Subhalo[dataset_name][:] # get data single file except KeyError: print(\"problem at file \", n) subhalo_file.close() continue if stack: data = np.vstack((data, data_n)) else: data = np.append(data, data_n) subhalo_file.close() return data","title":"read_subhalo"},{"location":"api/ReadData/#measureia.ReadData.read_snapshot","text":"Read the data from the snapshot files and optionally write to output file. Parameters: dataset_name \u2013 The dataset name for the requested data Returns: ndarray \u2013 The requested dataset or nothing if output_file_name is specified Source code in src/measureia/read_data.py def read_snapshot(self, dataset_name): \"\"\"Read the data from the snapshot files and optionally write to output file. Parameters ---------- dataset_name : The dataset name for the requested data Returns ------- ndarray The requested dataset or nothing if output_file_name is specified \"\"\" if self.output_file_name != None: output_file = h5py.File(self.output_file_name, \"a\") group_out = create_group_hdf5(output_file, self.snap_group) write_output = True else: write_output = False print(dataset_name) snap_file = h5py.File(f\"{self.data_path}{self.snap_folder}.0.hdf5\", \"r\") Snap_data = snap_file[self.catalogue] try: data = Snap_data[dataset_name][:] except KeyError: print(f\"Variable not found in Snapshot files: {dataset_name}. Choose from \", Snap_data.keys()) if len(np.shape(data)) > 1: stack = True else: stack = False if write_output: try: dataset = group_out[dataset_name] del group_out[dataset_name] except: pass if stack: group_out.create_dataset(dataset_name, data=data, maxshape=(None, np.shape(data)[1]), chunks=True) else: group_out.create_dataset(dataset_name, data=data, maxshape=(None,), chunks=True) snap_file.close() for n in np.arange(1, self.N_files): snap_file = h5py.File(f\"{self.data_path}{self.snap_folder}.{n}.hdf5\", \"r\") try: Snap_data = snap_file[self.catalogue] data_n = Snap_data[dataset_name][:] # get data single file except KeyError: print(\"problem at file \", n) snap_file.close() continue if write_output: group_out[dataset_name].resize((group_out[dataset_name].shape[0] + data_n.shape[0]), axis=0) group_out[dataset_name][-data_n.shape[0]:] = data_n else: if stack: data = np.vstack((data, data_n)) else: data = np.append(data, data_n) snap_file.close() if write_output: output_file.close() return else: return data","title":"read_snapshot"},{"location":"api/ReadData/#measureia.ReadData.read_snapshot_multiple","text":"Read multiple datasets from the snapshot files for a specified shapshot number. Parameters: dataset_name ( list or str ) \u2013 The dataset names for the requested data Returns: ndarray \u2013 The requested datasets or nothing if output_file_name is specified Source code in src/measureia/read_data.py def read_snapshot_multiple(self, dataset_name): \"\"\"Read multiple datasets from the snapshot files for a specified shapshot number. Parameters ---------- dataset_name : list or str The dataset names for the requested data Returns ------- ndarray The requested datasets or nothing if output_file_name is specified \"\"\" if self.output_file_name != None: output_file = h5py.File(self.output_file_name, \"a\") group_out = create_group_hdf5(output_file, self.snap_group) write_output = True else: write_output = False snap_file = h5py.File(f\"{self.data_path}{self.snap_folder}.0.hdf5\", \"r\") Snap_data = snap_file[self.catalogue] stack = [] for i, variable in enumerate(dataset_name): try: data = Snap_data[dataset_name[i]][:] except KeyError: print(f\"Variable not found in Snapshot files {variable}. Choose from \", Snap_data.keys()) if len(np.shape(data)) > 1: stack.append(True) else: stack.append(False) if write_output: try: dataset = group_out[variable] del group_out[variable] except: pass if stack[i]: group_out.create_dataset(variable, data=data, maxshape=(None, np.shape(data)[1]), chunks=True) else: group_out.create_dataset(variable, data=data, maxshape=(None,), chunks=True) snap_file.close() for n in np.arange(1, self.N_files): snap_file = h5py.File(f\"{self.data_path}{self.snap_folder}.{n}.hdf5\", \"r\") for i, variable in enumerate(dataset_name): try: Snap_data = snap_file[self.catalogue] data_n = Snap_data[variable][:] # get data single file except KeyError: print(\"problem at file \", n) snap_file.close() continue if write_output: group_out[variable].resize((group_out[variable].shape[0] + data_n.shape[0]), axis=0) group_out[variable][-data_n.shape[0]:] = data_n else: if stack[i]: data = np.vstack((data, data_n)) else: data = np.append(data, data_n) snap_file.close() if write_output: output_file.close() return else: return data","title":"read_snapshot_multiple"},{"location":"api/ReadData/#measureia.ReadData.read_MeasureIA_output","text":"Fills in the available w_gg, w_gp, multipoles_gg, multipoles_gp, r, rp, and associated cov and errors attributes for a given dataset and num_jk from the output file of MeasureIA. Parameters: dataset_name \u2013 Name of the dataset in the output file of MeasureIA. num_jk \u2013 Number of jackknife patches to be generated internally. If None, the covariance will not be read. Source code in src/measureia/read_data.py def read_MeasureIA_output(self, dataset_name, num_jk): \"\"\" Fills in the available w_gg, w_gp, multipoles_gg, multipoles_gp, r, rp, and associated cov and errors attributes for a given dataset and num_jk from the output file of MeasureIA. Parameters ---------- dataset_name: str Name of the dataset in the output file of MeasureIA. num_jk: int or str or NoneType Number of jackknife patches to be generated internally. If None, the covariance will not be read. Returns ------- \"\"\" # reset parameters (if same object is used for multiple datasets) self.r = None self.rp = None self.w_gg = None self.w_gp = None self.multipoles_gg = None self.multipoles_gp = None self.cov_multipoles_gg = None self.errors_multipoles_gg = None self.cov_multipoles_gp = None self.errors_multipoles_gp = None self.cov_w_gg = None self.errors_w_gg = None self.cov_w_gp = None self.errors_w_gp = None file = h5py.File(f\"{self.data_path}{self.catalogue}.hdf5\", \"r\") if self.snap_group != \"\": data_group = file[self.snap_group] else: data_group = file try: self.multipoles_gg = data_group[f\"multipoles_gg/{dataset_name}\"][:] self.r = data_group[f\"multipoles_gg/{dataset_name}_r\"][:] if num_jk != None: self.cov_multipoles_gg = data_group[f\"multipoles_gg/{dataset_name}_jackknife_cov_{num_jk}\"][:] self.errors_multipoles_gg = data_group[f\"multipoles_gg/{dataset_name}_jackknife_{num_jk}\"][:] except KeyError: pass try: self.multipoles_gp = data_group[f\"multipoles_g_plus/{dataset_name}\"][:] self.r = data_group[f\"multipoles_g_plus/{dataset_name}_r\"][:] if num_jk != None: self.cov_multipoles_gp = data_group[f\"multipoles_g_plus/{dataset_name}_jackknife_cov_{num_jk}\"][:] self.errors_multipoles_gp = data_group[f\"multipoles_g_plus/{dataset_name}_jackknife_{num_jk}\"][:] except KeyError: pass try: self.w_gg = data_group[f\"w_gg/{dataset_name}\"][:] self.rp = data_group[f\"w_gg/{dataset_name}_rp\"][:] if num_jk != None: self.cov_w_gg = data_group[f\"w_gg/{dataset_name}_jackknife_cov_{num_jk}\"][:] self.errors_w_gg = data_group[f\"w_gg/{dataset_name}_jackknife_{num_jk}\"][:] except KeyError: pass try: self.w_gp = data_group[f\"w_g_plus/{dataset_name}\"][:] self.rp = data_group[f\"w_g_plus/{dataset_name}_rp\"][:] if num_jk != None: self.cov_w_gp = data_group[f\"w_g_plus/{dataset_name}_jackknife_cov_{num_jk}\"][:] self.errors_w_gp = data_group[f\"w_g_plus/{dataset_name}_jackknife_{num_jk}\"][:] except KeyError: pass file.close() return","title":"read_MeasureIA_output"},{"location":"api/SimInfo/","text":"SimInfo measureia.SimInfo Class that stores simulation information in an object to be inherited by other classes. Simulation information is hard coded and therefore uses are limited. However, can easily be expanded. Currently, these simulations are available: [TNG100, TNG100_2, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1, FLAMINGO_L2p8]. Attributes: simname ( str or NoneType ) \u2013 Identifier of the simulation, allowing for correct information to be obtained. snapshot ( int or str or NoneType ) \u2013 Number of the snapshot. snap_group ( str ) \u2013 Name of group in output file. Equal to 'Snapshot_[snapshot]' if snapshot is given, otherwise emtpy string. boxsize ( int or float or NoneType, default=None ) \u2013 Size of simulation box. If simname is in SimInfo, units are cMpc/h. Otherwise, manual input. L_0p5 ( int or float, default=None ) \u2013 Half of the boxsize. h ( float, default=None ) \u2013 Value of cosmological h parameter, for easy access to convert units. N_files ( int, default=None ) \u2013 Number of files of snapshot or subhalo data. Used in ReadData class. fof_folder ( str, default=None ) \u2013 Name of folder where fof files are saved (only available for TNG). snap_folder ( str, default=None ) \u2013 Name of folder where snapshot files are saved. Methods: Name Description get_specs Obtains the boxsize, L_0p5 and h parameters that are stored. get_file_info Creates N_files, fof_folder and snap_folder attributed needed by ReadData class. Source code in src/measureia/Sim_info.py class SimInfo: \"\"\"Class that stores simulation information in an object to be inherited by other classes. Simulation information is hard coded and therefore uses are limited. However, can easily be expanded. Currently, these simulations are available: [TNG100, TNG100_2, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1, FLAMINGO_L2p8]. Attributes ---------- simname : str or NoneType Identifier of the simulation, allowing for correct information to be obtained. snapshot : int or str or NoneType Number of the snapshot. snap_group : str Name of group in output file. Equal to 'Snapshot_[snapshot]' if snapshot is given, otherwise emtpy string. boxsize : int or float or NoneType, default=None Size of simulation box. If simname is in SimInfo, units are cMpc/h. Otherwise, manual input. L_0p5 : int or float, default=None Half of the boxsize. h : float, default=None Value of cosmological h parameter, for easy access to convert units. N_files : int, default=None Number of files of snapshot or subhalo data. Used in ReadData class. fof_folder : str, default=None Name of folder where fof files are saved (only available for TNG). snap_folder : str, default=None Name of folder where snapshot files are saved. Methods ------- get_specs() Obtains the boxsize, L_0p5 and h parameters that are stored. get_file_info() Creates N_files, fof_folder and snap_folder attributed needed by ReadData class. \"\"\" def __init__(self, sim_name, snapshot, boxsize=None, h=None, file_info=False): \"\"\" The __init__ method of SimInfo class. Creates all attributes and obtains information that is hardcoded in the class. Parameters ---------- sim_name : str or NoneType Identifier of the simulation, allowing for correct information to be obtained. Choose from [TNG100, TNG100_2, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1, FLAMINGO_L2p8]. If None, no information will be returned that is not already given as input. snapshot : int or str or NoneType Number of the snapshot, which, if given, will ensure that the output file to contains a group 'Snapshot_[snapshot]'. If None, the group is omitted from the output file structure. Default is None. boxsize : int or float, default=None Size of simulation box. Use if your simulation information is not provided by SimInfo. Make sure that the boxsize is in the same units as your position coordinates. h : float, default=None Value of cosmological h parameter, for easy access to convert units. file_info : bool, default=False If True, calls get_file_info method \"\"\" self.simname = sim_name self.N_files = None self.fof_folder = None self.snap_folder = None if snapshot is None: self.snapshot = None self.snap_group = \"\" else: self.snapshot = str(snapshot) self.snap_group = f\"Snapshot_{self.snapshot}/\" if type(sim_name) == str: self.get_specs() if file_info: self.get_file_info() else: self.boxsize = boxsize self.h = h if boxsize is None: self.L_0p5 = None else: self.L_0p5 = boxsize / 2. return def get_specs(self): \"\"\"Obtains the boxsize, L_0p5 and h parameters that are stored for [TNG100, TNG100_2, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1, FLAMINGO_L2p8]. Raises ------ KeyError If unknown simname is given. \"\"\" if self.simname == \"TNG100\": self.boxsize = 75.0 # cMpc/h self.L_0p5 = self.boxsize / 2.0 self.h = 0.6774 elif self.simname == \"TNG100_2\": self.boxsize = 75.0 # cMpc/h self.L_0p5 = self.boxsize / 2.0 self.h = 0.6774 elif self.simname == \"TNG300\": self.boxsize = 205.0 # cMpc/h self.L_0p5 = self.boxsize / 2.0 self.h = 0.6774 elif self.simname == \"EAGLE\": self.boxsize = 100.0 * 0.6777 # cMpc/h self.L_0p5 = self.boxsize / 2.0 self.h = 0.6777 elif self.simname == \"HorizonAGN\": self.boxsize = 100.0 # cMpc/h self.L_0p5 = self.boxsize / 2.0 self.h = 0.704 elif \"FLAMINGO\" in self.simname: if \"L1\" in self.simname: self.boxsize = 1000.0 * 0.681 # cMpc/h elif \"L2p8\" in self.simname: self.boxsize = 2800.0 * 0.681 # cMpc/h else: raise KeyError(\"Add an L1 or L2p8 suffix to your simname to specify which boxsize is used\") self.L_0p5 = self.boxsize / 2.0 self.h = 0.681 elif \"COLIBRE\" in self.simname: if \"L4\" in self.simname: self.boxsize = 400.0 * 0.681 # cMpc/h elif \"L2\" in self.simname: self.boxsize = 200.0 * 0.681 # cMpc/h else: raise KeyError(\"Add an L4 or L2 suffix to your simname to specify which boxsize is used\") self.L_0p5 = self.boxsize / 2.0 self.h = 0.681 else: raise KeyError( \"Simulation name not recognised. Choose from [TNG100, TNG100_2, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1, \" \"FLAMINGO_L2p8, COLIBRE_L400, COLIBRE_L200].\") return def get_file_info(self): \"\"\"Creates N_files, fof_folder and snap_folder attributed needed by ReadData class. \"\"\" if self.simname == \"TNG100\": self.fof_folder = f\"/fof_subhalo_tab_0{self.snapshot}/fof_subhalo_tab_0{self.snapshot}\" self.snap_folder = f\"/snap_0{self.snapshot}/snap_0{self.snapshot}\" self.N_files = 448 elif self.simname == \"TNG100_2\": self.fof_folder = f\"/fof_subhalo_tab_0{self.snapshot}/fof_subhalo_tab_0{self.snapshot}\" self.snap_folder = f\"/snap_0{self.snapshot}/snap_0{self.snapshot}\" self.N_files = 56 elif self.simname == \"TNG300\": self.fof_folder = f\"/fof_subhalo_tab_0{self.snapshot}/fof_subhalo_tab_0{self.snapshot}\" self.snap_folder = f\"/snap_0{self.snapshot}/snap_0{self.snapshot}\" self.N_files = 600 elif self.simname == \"EAGLE\": znames = {\"28\": \"z000p000\", \"17\": \"z001p487\", \"19\": \"z001p004\", \"21\": \"z000p736\", \"23\": \"z000p503\", \"25\": \"z000p271\"} zname = znames[self.snapshot] self.snap_folder = f\"/snap_0{self.snapshot}/RefL0100N1504/snapshot_0{self.snapshot}_{zname}/snap_0{self.snapshot}_{zname}\" # update for different z? self.fof_folder = None self.N_files = 256 elif self.simname == \"HorizonAGN\": self.fof_folder = None self.snap_folder = None self.N_files = 1. elif \"FLAMINGO\" in self.simname: self.fof_folder = None self.snap_folder = None self.N_files = 1 elif \"COLIBRE\" in self.simname: self.fof_folder = None self.snap_folder = None self.N_files = 1 else: raise KeyError( \"Simulation name not recognised. Choose from [TNG100, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1_m8, FLAMINGO_L1_m9, FLAMINGO_L1_m10, FLAMINGO_L2p8_m9,, COLIBRE_L400, COLIBRE_L200].\") return __init__(sim_name, snapshot, boxsize=None, h=None, file_info=False) The init method of SimInfo class. Creates all attributes and obtains information that is hardcoded in the class. Parameters: sim_name ( str or NoneType ) \u2013 Identifier of the simulation, allowing for correct information to be obtained. Choose from [TNG100, TNG100_2, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1, FLAMINGO_L2p8]. If None, no information will be returned that is not already given as input. snapshot ( int or str or NoneType ) \u2013 Number of the snapshot, which, if given, will ensure that the output file to contains a group 'Snapshot_[snapshot]'. If None, the group is omitted from the output file structure. Default is None. boxsize ( int or float , default: None ) \u2013 Size of simulation box. Use if your simulation information is not provided by SimInfo. Make sure that the boxsize is in the same units as your position coordinates. h ( float , default: None ) \u2013 Value of cosmological h parameter, for easy access to convert units. file_info ( bool , default: False ) \u2013 If True, calls get_file_info method Source code in src/measureia/Sim_info.py def __init__(self, sim_name, snapshot, boxsize=None, h=None, file_info=False): \"\"\" The __init__ method of SimInfo class. Creates all attributes and obtains information that is hardcoded in the class. Parameters ---------- sim_name : str or NoneType Identifier of the simulation, allowing for correct information to be obtained. Choose from [TNG100, TNG100_2, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1, FLAMINGO_L2p8]. If None, no information will be returned that is not already given as input. snapshot : int or str or NoneType Number of the snapshot, which, if given, will ensure that the output file to contains a group 'Snapshot_[snapshot]'. If None, the group is omitted from the output file structure. Default is None. boxsize : int or float, default=None Size of simulation box. Use if your simulation information is not provided by SimInfo. Make sure that the boxsize is in the same units as your position coordinates. h : float, default=None Value of cosmological h parameter, for easy access to convert units. file_info : bool, default=False If True, calls get_file_info method \"\"\" self.simname = sim_name self.N_files = None self.fof_folder = None self.snap_folder = None if snapshot is None: self.snapshot = None self.snap_group = \"\" else: self.snapshot = str(snapshot) self.snap_group = f\"Snapshot_{self.snapshot}/\" if type(sim_name) == str: self.get_specs() if file_info: self.get_file_info() else: self.boxsize = boxsize self.h = h if boxsize is None: self.L_0p5 = None else: self.L_0p5 = boxsize / 2. return get_specs() Obtains the boxsize, L_0p5 and h parameters that are stored for [TNG100, TNG100_2, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1, FLAMINGO_L2p8]. Raises: KeyError \u2013 If unknown simname is given. Source code in src/measureia/Sim_info.py def get_specs(self): \"\"\"Obtains the boxsize, L_0p5 and h parameters that are stored for [TNG100, TNG100_2, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1, FLAMINGO_L2p8]. Raises ------ KeyError If unknown simname is given. \"\"\" if self.simname == \"TNG100\": self.boxsize = 75.0 # cMpc/h self.L_0p5 = self.boxsize / 2.0 self.h = 0.6774 elif self.simname == \"TNG100_2\": self.boxsize = 75.0 # cMpc/h self.L_0p5 = self.boxsize / 2.0 self.h = 0.6774 elif self.simname == \"TNG300\": self.boxsize = 205.0 # cMpc/h self.L_0p5 = self.boxsize / 2.0 self.h = 0.6774 elif self.simname == \"EAGLE\": self.boxsize = 100.0 * 0.6777 # cMpc/h self.L_0p5 = self.boxsize / 2.0 self.h = 0.6777 elif self.simname == \"HorizonAGN\": self.boxsize = 100.0 # cMpc/h self.L_0p5 = self.boxsize / 2.0 self.h = 0.704 elif \"FLAMINGO\" in self.simname: if \"L1\" in self.simname: self.boxsize = 1000.0 * 0.681 # cMpc/h elif \"L2p8\" in self.simname: self.boxsize = 2800.0 * 0.681 # cMpc/h else: raise KeyError(\"Add an L1 or L2p8 suffix to your simname to specify which boxsize is used\") self.L_0p5 = self.boxsize / 2.0 self.h = 0.681 elif \"COLIBRE\" in self.simname: if \"L4\" in self.simname: self.boxsize = 400.0 * 0.681 # cMpc/h elif \"L2\" in self.simname: self.boxsize = 200.0 * 0.681 # cMpc/h else: raise KeyError(\"Add an L4 or L2 suffix to your simname to specify which boxsize is used\") self.L_0p5 = self.boxsize / 2.0 self.h = 0.681 else: raise KeyError( \"Simulation name not recognised. Choose from [TNG100, TNG100_2, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1, \" \"FLAMINGO_L2p8, COLIBRE_L400, COLIBRE_L200].\") return get_file_info() Creates N_files, fof_folder and snap_folder attributed needed by ReadData class. Source code in src/measureia/Sim_info.py def get_file_info(self): \"\"\"Creates N_files, fof_folder and snap_folder attributed needed by ReadData class. \"\"\" if self.simname == \"TNG100\": self.fof_folder = f\"/fof_subhalo_tab_0{self.snapshot}/fof_subhalo_tab_0{self.snapshot}\" self.snap_folder = f\"/snap_0{self.snapshot}/snap_0{self.snapshot}\" self.N_files = 448 elif self.simname == \"TNG100_2\": self.fof_folder = f\"/fof_subhalo_tab_0{self.snapshot}/fof_subhalo_tab_0{self.snapshot}\" self.snap_folder = f\"/snap_0{self.snapshot}/snap_0{self.snapshot}\" self.N_files = 56 elif self.simname == \"TNG300\": self.fof_folder = f\"/fof_subhalo_tab_0{self.snapshot}/fof_subhalo_tab_0{self.snapshot}\" self.snap_folder = f\"/snap_0{self.snapshot}/snap_0{self.snapshot}\" self.N_files = 600 elif self.simname == \"EAGLE\": znames = {\"28\": \"z000p000\", \"17\": \"z001p487\", \"19\": \"z001p004\", \"21\": \"z000p736\", \"23\": \"z000p503\", \"25\": \"z000p271\"} zname = znames[self.snapshot] self.snap_folder = f\"/snap_0{self.snapshot}/RefL0100N1504/snapshot_0{self.snapshot}_{zname}/snap_0{self.snapshot}_{zname}\" # update for different z? self.fof_folder = None self.N_files = 256 elif self.simname == \"HorizonAGN\": self.fof_folder = None self.snap_folder = None self.N_files = 1. elif \"FLAMINGO\" in self.simname: self.fof_folder = None self.snap_folder = None self.N_files = 1 elif \"COLIBRE\" in self.simname: self.fof_folder = None self.snap_folder = None self.N_files = 1 else: raise KeyError( \"Simulation name not recognised. Choose from [TNG100, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1_m8, FLAMINGO_L1_m9, FLAMINGO_L1_m10, FLAMINGO_L2p8_m9,, COLIBRE_L400, COLIBRE_L200].\") return","title":"SimInfo"},{"location":"api/SimInfo/#siminfo","text":"","title":"SimInfo"},{"location":"api/SimInfo/#measureia.SimInfo","text":"Class that stores simulation information in an object to be inherited by other classes. Simulation information is hard coded and therefore uses are limited. However, can easily be expanded. Currently, these simulations are available: [TNG100, TNG100_2, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1, FLAMINGO_L2p8]. Attributes: simname ( str or NoneType ) \u2013 Identifier of the simulation, allowing for correct information to be obtained. snapshot ( int or str or NoneType ) \u2013 Number of the snapshot. snap_group ( str ) \u2013 Name of group in output file. Equal to 'Snapshot_[snapshot]' if snapshot is given, otherwise emtpy string. boxsize ( int or float or NoneType, default=None ) \u2013 Size of simulation box. If simname is in SimInfo, units are cMpc/h. Otherwise, manual input. L_0p5 ( int or float, default=None ) \u2013 Half of the boxsize. h ( float, default=None ) \u2013 Value of cosmological h parameter, for easy access to convert units. N_files ( int, default=None ) \u2013 Number of files of snapshot or subhalo data. Used in ReadData class. fof_folder ( str, default=None ) \u2013 Name of folder where fof files are saved (only available for TNG). snap_folder ( str, default=None ) \u2013 Name of folder where snapshot files are saved. Methods: Name Description get_specs Obtains the boxsize, L_0p5 and h parameters that are stored. get_file_info Creates N_files, fof_folder and snap_folder attributed needed by ReadData class. Source code in src/measureia/Sim_info.py class SimInfo: \"\"\"Class that stores simulation information in an object to be inherited by other classes. Simulation information is hard coded and therefore uses are limited. However, can easily be expanded. Currently, these simulations are available: [TNG100, TNG100_2, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1, FLAMINGO_L2p8]. Attributes ---------- simname : str or NoneType Identifier of the simulation, allowing for correct information to be obtained. snapshot : int or str or NoneType Number of the snapshot. snap_group : str Name of group in output file. Equal to 'Snapshot_[snapshot]' if snapshot is given, otherwise emtpy string. boxsize : int or float or NoneType, default=None Size of simulation box. If simname is in SimInfo, units are cMpc/h. Otherwise, manual input. L_0p5 : int or float, default=None Half of the boxsize. h : float, default=None Value of cosmological h parameter, for easy access to convert units. N_files : int, default=None Number of files of snapshot or subhalo data. Used in ReadData class. fof_folder : str, default=None Name of folder where fof files are saved (only available for TNG). snap_folder : str, default=None Name of folder where snapshot files are saved. Methods ------- get_specs() Obtains the boxsize, L_0p5 and h parameters that are stored. get_file_info() Creates N_files, fof_folder and snap_folder attributed needed by ReadData class. \"\"\" def __init__(self, sim_name, snapshot, boxsize=None, h=None, file_info=False): \"\"\" The __init__ method of SimInfo class. Creates all attributes and obtains information that is hardcoded in the class. Parameters ---------- sim_name : str or NoneType Identifier of the simulation, allowing for correct information to be obtained. Choose from [TNG100, TNG100_2, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1, FLAMINGO_L2p8]. If None, no information will be returned that is not already given as input. snapshot : int or str or NoneType Number of the snapshot, which, if given, will ensure that the output file to contains a group 'Snapshot_[snapshot]'. If None, the group is omitted from the output file structure. Default is None. boxsize : int or float, default=None Size of simulation box. Use if your simulation information is not provided by SimInfo. Make sure that the boxsize is in the same units as your position coordinates. h : float, default=None Value of cosmological h parameter, for easy access to convert units. file_info : bool, default=False If True, calls get_file_info method \"\"\" self.simname = sim_name self.N_files = None self.fof_folder = None self.snap_folder = None if snapshot is None: self.snapshot = None self.snap_group = \"\" else: self.snapshot = str(snapshot) self.snap_group = f\"Snapshot_{self.snapshot}/\" if type(sim_name) == str: self.get_specs() if file_info: self.get_file_info() else: self.boxsize = boxsize self.h = h if boxsize is None: self.L_0p5 = None else: self.L_0p5 = boxsize / 2. return def get_specs(self): \"\"\"Obtains the boxsize, L_0p5 and h parameters that are stored for [TNG100, TNG100_2, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1, FLAMINGO_L2p8]. Raises ------ KeyError If unknown simname is given. \"\"\" if self.simname == \"TNG100\": self.boxsize = 75.0 # cMpc/h self.L_0p5 = self.boxsize / 2.0 self.h = 0.6774 elif self.simname == \"TNG100_2\": self.boxsize = 75.0 # cMpc/h self.L_0p5 = self.boxsize / 2.0 self.h = 0.6774 elif self.simname == \"TNG300\": self.boxsize = 205.0 # cMpc/h self.L_0p5 = self.boxsize / 2.0 self.h = 0.6774 elif self.simname == \"EAGLE\": self.boxsize = 100.0 * 0.6777 # cMpc/h self.L_0p5 = self.boxsize / 2.0 self.h = 0.6777 elif self.simname == \"HorizonAGN\": self.boxsize = 100.0 # cMpc/h self.L_0p5 = self.boxsize / 2.0 self.h = 0.704 elif \"FLAMINGO\" in self.simname: if \"L1\" in self.simname: self.boxsize = 1000.0 * 0.681 # cMpc/h elif \"L2p8\" in self.simname: self.boxsize = 2800.0 * 0.681 # cMpc/h else: raise KeyError(\"Add an L1 or L2p8 suffix to your simname to specify which boxsize is used\") self.L_0p5 = self.boxsize / 2.0 self.h = 0.681 elif \"COLIBRE\" in self.simname: if \"L4\" in self.simname: self.boxsize = 400.0 * 0.681 # cMpc/h elif \"L2\" in self.simname: self.boxsize = 200.0 * 0.681 # cMpc/h else: raise KeyError(\"Add an L4 or L2 suffix to your simname to specify which boxsize is used\") self.L_0p5 = self.boxsize / 2.0 self.h = 0.681 else: raise KeyError( \"Simulation name not recognised. Choose from [TNG100, TNG100_2, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1, \" \"FLAMINGO_L2p8, COLIBRE_L400, COLIBRE_L200].\") return def get_file_info(self): \"\"\"Creates N_files, fof_folder and snap_folder attributed needed by ReadData class. \"\"\" if self.simname == \"TNG100\": self.fof_folder = f\"/fof_subhalo_tab_0{self.snapshot}/fof_subhalo_tab_0{self.snapshot}\" self.snap_folder = f\"/snap_0{self.snapshot}/snap_0{self.snapshot}\" self.N_files = 448 elif self.simname == \"TNG100_2\": self.fof_folder = f\"/fof_subhalo_tab_0{self.snapshot}/fof_subhalo_tab_0{self.snapshot}\" self.snap_folder = f\"/snap_0{self.snapshot}/snap_0{self.snapshot}\" self.N_files = 56 elif self.simname == \"TNG300\": self.fof_folder = f\"/fof_subhalo_tab_0{self.snapshot}/fof_subhalo_tab_0{self.snapshot}\" self.snap_folder = f\"/snap_0{self.snapshot}/snap_0{self.snapshot}\" self.N_files = 600 elif self.simname == \"EAGLE\": znames = {\"28\": \"z000p000\", \"17\": \"z001p487\", \"19\": \"z001p004\", \"21\": \"z000p736\", \"23\": \"z000p503\", \"25\": \"z000p271\"} zname = znames[self.snapshot] self.snap_folder = f\"/snap_0{self.snapshot}/RefL0100N1504/snapshot_0{self.snapshot}_{zname}/snap_0{self.snapshot}_{zname}\" # update for different z? self.fof_folder = None self.N_files = 256 elif self.simname == \"HorizonAGN\": self.fof_folder = None self.snap_folder = None self.N_files = 1. elif \"FLAMINGO\" in self.simname: self.fof_folder = None self.snap_folder = None self.N_files = 1 elif \"COLIBRE\" in self.simname: self.fof_folder = None self.snap_folder = None self.N_files = 1 else: raise KeyError( \"Simulation name not recognised. Choose from [TNG100, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1_m8, FLAMINGO_L1_m9, FLAMINGO_L1_m10, FLAMINGO_L2p8_m9,, COLIBRE_L400, COLIBRE_L200].\") return","title":"SimInfo"},{"location":"api/SimInfo/#measureia.SimInfo.__init__","text":"The init method of SimInfo class. Creates all attributes and obtains information that is hardcoded in the class. Parameters: sim_name ( str or NoneType ) \u2013 Identifier of the simulation, allowing for correct information to be obtained. Choose from [TNG100, TNG100_2, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1, FLAMINGO_L2p8]. If None, no information will be returned that is not already given as input. snapshot ( int or str or NoneType ) \u2013 Number of the snapshot, which, if given, will ensure that the output file to contains a group 'Snapshot_[snapshot]'. If None, the group is omitted from the output file structure. Default is None. boxsize ( int or float , default: None ) \u2013 Size of simulation box. Use if your simulation information is not provided by SimInfo. Make sure that the boxsize is in the same units as your position coordinates. h ( float , default: None ) \u2013 Value of cosmological h parameter, for easy access to convert units. file_info ( bool , default: False ) \u2013 If True, calls get_file_info method Source code in src/measureia/Sim_info.py def __init__(self, sim_name, snapshot, boxsize=None, h=None, file_info=False): \"\"\" The __init__ method of SimInfo class. Creates all attributes and obtains information that is hardcoded in the class. Parameters ---------- sim_name : str or NoneType Identifier of the simulation, allowing for correct information to be obtained. Choose from [TNG100, TNG100_2, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1, FLAMINGO_L2p8]. If None, no information will be returned that is not already given as input. snapshot : int or str or NoneType Number of the snapshot, which, if given, will ensure that the output file to contains a group 'Snapshot_[snapshot]'. If None, the group is omitted from the output file structure. Default is None. boxsize : int or float, default=None Size of simulation box. Use if your simulation information is not provided by SimInfo. Make sure that the boxsize is in the same units as your position coordinates. h : float, default=None Value of cosmological h parameter, for easy access to convert units. file_info : bool, default=False If True, calls get_file_info method \"\"\" self.simname = sim_name self.N_files = None self.fof_folder = None self.snap_folder = None if snapshot is None: self.snapshot = None self.snap_group = \"\" else: self.snapshot = str(snapshot) self.snap_group = f\"Snapshot_{self.snapshot}/\" if type(sim_name) == str: self.get_specs() if file_info: self.get_file_info() else: self.boxsize = boxsize self.h = h if boxsize is None: self.L_0p5 = None else: self.L_0p5 = boxsize / 2. return","title":"__init__"},{"location":"api/SimInfo/#measureia.SimInfo.get_specs","text":"Obtains the boxsize, L_0p5 and h parameters that are stored for [TNG100, TNG100_2, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1, FLAMINGO_L2p8]. Raises: KeyError \u2013 If unknown simname is given. Source code in src/measureia/Sim_info.py def get_specs(self): \"\"\"Obtains the boxsize, L_0p5 and h parameters that are stored for [TNG100, TNG100_2, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1, FLAMINGO_L2p8]. Raises ------ KeyError If unknown simname is given. \"\"\" if self.simname == \"TNG100\": self.boxsize = 75.0 # cMpc/h self.L_0p5 = self.boxsize / 2.0 self.h = 0.6774 elif self.simname == \"TNG100_2\": self.boxsize = 75.0 # cMpc/h self.L_0p5 = self.boxsize / 2.0 self.h = 0.6774 elif self.simname == \"TNG300\": self.boxsize = 205.0 # cMpc/h self.L_0p5 = self.boxsize / 2.0 self.h = 0.6774 elif self.simname == \"EAGLE\": self.boxsize = 100.0 * 0.6777 # cMpc/h self.L_0p5 = self.boxsize / 2.0 self.h = 0.6777 elif self.simname == \"HorizonAGN\": self.boxsize = 100.0 # cMpc/h self.L_0p5 = self.boxsize / 2.0 self.h = 0.704 elif \"FLAMINGO\" in self.simname: if \"L1\" in self.simname: self.boxsize = 1000.0 * 0.681 # cMpc/h elif \"L2p8\" in self.simname: self.boxsize = 2800.0 * 0.681 # cMpc/h else: raise KeyError(\"Add an L1 or L2p8 suffix to your simname to specify which boxsize is used\") self.L_0p5 = self.boxsize / 2.0 self.h = 0.681 elif \"COLIBRE\" in self.simname: if \"L4\" in self.simname: self.boxsize = 400.0 * 0.681 # cMpc/h elif \"L2\" in self.simname: self.boxsize = 200.0 * 0.681 # cMpc/h else: raise KeyError(\"Add an L4 or L2 suffix to your simname to specify which boxsize is used\") self.L_0p5 = self.boxsize / 2.0 self.h = 0.681 else: raise KeyError( \"Simulation name not recognised. Choose from [TNG100, TNG100_2, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1, \" \"FLAMINGO_L2p8, COLIBRE_L400, COLIBRE_L200].\") return","title":"get_specs"},{"location":"api/SimInfo/#measureia.SimInfo.get_file_info","text":"Creates N_files, fof_folder and snap_folder attributed needed by ReadData class. Source code in src/measureia/Sim_info.py def get_file_info(self): \"\"\"Creates N_files, fof_folder and snap_folder attributed needed by ReadData class. \"\"\" if self.simname == \"TNG100\": self.fof_folder = f\"/fof_subhalo_tab_0{self.snapshot}/fof_subhalo_tab_0{self.snapshot}\" self.snap_folder = f\"/snap_0{self.snapshot}/snap_0{self.snapshot}\" self.N_files = 448 elif self.simname == \"TNG100_2\": self.fof_folder = f\"/fof_subhalo_tab_0{self.snapshot}/fof_subhalo_tab_0{self.snapshot}\" self.snap_folder = f\"/snap_0{self.snapshot}/snap_0{self.snapshot}\" self.N_files = 56 elif self.simname == \"TNG300\": self.fof_folder = f\"/fof_subhalo_tab_0{self.snapshot}/fof_subhalo_tab_0{self.snapshot}\" self.snap_folder = f\"/snap_0{self.snapshot}/snap_0{self.snapshot}\" self.N_files = 600 elif self.simname == \"EAGLE\": znames = {\"28\": \"z000p000\", \"17\": \"z001p487\", \"19\": \"z001p004\", \"21\": \"z000p736\", \"23\": \"z000p503\", \"25\": \"z000p271\"} zname = znames[self.snapshot] self.snap_folder = f\"/snap_0{self.snapshot}/RefL0100N1504/snapshot_0{self.snapshot}_{zname}/snap_0{self.snapshot}_{zname}\" # update for different z? self.fof_folder = None self.N_files = 256 elif self.simname == \"HorizonAGN\": self.fof_folder = None self.snap_folder = None self.N_files = 1. elif \"FLAMINGO\" in self.simname: self.fof_folder = None self.snap_folder = None self.N_files = 1 elif \"COLIBRE\" in self.simname: self.fof_folder = None self.snap_folder = None self.N_files = 1 else: raise KeyError( \"Simulation name not recognised. Choose from [TNG100, TNG300, EAGLE, HorizonAGN, FLAMINGO_L1_m8, FLAMINGO_L1_m9, FLAMINGO_L1_m10, FLAMINGO_L2p8_m9,, COLIBRE_L400, COLIBRE_L200].\") return","title":"get_file_info"},{"location":"api/measureIABox/","text":"MeasureIA measureia.MeasureIABox Bases: MeasureWBox , MeasureMultipolesBox , MeasureWBoxJackknife , MeasureMBoxJackknife Manages the IA correlation function measurement methods used in the MeasureIA package based on speed and input. This class is used to call the methods that measure \\(w_{gg}\\) , \\(w_{g+}\\) and multipoles for simulations in cartesian coordinates. Depending on the input parameters, various correlations incl covariance estimates are measured for given data. Methods: Name Description measure_xi_w Compute projected correlations \\(w_{gg}\\) and/or \\(w_{g+}\\) . measure_xi_multipoles Compute multipoles of the correlation functions, \\(\\tilde{\\xi}_{gg,0}\\) and/or \\(\\tilde{\\xi}_{g+,2}\\) . Notes Inherits attributes from 'SimInfo', where 'boxsize', 'L_0p5' and 'snap_group' are used in this class. Inherits attributes from 'MeasureIABase', where 'data', 'output_file_name', 'periodicity', 'Num_position', 'Num_shape', 'r_min', 'r_max', 'num_bins_r', 'num_bins_pi', 'r_bins', 'pi_bins', 'mu_r_bins' are used. Source code in src/measureia/measure_IA.py class MeasureIABox(MeasureWBox, MeasureMultipolesBox, MeasureWBoxJackknife, MeasureMBoxJackknife): r\"\"\"Manages the IA correlation function measurement methods used in the MeasureIA package based on speed and input. This class is used to call the methods that measure $w_{gg}$, $w_{g+}$ and multipoles for simulations in cartesian coordinates. Depending on the input parameters, various correlations incl covariance estimates are measured for given data. Methods ------- measure_xi_w() Compute projected correlations $w_{gg}$ and/or $w_{g+}$. measure_xi_multipoles() Compute multipoles of the correlation functions, $\\tilde{\\xi}_{gg,0}$ and/or $\\tilde{\\xi}_{g+,2}$. Notes ----- Inherits attributes from 'SimInfo', where 'boxsize', 'L_0p5' and 'snap_group' are used in this class. Inherits attributes from 'MeasureIABase', where 'data', 'output_file_name', 'periodicity', 'Num_position', 'Num_shape', 'r_min', 'r_max', 'num_bins_r', 'num_bins_pi', 'r_bins', 'pi_bins', 'mu_r_bins' are used. \"\"\" def __init__( self, data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True, num_nodes=1, ): \"\"\" The __init__ method of the MeasureIABox class. Parameters ---------- num_nodes : int, optional Number of cores to be used in multiprocessing. Default is 1. Notes ----- Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. \"\"\" super().__init__(data, output_file_name, simulation, snapshot, separation_limits, num_bins_r, num_bins_pi, pi_max, boxsize, periodicity) self.num_nodes = num_nodes self.randoms_data = None self.data_dir = None self.num_samples = None return def measure_xi_w(self, dataset_name, corr_type, num_jk=0, temp_file_path=None, masks=None, ellipticity='distortion', chunk_size=1000): r\"\"\"Measures $\\xi_{gg}$, $\\xi_{g+}$ and $w_{gg}$, $w_{g+}$ including jackknife covariance if desired. Manages the various _measure_xi_rp_pi_box method options in MeasureWBox and MeasureWBoxJackknife. Parameters ---------- dataset_name : str Name of the dataset in the output file. corr_type : str Type of correlation to be measured. Choose from [g+, gg, both]. num_jk : int, optional Number of jackknife regions (needs to be x^3, with x an int) for the covariance measurement. Default is 0 (no covariance). temp_file_path : str or NoneType, optional Path to where the data is temporarily stored [file name generated automatically]. masks : dict or NoneType, optional Directory of mask information in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. chunk_size: int, optional Size of the chunks of data sent to each multiprocessing node. If larger, more RAM is needed per node. Default is 1000. ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. \"\"\" if num_jk > 0: try: assert sympy.integer_nthroot(num_jk, 3)[1] L = sympy.integer_nthroot(num_jk, 3)[0] except AssertionError: raise ValueError( f\"Use x^3 as input for num_jk, with x as an int. {float(int(num_jk ** (1. / 3)))},{num_jk ** (1. / 3)}\") if temp_file_path == False: temp_storage = False temp_file_path = None else: temp_storage = True if temp_storage and temp_file_path == None: raise ValueError( \"Input temp_file_path for faster computation. Do not want to save data temporarily? Input file_path_tree=False.\") # replace by better checks of input data try: RA = self.data[\"RA\"] print(\"Given data is lightcone, use measure_xi_w_lightcone method instead.\") exit() except: pass if num_jk > 0: # include covariance if self.num_nodes > 1 and temp_storage: self._measure_xi_rp_pi_box_jk_multiprocessing(masks=masks, L_subboxes=L, dataset_name=dataset_name, return_output=False, num_nodes=self.num_nodes, jk_group_name=f\"{dataset_name}_jk{num_jk}\", chunk_size=chunk_size, ellipticity=ellipticity, temp_file_path=temp_file_path) elif temp_storage: self._measure_xi_rp_pi_box_jk_tree(masks=masks, L_subboxes=L, dataset_name=dataset_name, return_output=False, ellipticity=ellipticity, jk_group_name=f\"{dataset_name}_jk{num_jk}\") else: self._measure_xi_rp_pi_box_jk_brute(masks=masks, L_subboxes=L, dataset_name=dataset_name, return_output=False, ellipticity=ellipticity, jk_group_name=f\"{dataset_name}_jk{num_jk}\") self._measure_w_g_i(corr_type=corr_type, dataset_name=dataset_name, return_output=False) for i in np.arange(num_jk): self._measure_w_g_i(corr_type=corr_type, dataset_name=f\"{dataset_name}_{i}\", jk_group_name=f\"{dataset_name}_jk{num_jk}\", return_output=False) if corr_type == \"both\": corr_group = [\"w_g_plus\", \"w_gg\"] elif corr_type == \"g+\": corr_group = [\"w_g_plus\"] elif corr_type == \"gg\": corr_group = [\"w_gg\"] else: raise KeyError(\"Unknown value for corr_type. Choose from [g+, gg, both]\") self._combine_jackknife_information(dataset_name=dataset_name, jk_group_name=f\"{dataset_name}_jk{num_jk}\", corr_group=corr_group, num_box=num_jk) else: # no covariance if self.num_nodes > 1 and temp_storage: self._measure_xi_rp_pi_box_multiprocessing(dataset_name=dataset_name, temp_file_path=temp_file_path, masks=masks, return_output=False, num_nodes=self.num_nodes, chunk_size=chunk_size, ellipticity=ellipticity) elif temp_storage: self._measure_xi_rp_pi_box_tree(masks=masks, dataset_name=dataset_name, return_output=False, ellipticity=ellipticity) else: self._measure_xi_rp_pi_box_brute(masks=masks, dataset_name=dataset_name, return_output=False, ellipticity=ellipticity) self._measure_w_g_i(corr_type=corr_type, dataset_name=dataset_name, return_output=False) return def measure_xi_multipoles(self, dataset_name, corr_type, num_jk=0, temp_file_path=None, masks=None, rp_cut=None, ellipticity='distortion', chunk_size=1000): r\"\"\"Measures $\\xi_{gg}$, $\\xi_{g+}$ and $\\tilde{\\xi}_{gg,0}$, $\\tilde{\\xi}_{g+,2}$ including jackknife covariance if desired. Manages the various _measure_xi_r_mur_box method options in MeasureMultipolesBox and MeasureMultipolesBoxJackknife. Parameters ---------- dataset_name : str Name of the dataset in the output file. corr_type : str Type of correlation to be measured. Choose from [g+, gg, both]. num_jk : int, optional Number of jackknife regions (needs to be x^3, with x an int) for the covariance measurement. Default is 0 (no covariance). temp_file_path : str or NoneType, optional Path to where the data is temporarily stored [file name generated automatically]. masks : dict or NoneType, optional Directory of mask information in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. rp_cut : float or NoneType, optional Applies a minimum r_p value condition for pairs to be included. Default is None. chunck_size: int, optional Size of the chunks of data sent to each multiprocessing node. If larger, more RAM is needed per node. ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. \"\"\" if num_jk > 0: try: assert sympy.integer_nthroot(num_jk, 3)[1] L = sympy.integer_nthroot(num_jk, 3)[0] except AssertionError: raise ValueError( f\"Use x^3 as input for num_jk, with x as an int. {float(int(num_jk ** (1. / 3)))},{num_jk ** (1. / 3)}\") if temp_file_path == False: temp_storage = False temp_file_path = None else: temp_storage = True if temp_storage and temp_file_path == None: raise ValueError( \"Input temp_file_path for faster computation. Do not want to save data temporarily? Input file_path_tree=False.\") # replace by better checks of input data try: RA = self.data[\"RA\"] print(\"Given data is lightcone, use measure_xi_w_lightcone method instead.\") exit() except: pass if num_jk > 0: # include covariance if self.num_nodes > 1 and temp_storage: self._measure_xi_r_mur_box_jk_multiprocessing(masks=masks, L_subboxes=L, dataset_name=dataset_name, return_output=False, num_nodes=self.num_nodes, jk_group_name=f\"{dataset_name}_jk{num_jk}\", chunk_size=chunk_size, ellipticity=ellipticity, file_tree_path=temp_file_path) elif temp_storage: self._measure_xi_r_mur_box_jk_tree(masks=masks, L_subboxes=L, dataset_name=dataset_name, return_output=False, ellipticity=ellipticity, jk_group_name=f\"{dataset_name}_jk{num_jk}\") else: self._measure_xi_r_mur_box_jk_brute(masks=masks, L_subboxes=L, dataset_name=dataset_name, return_output=False, ellipticity=ellipticity, jk_group_name=f\"{dataset_name}_jk{num_jk}\") self._measure_multipoles(corr_type=corr_type, dataset_name=dataset_name, return_output=False) for i in np.arange(num_jk): self._measure_multipoles(corr_type=corr_type, dataset_name=f\"{dataset_name}_{i}\", jk_group_name=f\"{dataset_name}_jk{num_jk}\", return_output=False) if corr_type == \"both\": corr_group = [\"multipoles_g_plus\", \"multipoles_gg\"] elif corr_type == \"g+\": corr_group = [\"multipoles_g_plus\"] elif corr_type == \"gg\": corr_group = [\"multipoles_gg\"] else: raise KeyError(\"Unknown value for corr_type. Choose from [g+, gg, both]\") self._combine_jackknife_information(dataset_name=dataset_name, jk_group_name=f\"{dataset_name}_jk{num_jk}\", corr_group=corr_group, num_box=num_jk) else: # no covariance if self.num_nodes > 1 and temp_storage: self._measure_xi_r_mur_box_multiprocessing(dataset_name=dataset_name, temp_file_path=temp_file_path, masks=masks, return_output=False, num_nodes=self.num_nodes, chunk_size=chunk_size, ellipticity=ellipticity) elif temp_storage: self._measure_xi_r_mur_box_tree(masks=masks, dataset_name=dataset_name, return_output=False, ellipticity=ellipticity) else: self._measure_xi_r_mur_box_brute(masks=masks, dataset_name=dataset_name, return_output=False, ellipticity=ellipticity) self._measure_multipoles(corr_type=corr_type, dataset_name=dataset_name, return_output=False) return __init__(data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True, num_nodes=1) The init method of the MeasureIABox class. Parameters: num_nodes ( int , default: 1 ) \u2013 Number of cores to be used in multiprocessing. Default is 1. Notes Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. Source code in src/measureia/measure_IA.py def __init__( self, data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True, num_nodes=1, ): \"\"\" The __init__ method of the MeasureIABox class. Parameters ---------- num_nodes : int, optional Number of cores to be used in multiprocessing. Default is 1. Notes ----- Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. \"\"\" super().__init__(data, output_file_name, simulation, snapshot, separation_limits, num_bins_r, num_bins_pi, pi_max, boxsize, periodicity) self.num_nodes = num_nodes self.randoms_data = None self.data_dir = None self.num_samples = None return measure_xi_w(dataset_name, corr_type, num_jk=0, temp_file_path=None, masks=None, ellipticity='distortion', chunk_size=1000) Measures \\(\\xi_{gg}\\) , \\(\\xi_{g+}\\) and \\(w_{gg}\\) , \\(w_{g+}\\) including jackknife covariance if desired. Manages the various _measure_xi_rp_pi_box method options in MeasureWBox and MeasureWBoxJackknife. Parameters: dataset_name ( str ) \u2013 Name of the dataset in the output file. corr_type ( str ) \u2013 Type of correlation to be measured. Choose from [g+, gg, both]. num_jk ( int , default: 0 ) \u2013 Number of jackknife regions (needs to be x^3, with x an int) for the covariance measurement. Default is 0 (no covariance). temp_file_path ( str or NoneType , default: None ) \u2013 Path to where the data is temporarily stored [file name generated automatically]. masks ( dict or NoneType , default: None ) \u2013 Directory of mask information in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. chunk_size \u2013 Size of the chunks of data sent to each multiprocessing node. If larger, more RAM is needed per node. Default is 1000. ellipticity ( str , default: 'distortion' ) \u2013 Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. Source code in src/measureia/measure_IA.py def measure_xi_w(self, dataset_name, corr_type, num_jk=0, temp_file_path=None, masks=None, ellipticity='distortion', chunk_size=1000): r\"\"\"Measures $\\xi_{gg}$, $\\xi_{g+}$ and $w_{gg}$, $w_{g+}$ including jackknife covariance if desired. Manages the various _measure_xi_rp_pi_box method options in MeasureWBox and MeasureWBoxJackknife. Parameters ---------- dataset_name : str Name of the dataset in the output file. corr_type : str Type of correlation to be measured. Choose from [g+, gg, both]. num_jk : int, optional Number of jackknife regions (needs to be x^3, with x an int) for the covariance measurement. Default is 0 (no covariance). temp_file_path : str or NoneType, optional Path to where the data is temporarily stored [file name generated automatically]. masks : dict or NoneType, optional Directory of mask information in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. chunk_size: int, optional Size of the chunks of data sent to each multiprocessing node. If larger, more RAM is needed per node. Default is 1000. ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. \"\"\" if num_jk > 0: try: assert sympy.integer_nthroot(num_jk, 3)[1] L = sympy.integer_nthroot(num_jk, 3)[0] except AssertionError: raise ValueError( f\"Use x^3 as input for num_jk, with x as an int. {float(int(num_jk ** (1. / 3)))},{num_jk ** (1. / 3)}\") if temp_file_path == False: temp_storage = False temp_file_path = None else: temp_storage = True if temp_storage and temp_file_path == None: raise ValueError( \"Input temp_file_path for faster computation. Do not want to save data temporarily? Input file_path_tree=False.\") # replace by better checks of input data try: RA = self.data[\"RA\"] print(\"Given data is lightcone, use measure_xi_w_lightcone method instead.\") exit() except: pass if num_jk > 0: # include covariance if self.num_nodes > 1 and temp_storage: self._measure_xi_rp_pi_box_jk_multiprocessing(masks=masks, L_subboxes=L, dataset_name=dataset_name, return_output=False, num_nodes=self.num_nodes, jk_group_name=f\"{dataset_name}_jk{num_jk}\", chunk_size=chunk_size, ellipticity=ellipticity, temp_file_path=temp_file_path) elif temp_storage: self._measure_xi_rp_pi_box_jk_tree(masks=masks, L_subboxes=L, dataset_name=dataset_name, return_output=False, ellipticity=ellipticity, jk_group_name=f\"{dataset_name}_jk{num_jk}\") else: self._measure_xi_rp_pi_box_jk_brute(masks=masks, L_subboxes=L, dataset_name=dataset_name, return_output=False, ellipticity=ellipticity, jk_group_name=f\"{dataset_name}_jk{num_jk}\") self._measure_w_g_i(corr_type=corr_type, dataset_name=dataset_name, return_output=False) for i in np.arange(num_jk): self._measure_w_g_i(corr_type=corr_type, dataset_name=f\"{dataset_name}_{i}\", jk_group_name=f\"{dataset_name}_jk{num_jk}\", return_output=False) if corr_type == \"both\": corr_group = [\"w_g_plus\", \"w_gg\"] elif corr_type == \"g+\": corr_group = [\"w_g_plus\"] elif corr_type == \"gg\": corr_group = [\"w_gg\"] else: raise KeyError(\"Unknown value for corr_type. Choose from [g+, gg, both]\") self._combine_jackknife_information(dataset_name=dataset_name, jk_group_name=f\"{dataset_name}_jk{num_jk}\", corr_group=corr_group, num_box=num_jk) else: # no covariance if self.num_nodes > 1 and temp_storage: self._measure_xi_rp_pi_box_multiprocessing(dataset_name=dataset_name, temp_file_path=temp_file_path, masks=masks, return_output=False, num_nodes=self.num_nodes, chunk_size=chunk_size, ellipticity=ellipticity) elif temp_storage: self._measure_xi_rp_pi_box_tree(masks=masks, dataset_name=dataset_name, return_output=False, ellipticity=ellipticity) else: self._measure_xi_rp_pi_box_brute(masks=masks, dataset_name=dataset_name, return_output=False, ellipticity=ellipticity) self._measure_w_g_i(corr_type=corr_type, dataset_name=dataset_name, return_output=False) return measure_xi_multipoles(dataset_name, corr_type, num_jk=0, temp_file_path=None, masks=None, rp_cut=None, ellipticity='distortion', chunk_size=1000) Measures \\(\\xi_{gg}\\) , \\(\\xi_{g+}\\) and \\(\\tilde{\\xi}_{gg,0}\\) , \\(\\tilde{\\xi}_{g+,2}\\) including jackknife covariance if desired. Manages the various _measure_xi_r_mur_box method options in MeasureMultipolesBox and MeasureMultipolesBoxJackknife. Parameters: dataset_name ( str ) \u2013 Name of the dataset in the output file. corr_type ( str ) \u2013 Type of correlation to be measured. Choose from [g+, gg, both]. num_jk ( int , default: 0 ) \u2013 Number of jackknife regions (needs to be x^3, with x an int) for the covariance measurement. Default is 0 (no covariance). temp_file_path ( str or NoneType , default: None ) \u2013 Path to where the data is temporarily stored [file name generated automatically]. masks ( dict or NoneType , default: None ) \u2013 Directory of mask information in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. rp_cut ( float or NoneType , default: None ) \u2013 Applies a minimum r_p value condition for pairs to be included. Default is None. chunck_size \u2013 Size of the chunks of data sent to each multiprocessing node. If larger, more RAM is needed per node. ellipticity ( str , default: 'distortion' ) \u2013 Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. Source code in src/measureia/measure_IA.py def measure_xi_multipoles(self, dataset_name, corr_type, num_jk=0, temp_file_path=None, masks=None, rp_cut=None, ellipticity='distortion', chunk_size=1000): r\"\"\"Measures $\\xi_{gg}$, $\\xi_{g+}$ and $\\tilde{\\xi}_{gg,0}$, $\\tilde{\\xi}_{g+,2}$ including jackknife covariance if desired. Manages the various _measure_xi_r_mur_box method options in MeasureMultipolesBox and MeasureMultipolesBoxJackknife. Parameters ---------- dataset_name : str Name of the dataset in the output file. corr_type : str Type of correlation to be measured. Choose from [g+, gg, both]. num_jk : int, optional Number of jackknife regions (needs to be x^3, with x an int) for the covariance measurement. Default is 0 (no covariance). temp_file_path : str or NoneType, optional Path to where the data is temporarily stored [file name generated automatically]. masks : dict or NoneType, optional Directory of mask information in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. rp_cut : float or NoneType, optional Applies a minimum r_p value condition for pairs to be included. Default is None. chunck_size: int, optional Size of the chunks of data sent to each multiprocessing node. If larger, more RAM is needed per node. ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. \"\"\" if num_jk > 0: try: assert sympy.integer_nthroot(num_jk, 3)[1] L = sympy.integer_nthroot(num_jk, 3)[0] except AssertionError: raise ValueError( f\"Use x^3 as input for num_jk, with x as an int. {float(int(num_jk ** (1. / 3)))},{num_jk ** (1. / 3)}\") if temp_file_path == False: temp_storage = False temp_file_path = None else: temp_storage = True if temp_storage and temp_file_path == None: raise ValueError( \"Input temp_file_path for faster computation. Do not want to save data temporarily? Input file_path_tree=False.\") # replace by better checks of input data try: RA = self.data[\"RA\"] print(\"Given data is lightcone, use measure_xi_w_lightcone method instead.\") exit() except: pass if num_jk > 0: # include covariance if self.num_nodes > 1 and temp_storage: self._measure_xi_r_mur_box_jk_multiprocessing(masks=masks, L_subboxes=L, dataset_name=dataset_name, return_output=False, num_nodes=self.num_nodes, jk_group_name=f\"{dataset_name}_jk{num_jk}\", chunk_size=chunk_size, ellipticity=ellipticity, file_tree_path=temp_file_path) elif temp_storage: self._measure_xi_r_mur_box_jk_tree(masks=masks, L_subboxes=L, dataset_name=dataset_name, return_output=False, ellipticity=ellipticity, jk_group_name=f\"{dataset_name}_jk{num_jk}\") else: self._measure_xi_r_mur_box_jk_brute(masks=masks, L_subboxes=L, dataset_name=dataset_name, return_output=False, ellipticity=ellipticity, jk_group_name=f\"{dataset_name}_jk{num_jk}\") self._measure_multipoles(corr_type=corr_type, dataset_name=dataset_name, return_output=False) for i in np.arange(num_jk): self._measure_multipoles(corr_type=corr_type, dataset_name=f\"{dataset_name}_{i}\", jk_group_name=f\"{dataset_name}_jk{num_jk}\", return_output=False) if corr_type == \"both\": corr_group = [\"multipoles_g_plus\", \"multipoles_gg\"] elif corr_type == \"g+\": corr_group = [\"multipoles_g_plus\"] elif corr_type == \"gg\": corr_group = [\"multipoles_gg\"] else: raise KeyError(\"Unknown value for corr_type. Choose from [g+, gg, both]\") self._combine_jackknife_information(dataset_name=dataset_name, jk_group_name=f\"{dataset_name}_jk{num_jk}\", corr_group=corr_group, num_box=num_jk) else: # no covariance if self.num_nodes > 1 and temp_storage: self._measure_xi_r_mur_box_multiprocessing(dataset_name=dataset_name, temp_file_path=temp_file_path, masks=masks, return_output=False, num_nodes=self.num_nodes, chunk_size=chunk_size, ellipticity=ellipticity) elif temp_storage: self._measure_xi_r_mur_box_tree(masks=masks, dataset_name=dataset_name, return_output=False, ellipticity=ellipticity) else: self._measure_xi_r_mur_box_brute(masks=masks, dataset_name=dataset_name, return_output=False, ellipticity=ellipticity) self._measure_multipoles(corr_type=corr_type, dataset_name=dataset_name, return_output=False) return","title":"MeasureIABox"},{"location":"api/measureIABox/#measureia","text":"","title":"MeasureIA"},{"location":"api/measureIABox/#measureia.MeasureIABox","text":"Bases: MeasureWBox , MeasureMultipolesBox , MeasureWBoxJackknife , MeasureMBoxJackknife Manages the IA correlation function measurement methods used in the MeasureIA package based on speed and input. This class is used to call the methods that measure \\(w_{gg}\\) , \\(w_{g+}\\) and multipoles for simulations in cartesian coordinates. Depending on the input parameters, various correlations incl covariance estimates are measured for given data. Methods: Name Description measure_xi_w Compute projected correlations \\(w_{gg}\\) and/or \\(w_{g+}\\) . measure_xi_multipoles Compute multipoles of the correlation functions, \\(\\tilde{\\xi}_{gg,0}\\) and/or \\(\\tilde{\\xi}_{g+,2}\\) . Notes Inherits attributes from 'SimInfo', where 'boxsize', 'L_0p5' and 'snap_group' are used in this class. Inherits attributes from 'MeasureIABase', where 'data', 'output_file_name', 'periodicity', 'Num_position', 'Num_shape', 'r_min', 'r_max', 'num_bins_r', 'num_bins_pi', 'r_bins', 'pi_bins', 'mu_r_bins' are used. Source code in src/measureia/measure_IA.py class MeasureIABox(MeasureWBox, MeasureMultipolesBox, MeasureWBoxJackknife, MeasureMBoxJackknife): r\"\"\"Manages the IA correlation function measurement methods used in the MeasureIA package based on speed and input. This class is used to call the methods that measure $w_{gg}$, $w_{g+}$ and multipoles for simulations in cartesian coordinates. Depending on the input parameters, various correlations incl covariance estimates are measured for given data. Methods ------- measure_xi_w() Compute projected correlations $w_{gg}$ and/or $w_{g+}$. measure_xi_multipoles() Compute multipoles of the correlation functions, $\\tilde{\\xi}_{gg,0}$ and/or $\\tilde{\\xi}_{g+,2}$. Notes ----- Inherits attributes from 'SimInfo', where 'boxsize', 'L_0p5' and 'snap_group' are used in this class. Inherits attributes from 'MeasureIABase', where 'data', 'output_file_name', 'periodicity', 'Num_position', 'Num_shape', 'r_min', 'r_max', 'num_bins_r', 'num_bins_pi', 'r_bins', 'pi_bins', 'mu_r_bins' are used. \"\"\" def __init__( self, data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True, num_nodes=1, ): \"\"\" The __init__ method of the MeasureIABox class. Parameters ---------- num_nodes : int, optional Number of cores to be used in multiprocessing. Default is 1. Notes ----- Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. \"\"\" super().__init__(data, output_file_name, simulation, snapshot, separation_limits, num_bins_r, num_bins_pi, pi_max, boxsize, periodicity) self.num_nodes = num_nodes self.randoms_data = None self.data_dir = None self.num_samples = None return def measure_xi_w(self, dataset_name, corr_type, num_jk=0, temp_file_path=None, masks=None, ellipticity='distortion', chunk_size=1000): r\"\"\"Measures $\\xi_{gg}$, $\\xi_{g+}$ and $w_{gg}$, $w_{g+}$ including jackknife covariance if desired. Manages the various _measure_xi_rp_pi_box method options in MeasureWBox and MeasureWBoxJackknife. Parameters ---------- dataset_name : str Name of the dataset in the output file. corr_type : str Type of correlation to be measured. Choose from [g+, gg, both]. num_jk : int, optional Number of jackknife regions (needs to be x^3, with x an int) for the covariance measurement. Default is 0 (no covariance). temp_file_path : str or NoneType, optional Path to where the data is temporarily stored [file name generated automatically]. masks : dict or NoneType, optional Directory of mask information in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. chunk_size: int, optional Size of the chunks of data sent to each multiprocessing node. If larger, more RAM is needed per node. Default is 1000. ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. \"\"\" if num_jk > 0: try: assert sympy.integer_nthroot(num_jk, 3)[1] L = sympy.integer_nthroot(num_jk, 3)[0] except AssertionError: raise ValueError( f\"Use x^3 as input for num_jk, with x as an int. {float(int(num_jk ** (1. / 3)))},{num_jk ** (1. / 3)}\") if temp_file_path == False: temp_storage = False temp_file_path = None else: temp_storage = True if temp_storage and temp_file_path == None: raise ValueError( \"Input temp_file_path for faster computation. Do not want to save data temporarily? Input file_path_tree=False.\") # replace by better checks of input data try: RA = self.data[\"RA\"] print(\"Given data is lightcone, use measure_xi_w_lightcone method instead.\") exit() except: pass if num_jk > 0: # include covariance if self.num_nodes > 1 and temp_storage: self._measure_xi_rp_pi_box_jk_multiprocessing(masks=masks, L_subboxes=L, dataset_name=dataset_name, return_output=False, num_nodes=self.num_nodes, jk_group_name=f\"{dataset_name}_jk{num_jk}\", chunk_size=chunk_size, ellipticity=ellipticity, temp_file_path=temp_file_path) elif temp_storage: self._measure_xi_rp_pi_box_jk_tree(masks=masks, L_subboxes=L, dataset_name=dataset_name, return_output=False, ellipticity=ellipticity, jk_group_name=f\"{dataset_name}_jk{num_jk}\") else: self._measure_xi_rp_pi_box_jk_brute(masks=masks, L_subboxes=L, dataset_name=dataset_name, return_output=False, ellipticity=ellipticity, jk_group_name=f\"{dataset_name}_jk{num_jk}\") self._measure_w_g_i(corr_type=corr_type, dataset_name=dataset_name, return_output=False) for i in np.arange(num_jk): self._measure_w_g_i(corr_type=corr_type, dataset_name=f\"{dataset_name}_{i}\", jk_group_name=f\"{dataset_name}_jk{num_jk}\", return_output=False) if corr_type == \"both\": corr_group = [\"w_g_plus\", \"w_gg\"] elif corr_type == \"g+\": corr_group = [\"w_g_plus\"] elif corr_type == \"gg\": corr_group = [\"w_gg\"] else: raise KeyError(\"Unknown value for corr_type. Choose from [g+, gg, both]\") self._combine_jackknife_information(dataset_name=dataset_name, jk_group_name=f\"{dataset_name}_jk{num_jk}\", corr_group=corr_group, num_box=num_jk) else: # no covariance if self.num_nodes > 1 and temp_storage: self._measure_xi_rp_pi_box_multiprocessing(dataset_name=dataset_name, temp_file_path=temp_file_path, masks=masks, return_output=False, num_nodes=self.num_nodes, chunk_size=chunk_size, ellipticity=ellipticity) elif temp_storage: self._measure_xi_rp_pi_box_tree(masks=masks, dataset_name=dataset_name, return_output=False, ellipticity=ellipticity) else: self._measure_xi_rp_pi_box_brute(masks=masks, dataset_name=dataset_name, return_output=False, ellipticity=ellipticity) self._measure_w_g_i(corr_type=corr_type, dataset_name=dataset_name, return_output=False) return def measure_xi_multipoles(self, dataset_name, corr_type, num_jk=0, temp_file_path=None, masks=None, rp_cut=None, ellipticity='distortion', chunk_size=1000): r\"\"\"Measures $\\xi_{gg}$, $\\xi_{g+}$ and $\\tilde{\\xi}_{gg,0}$, $\\tilde{\\xi}_{g+,2}$ including jackknife covariance if desired. Manages the various _measure_xi_r_mur_box method options in MeasureMultipolesBox and MeasureMultipolesBoxJackknife. Parameters ---------- dataset_name : str Name of the dataset in the output file. corr_type : str Type of correlation to be measured. Choose from [g+, gg, both]. num_jk : int, optional Number of jackknife regions (needs to be x^3, with x an int) for the covariance measurement. Default is 0 (no covariance). temp_file_path : str or NoneType, optional Path to where the data is temporarily stored [file name generated automatically]. masks : dict or NoneType, optional Directory of mask information in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. rp_cut : float or NoneType, optional Applies a minimum r_p value condition for pairs to be included. Default is None. chunck_size: int, optional Size of the chunks of data sent to each multiprocessing node. If larger, more RAM is needed per node. ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. \"\"\" if num_jk > 0: try: assert sympy.integer_nthroot(num_jk, 3)[1] L = sympy.integer_nthroot(num_jk, 3)[0] except AssertionError: raise ValueError( f\"Use x^3 as input for num_jk, with x as an int. {float(int(num_jk ** (1. / 3)))},{num_jk ** (1. / 3)}\") if temp_file_path == False: temp_storage = False temp_file_path = None else: temp_storage = True if temp_storage and temp_file_path == None: raise ValueError( \"Input temp_file_path for faster computation. Do not want to save data temporarily? Input file_path_tree=False.\") # replace by better checks of input data try: RA = self.data[\"RA\"] print(\"Given data is lightcone, use measure_xi_w_lightcone method instead.\") exit() except: pass if num_jk > 0: # include covariance if self.num_nodes > 1 and temp_storage: self._measure_xi_r_mur_box_jk_multiprocessing(masks=masks, L_subboxes=L, dataset_name=dataset_name, return_output=False, num_nodes=self.num_nodes, jk_group_name=f\"{dataset_name}_jk{num_jk}\", chunk_size=chunk_size, ellipticity=ellipticity, file_tree_path=temp_file_path) elif temp_storage: self._measure_xi_r_mur_box_jk_tree(masks=masks, L_subboxes=L, dataset_name=dataset_name, return_output=False, ellipticity=ellipticity, jk_group_name=f\"{dataset_name}_jk{num_jk}\") else: self._measure_xi_r_mur_box_jk_brute(masks=masks, L_subboxes=L, dataset_name=dataset_name, return_output=False, ellipticity=ellipticity, jk_group_name=f\"{dataset_name}_jk{num_jk}\") self._measure_multipoles(corr_type=corr_type, dataset_name=dataset_name, return_output=False) for i in np.arange(num_jk): self._measure_multipoles(corr_type=corr_type, dataset_name=f\"{dataset_name}_{i}\", jk_group_name=f\"{dataset_name}_jk{num_jk}\", return_output=False) if corr_type == \"both\": corr_group = [\"multipoles_g_plus\", \"multipoles_gg\"] elif corr_type == \"g+\": corr_group = [\"multipoles_g_plus\"] elif corr_type == \"gg\": corr_group = [\"multipoles_gg\"] else: raise KeyError(\"Unknown value for corr_type. Choose from [g+, gg, both]\") self._combine_jackknife_information(dataset_name=dataset_name, jk_group_name=f\"{dataset_name}_jk{num_jk}\", corr_group=corr_group, num_box=num_jk) else: # no covariance if self.num_nodes > 1 and temp_storage: self._measure_xi_r_mur_box_multiprocessing(dataset_name=dataset_name, temp_file_path=temp_file_path, masks=masks, return_output=False, num_nodes=self.num_nodes, chunk_size=chunk_size, ellipticity=ellipticity) elif temp_storage: self._measure_xi_r_mur_box_tree(masks=masks, dataset_name=dataset_name, return_output=False, ellipticity=ellipticity) else: self._measure_xi_r_mur_box_brute(masks=masks, dataset_name=dataset_name, return_output=False, ellipticity=ellipticity) self._measure_multipoles(corr_type=corr_type, dataset_name=dataset_name, return_output=False) return","title":"MeasureIABox"},{"location":"api/measureIABox/#measureia.MeasureIABox.__init__","text":"The init method of the MeasureIABox class. Parameters: num_nodes ( int , default: 1 ) \u2013 Number of cores to be used in multiprocessing. Default is 1. Notes Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. Source code in src/measureia/measure_IA.py def __init__( self, data, output_file_name, simulation=None, snapshot=None, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, boxsize=None, periodicity=True, num_nodes=1, ): \"\"\" The __init__ method of the MeasureIABox class. Parameters ---------- num_nodes : int, optional Number of cores to be used in multiprocessing. Default is 1. Notes ----- Constructor parameters 'data', 'output_file_name', 'simulation', 'snapshot', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', 'boxsize' and 'periodicity' are passed to MeasureIABase. \"\"\" super().__init__(data, output_file_name, simulation, snapshot, separation_limits, num_bins_r, num_bins_pi, pi_max, boxsize, periodicity) self.num_nodes = num_nodes self.randoms_data = None self.data_dir = None self.num_samples = None return","title":"__init__"},{"location":"api/measureIABox/#measureia.MeasureIABox.measure_xi_w","text":"Measures \\(\\xi_{gg}\\) , \\(\\xi_{g+}\\) and \\(w_{gg}\\) , \\(w_{g+}\\) including jackknife covariance if desired. Manages the various _measure_xi_rp_pi_box method options in MeasureWBox and MeasureWBoxJackknife. Parameters: dataset_name ( str ) \u2013 Name of the dataset in the output file. corr_type ( str ) \u2013 Type of correlation to be measured. Choose from [g+, gg, both]. num_jk ( int , default: 0 ) \u2013 Number of jackknife regions (needs to be x^3, with x an int) for the covariance measurement. Default is 0 (no covariance). temp_file_path ( str or NoneType , default: None ) \u2013 Path to where the data is temporarily stored [file name generated automatically]. masks ( dict or NoneType , default: None ) \u2013 Directory of mask information in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. chunk_size \u2013 Size of the chunks of data sent to each multiprocessing node. If larger, more RAM is needed per node. Default is 1000. ellipticity ( str , default: 'distortion' ) \u2013 Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. Source code in src/measureia/measure_IA.py def measure_xi_w(self, dataset_name, corr_type, num_jk=0, temp_file_path=None, masks=None, ellipticity='distortion', chunk_size=1000): r\"\"\"Measures $\\xi_{gg}$, $\\xi_{g+}$ and $w_{gg}$, $w_{g+}$ including jackknife covariance if desired. Manages the various _measure_xi_rp_pi_box method options in MeasureWBox and MeasureWBoxJackknife. Parameters ---------- dataset_name : str Name of the dataset in the output file. corr_type : str Type of correlation to be measured. Choose from [g+, gg, both]. num_jk : int, optional Number of jackknife regions (needs to be x^3, with x an int) for the covariance measurement. Default is 0 (no covariance). temp_file_path : str or NoneType, optional Path to where the data is temporarily stored [file name generated automatically]. masks : dict or NoneType, optional Directory of mask information in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. chunk_size: int, optional Size of the chunks of data sent to each multiprocessing node. If larger, more RAM is needed per node. Default is 1000. ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. \"\"\" if num_jk > 0: try: assert sympy.integer_nthroot(num_jk, 3)[1] L = sympy.integer_nthroot(num_jk, 3)[0] except AssertionError: raise ValueError( f\"Use x^3 as input for num_jk, with x as an int. {float(int(num_jk ** (1. / 3)))},{num_jk ** (1. / 3)}\") if temp_file_path == False: temp_storage = False temp_file_path = None else: temp_storage = True if temp_storage and temp_file_path == None: raise ValueError( \"Input temp_file_path for faster computation. Do not want to save data temporarily? Input file_path_tree=False.\") # replace by better checks of input data try: RA = self.data[\"RA\"] print(\"Given data is lightcone, use measure_xi_w_lightcone method instead.\") exit() except: pass if num_jk > 0: # include covariance if self.num_nodes > 1 and temp_storage: self._measure_xi_rp_pi_box_jk_multiprocessing(masks=masks, L_subboxes=L, dataset_name=dataset_name, return_output=False, num_nodes=self.num_nodes, jk_group_name=f\"{dataset_name}_jk{num_jk}\", chunk_size=chunk_size, ellipticity=ellipticity, temp_file_path=temp_file_path) elif temp_storage: self._measure_xi_rp_pi_box_jk_tree(masks=masks, L_subboxes=L, dataset_name=dataset_name, return_output=False, ellipticity=ellipticity, jk_group_name=f\"{dataset_name}_jk{num_jk}\") else: self._measure_xi_rp_pi_box_jk_brute(masks=masks, L_subboxes=L, dataset_name=dataset_name, return_output=False, ellipticity=ellipticity, jk_group_name=f\"{dataset_name}_jk{num_jk}\") self._measure_w_g_i(corr_type=corr_type, dataset_name=dataset_name, return_output=False) for i in np.arange(num_jk): self._measure_w_g_i(corr_type=corr_type, dataset_name=f\"{dataset_name}_{i}\", jk_group_name=f\"{dataset_name}_jk{num_jk}\", return_output=False) if corr_type == \"both\": corr_group = [\"w_g_plus\", \"w_gg\"] elif corr_type == \"g+\": corr_group = [\"w_g_plus\"] elif corr_type == \"gg\": corr_group = [\"w_gg\"] else: raise KeyError(\"Unknown value for corr_type. Choose from [g+, gg, both]\") self._combine_jackknife_information(dataset_name=dataset_name, jk_group_name=f\"{dataset_name}_jk{num_jk}\", corr_group=corr_group, num_box=num_jk) else: # no covariance if self.num_nodes > 1 and temp_storage: self._measure_xi_rp_pi_box_multiprocessing(dataset_name=dataset_name, temp_file_path=temp_file_path, masks=masks, return_output=False, num_nodes=self.num_nodes, chunk_size=chunk_size, ellipticity=ellipticity) elif temp_storage: self._measure_xi_rp_pi_box_tree(masks=masks, dataset_name=dataset_name, return_output=False, ellipticity=ellipticity) else: self._measure_xi_rp_pi_box_brute(masks=masks, dataset_name=dataset_name, return_output=False, ellipticity=ellipticity) self._measure_w_g_i(corr_type=corr_type, dataset_name=dataset_name, return_output=False) return","title":"measure_xi_w"},{"location":"api/measureIABox/#measureia.MeasureIABox.measure_xi_multipoles","text":"Measures \\(\\xi_{gg}\\) , \\(\\xi_{g+}\\) and \\(\\tilde{\\xi}_{gg,0}\\) , \\(\\tilde{\\xi}_{g+,2}\\) including jackknife covariance if desired. Manages the various _measure_xi_r_mur_box method options in MeasureMultipolesBox and MeasureMultipolesBoxJackknife. Parameters: dataset_name ( str ) \u2013 Name of the dataset in the output file. corr_type ( str ) \u2013 Type of correlation to be measured. Choose from [g+, gg, both]. num_jk ( int , default: 0 ) \u2013 Number of jackknife regions (needs to be x^3, with x an int) for the covariance measurement. Default is 0 (no covariance). temp_file_path ( str or NoneType , default: None ) \u2013 Path to where the data is temporarily stored [file name generated automatically]. masks ( dict or NoneType , default: None ) \u2013 Directory of mask information in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. rp_cut ( float or NoneType , default: None ) \u2013 Applies a minimum r_p value condition for pairs to be included. Default is None. chunck_size \u2013 Size of the chunks of data sent to each multiprocessing node. If larger, more RAM is needed per node. ellipticity ( str , default: 'distortion' ) \u2013 Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. Source code in src/measureia/measure_IA.py def measure_xi_multipoles(self, dataset_name, corr_type, num_jk=0, temp_file_path=None, masks=None, rp_cut=None, ellipticity='distortion', chunk_size=1000): r\"\"\"Measures $\\xi_{gg}$, $\\xi_{g+}$ and $\\tilde{\\xi}_{gg,0}$, $\\tilde{\\xi}_{g+,2}$ including jackknife covariance if desired. Manages the various _measure_xi_r_mur_box method options in MeasureMultipolesBox and MeasureMultipolesBoxJackknife. Parameters ---------- dataset_name : str Name of the dataset in the output file. corr_type : str Type of correlation to be measured. Choose from [g+, gg, both]. num_jk : int, optional Number of jackknife regions (needs to be x^3, with x an int) for the covariance measurement. Default is 0 (no covariance). temp_file_path : str or NoneType, optional Path to where the data is temporarily stored [file name generated automatically]. masks : dict or NoneType, optional Directory of mask information in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. rp_cut : float or NoneType, optional Applies a minimum r_p value condition for pairs to be included. Default is None. chunck_size: int, optional Size of the chunks of data sent to each multiprocessing node. If larger, more RAM is needed per node. ellipticity : str, optional Definition of ellipticity. Choose from 'distortion', defined as (1-q^2)/(1+q^2), or 'ellipticity', defined as (1-q)/(1+q). Default is 'distortion'. \"\"\" if num_jk > 0: try: assert sympy.integer_nthroot(num_jk, 3)[1] L = sympy.integer_nthroot(num_jk, 3)[0] except AssertionError: raise ValueError( f\"Use x^3 as input for num_jk, with x as an int. {float(int(num_jk ** (1. / 3)))},{num_jk ** (1. / 3)}\") if temp_file_path == False: temp_storage = False temp_file_path = None else: temp_storage = True if temp_storage and temp_file_path == None: raise ValueError( \"Input temp_file_path for faster computation. Do not want to save data temporarily? Input file_path_tree=False.\") # replace by better checks of input data try: RA = self.data[\"RA\"] print(\"Given data is lightcone, use measure_xi_w_lightcone method instead.\") exit() except: pass if num_jk > 0: # include covariance if self.num_nodes > 1 and temp_storage: self._measure_xi_r_mur_box_jk_multiprocessing(masks=masks, L_subboxes=L, dataset_name=dataset_name, return_output=False, num_nodes=self.num_nodes, jk_group_name=f\"{dataset_name}_jk{num_jk}\", chunk_size=chunk_size, ellipticity=ellipticity, file_tree_path=temp_file_path) elif temp_storage: self._measure_xi_r_mur_box_jk_tree(masks=masks, L_subboxes=L, dataset_name=dataset_name, return_output=False, ellipticity=ellipticity, jk_group_name=f\"{dataset_name}_jk{num_jk}\") else: self._measure_xi_r_mur_box_jk_brute(masks=masks, L_subboxes=L, dataset_name=dataset_name, return_output=False, ellipticity=ellipticity, jk_group_name=f\"{dataset_name}_jk{num_jk}\") self._measure_multipoles(corr_type=corr_type, dataset_name=dataset_name, return_output=False) for i in np.arange(num_jk): self._measure_multipoles(corr_type=corr_type, dataset_name=f\"{dataset_name}_{i}\", jk_group_name=f\"{dataset_name}_jk{num_jk}\", return_output=False) if corr_type == \"both\": corr_group = [\"multipoles_g_plus\", \"multipoles_gg\"] elif corr_type == \"g+\": corr_group = [\"multipoles_g_plus\"] elif corr_type == \"gg\": corr_group = [\"multipoles_gg\"] else: raise KeyError(\"Unknown value for corr_type. Choose from [g+, gg, both]\") self._combine_jackknife_information(dataset_name=dataset_name, jk_group_name=f\"{dataset_name}_jk{num_jk}\", corr_group=corr_group, num_box=num_jk) else: # no covariance if self.num_nodes > 1 and temp_storage: self._measure_xi_r_mur_box_multiprocessing(dataset_name=dataset_name, temp_file_path=temp_file_path, masks=masks, return_output=False, num_nodes=self.num_nodes, chunk_size=chunk_size, ellipticity=ellipticity) elif temp_storage: self._measure_xi_r_mur_box_tree(masks=masks, dataset_name=dataset_name, return_output=False, ellipticity=ellipticity) else: self._measure_xi_r_mur_box_brute(masks=masks, dataset_name=dataset_name, return_output=False, ellipticity=ellipticity) self._measure_multipoles(corr_type=corr_type, dataset_name=dataset_name, return_output=False) return","title":"measure_xi_multipoles"},{"location":"api/measureIALightcone/","text":"Measure measureia.MeasureIALightcone Bases: MeasureJackknife Manages the IA correlation function measurement methods used in the MeasureIA package based on speed and input. This class is used to call the methods that measure w_gg, w_g+ and multipoles for simulations (and observations), with lightcone data. Depending on the input parameters, various correlations incl covariance estimates are measured for given data. Attributes: data_dir ( dict or NoneType ) \u2013 Temporary storage space for added data directory to allow for flexibility in passing data or randoms to internal methods. num_samples ( dict or NoneType ) \u2013 Dictionary containing the numbers of objects for each sample for lightcone-type measurements. Filled internally, no input needed. Methods: Name Description measure_xi_w Compute projected correlations \\(w_{gg}\\) and/or \\(w_{g+}\\) . measure_xi_multipoles Compute multipoles of the correlation functions, \\(\\tilde{\\xi}_{gg,0}\\) and/or \\(\\tilde{\\xi}_{g+,2}\\) . Notes Inherits attributes from 'SimInfo', where none are used in this class. Inherits attributes from 'MeasureIABase', where 'data', 'output_file_name', 'Num_position', 'Num_shape', 'r_min', 'r_max', 'num_bins_r', 'num_bins_pi', 'r_bins', 'pi_bins', 'mu_r_bins' are used. Source code in src/measureia/measure_IA.py class MeasureIALightcone(MeasureJackknife): r\"\"\"Manages the IA correlation function measurement methods used in the MeasureIA package based on speed and input. This class is used to call the methods that measure w_gg, w_g+ and multipoles for simulations (and observations), with lightcone data. Depending on the input parameters, various correlations incl covariance estimates are measured for given data. Attributes ---------- data_dir : dict or NoneType Temporary storage space for added data directory to allow for flexibility in passing data or randoms to internal methods. num_samples : dict or NoneType Dictionary containing the numbers of objects for each sample for lightcone-type measurements. Filled internally, no input needed. Methods ------- measure_xi_w() Compute projected correlations $w_{gg}$ and/or $w_{g+}$. measure_xi_multipoles() Compute multipoles of the correlation functions, $\\tilde{\\xi}_{gg,0}$ and/or $\\tilde{\\xi}_{g+,2}$. Notes ----- Inherits attributes from 'SimInfo', where none are used in this class. Inherits attributes from 'MeasureIABase', where 'data', 'output_file_name', 'Num_position', 'Num_shape', 'r_min', 'r_max', 'num_bins_r', 'num_bins_pi', 'r_bins', 'pi_bins', 'mu_r_bins' are used. \"\"\" def __init__( self, data, randoms_data, output_file_name, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, num_nodes=1, ): \"\"\" The __init__ method of the MeasureIALightcone class. Parameters ---------- randoms_data : dict or NoneType Dictionary with data of the randoms needed for lightcone-type measurements. The keywords are: 'Redshift' and 'Redshift_shape_sample': (N_p) and (N_s) ndarray with redshifts of position and shape samples. 'RA' and 'RA_shape_sample': (N_p) and (N_s) ndarray with RA coordinate of position and shape samples. 'DEC' and 'DEC_shape_sample': (N_p) and (N_s) ndarray with DEC coordinate of position and shape samples. If only 'Redshift', 'RA' and 'DEC' are added, the sample will be used for both position and shape sample randoms. num_nodes : int, optional Number of cores to be used in multiprocessing. Default is 1. Notes ----- Constructor parameters 'data', 'output_file_name', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', are passed to MeasureIABase. \"\"\" super().__init__(data, output_file_name, False, None, separation_limits, num_bins_r, num_bins_pi, pi_max, None, False) self.num_nodes = num_nodes self.randoms_data = randoms_data self.data_dir = None self.num_samples = None return def measure_xi_w(self, IA_estimator, dataset_name, corr_type, jk_patches=None, num_jk=None, measure_cov=True, masks=None, masks_randoms=None, cosmology=None, over_h=False): \"\"\"Measures xi_gg, xi_g+ and w_gg, w_g+ including jackknife covariance if desired for lightcone data. Manages the various _measure_xi_rp_pi_obs and _measure_jackknife_covariance options in MeasureWObservations and MeasureJackknife. Parameters ---------- IA_estimator : str Choose which type of xi estimator is used. Choose from \"clusters\" or \"galaxies\". dataset_name : str Name of the dataset in the output file. corr_type : str Type of correlation to be measured. Choose from [g+, gg, both]. randoms_data : dict or NoneType Dictionary that includes the randoms data in the same form as the data dictionary. jk_patches : dict or NoneType, optional Dictionary with entries of the jackknife patch numbers (ndarray) for each sample, named \"position\", \"shape\" and \"random\". Default is None. num_jk : int, optional Number of jackknife patches to be generated internally. Default is None. measure_cov : bool, optional If True, jackknife errors are calculated. Default is True. masks : dict or NoneType, optional Dictionary of mask information in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. masks_randoms : dict or NoneType, optional Dictionary of mask information for the randoms data in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. cosmology : pyccl cosmology object or NoneType, optional Pyccl cosmology to use in the calculation. If None (default), the cosmology is used: ccl.Cosmology(Omega_c=0.225, Omega_b=0.045, sigma8=0.8, h=0.7, n_s=1.0) over_h : bool, optional If True, the units are assumed to be in not-over-h and converted to over-h units. Default is False. \"\"\" if IA_estimator == \"clusters\": if self.randoms_data == None: print(\"No randoms given, correlation defined as S+D/DD\") raise KeyError(\"This version does not work yet, add randoms.\") else: print(\"xi_g+ defined as S+D/SD - S+R/SR, xi_gg as (SD - RD - SR)/RR - 1\") if masks != None and masks_randoms == None: print(\"Warning, masks given for data vector but not for randoms.\") elif IA_estimator == \"galaxies\": if self.randoms_data == None: raise KeyError(\"No randoms given. Please provide input.\") else: print(\"xi_g+ defined as (S+D - S+R)/RR, xi_gg as (SD - RD - SR)/RR - 1\") if masks != None and masks_randoms == None: print(\"Warning, masks given for data vector but not for randoms.\") print(\"WARNING: this version of the code has not been fully validated. Proceed with caution.\") else: raise KeyError(\"Unknown input for IA_estimator, choose from [clusters, galaxies].\") # todo: Expand to include methods with trees and internal multiproc # todo: Checks to see if data directories include everything they need data = self.data # temporary save so it can be restored at the end of the calculation try: # Are there one or two random samples given? random_shape = self.randoms_data[\"RA_shape_sample\"] one_random_sample = False except: one_random_sample = True self.randoms_data[\"RA_shape_sample\"] = self.randoms_data[\"RA\"] self.randoms_data[\"DEC_shape_sample\"] = self.randoms_data[\"DEC\"] self.randoms_data[\"Redshift_shape_sample\"] = self.randoms_data[\"Redshift\"] try: weight = self.randoms_data[\"weight\"] except: self.randoms_data[\"weight\"] = np.ones(len(self.randoms_data[\"RA\"])) try: weight = self.randoms_data[\"weight_shape_sample\"] except: if one_random_sample: self.randoms_data[\"weight_shape_sample\"] = self.randoms_data[\"weight\"] # in case weights are given else: self.randoms_data[\"weight_shape_sample\"] = np.ones(len(self.randoms_data[\"RA_shape_sample\"])) if measure_cov: if jk_patches == None: if num_jk != None: jk_patches = self.assign_jackknife_patches(data, self.randoms_data, num_jk) else: raise ValueError(\"Set calc_errors to False, or provide either jk_patches or num_jk input.\") else: if one_random_sample: jk_patches[\"randoms_position\"] = jk_patches[\"randoms\"] jk_patches[\"randoms_shape\"] = jk_patches[\"randoms\"] self.data_dir = data try: weight = self.data_dir[\"weight\"] except: self.data_dir[\"weight\"] = np.ones(len(self.data_dir[\"RA\"])) try: weight = self.data_dir[\"weight_shape_sample\"] except: self.data_dir[\"weight_shape_sample\"] = np.ones(len(self.data_dir[\"RA_shape_sample\"])) num_samples = {} # Needed to correct for different number of randoms and galaxies/clusters in data if masks == None: num_samples[\"D\"] = len(self.data_dir[\"RA\"]) num_samples[\"S\"] = len(self.data_dir[\"RA_shape_sample\"]) else: num_samples[\"D\"] = len(self.data_dir[\"RA\"][masks[\"RA\"]]) num_samples[\"S\"] = len(self.data_dir[\"RA_shape_sample\"][masks[\"RA_shape_sample\"]]) if masks_randoms == None: num_samples[\"R_D\"] = len(self.randoms_data[\"RA\"]) num_samples[\"R_S\"] = len(self.randoms_data[\"RA_shape_sample\"]) else: num_samples[\"R_D\"] = len(self.randoms_data[\"RA\"][masks_randoms[\"RA\"]]) num_samples[\"R_S\"] = len(self.randoms_data[\"RA_shape_sample\"][masks_randoms[\"RA_shape_sample\"]]) # print(self.data_dir,self.randoms_data) # Shape-position combinations: # S+D (Cg+, Gg+) # S+R (Cg+, Gg+) if corr_type == \"g+\" or corr_type == \"both\": # S+D self.data = self.data_dir self._measure_xi_rp_pi_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology) # S+R self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"e1\": self.data_dir[\"e1\"], \"e2\": self.data_dir[\"e2\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } # print(self.data) self._measure_xi_rp_pi_lightcone_brute(masks=masks, dataset_name=f\"{dataset_name}_randoms\", over_h=over_h, cosmology=cosmology) # Position-position combinations: # SD (Cgg, Ggg) # SR (Cg+, Cgg, Ggg) # RD (Cgg, Ggg) # RR (Cgg, Gg+, Ggg) if corr_type == \"gg\": # already have it for 'both' # SD (Cgg, Ggg) self.data = { \"Redshift\": self.data_dir[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.data_dir[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.data_dir[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"weight\": self.data_dir[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._count_pairs_xi_rp_pi_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology, data_suffix=\"_DD\") # SR (Cg+, Cgg, Ggg) - watch name (Obs estimator) # if g+ or both, already have it self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._count_pairs_xi_rp_pi_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology, data_suffix=\"_SR\") if corr_type == \"gg\" or corr_type == \"both\": # RD (Cgg, Ggg) self.data = { \"Redshift\": self.data_dir[\"Redshift\"], \"Redshift_shape_sample\": self.randoms_data[\"Redshift_shape_sample\"], \"RA\": self.data_dir[\"RA\"], \"RA_shape_sample\": self.randoms_data[\"RA_shape_sample\"], \"DEC\": self.data_dir[\"DEC\"], \"DEC_shape_sample\": self.randoms_data[\"DEC_shape_sample\"], \"weight\": self.data_dir[\"weight\"], \"weight_shape_sample\": self.randoms_data[\"weight_shape_sample\"] } self._count_pairs_xi_rp_pi_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology, data_suffix=\"_RD\") if IA_estimator == \"galaxies\" or corr_type == \"gg\" or corr_type == \"both\": # RR (Cgg, Gg+, Ggg) self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.randoms_data[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.randoms_data[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.randoms_data[\"DEC_shape_sample\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.randoms_data[\"weight_shape_sample\"] } self._count_pairs_xi_rp_pi_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology, data_suffix=\"_RR\") self._obs_estimator([corr_type, \"w\"], IA_estimator, dataset_name, f\"{dataset_name}_randoms\", num_samples) self._measure_w_g_i(corr_type=corr_type, dataset_name=dataset_name, return_output=False) if measure_cov: self.num_samples = {} min_patch, max_patch = int(min(jk_patches[\"shape\"])), int(max(jk_patches[\"shape\"])) for n in np.arange(min_patch, max_patch + 1): self.num_samples[f\"{n}\"] = {} # Shape-position combinations: # S+D (Cg+, Gg+) # S+R (Cg+, Gg+) if corr_type == \"g+\" or corr_type == \"both\": # S+D self.data = self.data_dir self._measure_jackknife_realisations_lightcone_multiprocessing(patches_pos=jk_patches[\"position\"], patches_shape=jk_patches[\"shape\"], corr_type=[corr_type, \"w\"], masks=masks, dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=False, num_sample_names=[\"S\", \"D\"]) # S+R self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"e1\": self.data_dir[\"e1\"], \"e2\": self.data_dir[\"e2\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } # print(self.data) self._measure_jackknife_realisations_lightcone_multiprocessing( patches_pos=jk_patches[\"randoms_position\"], patches_shape=jk_patches[\"shape\"], corr_type=[corr_type, \"w\"], masks=masks, dataset_name=f\"{dataset_name}_randoms\", num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=False, num_sample_names=[\"S\", \"R_D\"]) # Position-position combinations: # SD (Cgg, Ggg) # SR (Cg+, Cgg, Ggg) # RD (Cgg, Ggg) # RR (Cgg, Gg+, Ggg) if corr_type == \"gg\": # already have it for 'both' # SD (Cgg, Ggg) self.data = { \"Redshift\": self.data_dir[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.data_dir[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.data_dir[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"weight\": self.data_dir[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing(patches_pos=jk_patches[\"position\"], patches_shape=jk_patches[\"shape\"], corr_type=[\"gg\", \"w\"], dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=True, data_suffix=\"_DD\", num_sample_names=[\"S\", \"D\"]) # SR (Cg+, Cgg, Ggg) - watch name (Obs estimator) # if g+ or both, already have it self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing( patches_pos=jk_patches[\"randoms_position\"], patches_shape=jk_patches[\"shape\"], corr_type=[\"gg\", \"w\"], dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=True, data_suffix=\"_SR\", num_sample_names=[\"S\", \"R_D\"]) if corr_type == \"gg\" or corr_type == \"both\": # RD (Cgg, Ggg) self.data = { \"Redshift\": self.data_dir[\"Redshift\"], \"Redshift_shape_sample\": self.randoms_data[\"Redshift_shape_sample\"], \"RA\": self.data_dir[\"RA\"], \"RA_shape_sample\": self.randoms_data[\"RA_shape_sample\"], \"DEC\": self.data_dir[\"DEC\"], \"DEC_shape_sample\": self.randoms_data[\"DEC_shape_sample\"], \"weight\": self.data_dir[\"weight\"], \"weight_shape_sample\": self.randoms_data[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing(patches_pos=jk_patches[\"position\"], patches_shape=jk_patches[ \"randoms_shape\"], corr_type=[\"gg\", \"w\"], dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=True, data_suffix=\"_RD\", num_sample_names=[\"R_S\", \"D\"]) if IA_estimator == \"galaxies\" or corr_type == \"gg\" or corr_type == \"both\": # RR (Cgg, Gg+, Ggg) self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.randoms_data[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.randoms_data[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.randoms_data[\"DEC_shape_sample\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.randoms_data[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing( patches_pos=jk_patches[\"randoms_position\"], patches_shape=jk_patches[\"randoms_shape\"], corr_type=[\"gg\", \"w\"], dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=True, data_suffix=\"_RR\", num_sample_names=[\"R_S\", \"R_D\"]) self._measure_jackknife_covariance_lightcone(IA_estimator=IA_estimator, max_patch=max(jk_patches['shape']), min_patch=min(jk_patches[\"shape\"]), corr_type=[corr_type, \"w\"], dataset_name=dataset_name, randoms_suf=\"_randoms\") self.data = data return def measure_xi_multipoles(self, IA_estimator, dataset_name, corr_type, jk_patches=None, num_jk=None, calc_errors=True, masks=None, masks_randoms=None, cosmology=None, over_h=False, rp_cut=None): \"\"\"Measures multipoles including jackknife covariance if desired for lightcone data. Manages the various _measure_xi_r_mu_r_obs and _measure_jackknife_covariance options in MeasureMultipolesObservations and MeasureJackknife. Parameters ---------- IA_estimator : str Choose which type of xi estimator is used. Choose from \"clusters\" or \"galaxies\". dataset_name : str Name of the dataset in the output file. corr_type : str Type of correlation to be measured. Choose from [g+, gg, both]. randoms_data : dict or NoneType Dictionary that includes the randoms data in the same form as the data dictionary. jk_patches : dict or NoneType, optional Dictionary with entries of the jackknife patch numbers (ndarray) for each sample, named \"position\", \"shape\" and \"random\". Default is None. num_jk : int, optional Number of jackknife patches to be generated internally. Default is None. measure_cov : bool, optional If True, jackknife errors are calculated. Default is True. masks : dict or NoneType, optional Dictionary of mask information in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. masks_randoms : dict or NoneType, optional Dictionary of mask information for the randoms data in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. cosmology : pyccl cosmology object or NoneType, optional Pyccl cosmology to use in the calculation. If None (default), the cosmology is used: ccl.Cosmology(Omega_c=0.225, Omega_b=0.045, sigma8=0.8, h=0.7, n_s=1.0) over_h : bool, optional If True, the units are assumed to be in not-over-h and converted to over-h units. Default is False. rp_cut : float or NoneType, optional Applies a minimum r_p value condition for pairs to be included. Default is None. Returns ------- \"\"\" if IA_estimator == \"clusters\": if self.randoms_data == None: print(\"No randoms given, correlation defined as S+D/DD\") raise KeyError(\"This version does not work yet, add randoms.\") else: print(\"xi_g+ defined as S+D/SD - S+R/SR, xi_gg as (SD - RD - SR)/RR - 1\") if masks != None and masks_randoms == None: print(\"Warning, masks given for data vector but not for randoms.\") elif IA_estimator == \"galaxies\": if self.randoms_data == None: raise KeyError(\"No randoms given. Please provide input.\") else: print(\"xi_g+ defined as (S+D - S+R)/RR, xi_gg as (SD - RD - SR)/RR - 1\") if masks != None and masks_randoms == None: print(\"Warning, masks given for data vector but not for randoms.\") print(\"WARNING: this version of the code has not been fully validated. Proceed with caution.\") else: raise KeyError(\"Unknown input for IA_estimator, choose from [clusters, galaxies].\") # todo: Expand to include methods with trees and internal multiproc # todo: Checks to see if data directories include everything they need data = self.data # temporary save so it can be restored at the end of the calculation try: # Are there one or two random samples given? random_shape = self.randoms_data[\"RA_shape_sample\"] one_random_sample = False except: one_random_sample = True self.randoms_data[\"RA_shape_sample\"] = self.randoms_data[\"RA\"] self.randoms_data[\"DEC_shape_sample\"] = self.randoms_data[\"DEC\"] self.randoms_data[\"Redshift_shape_sample\"] = self.randoms_data[\"Redshift\"] try: weight = self.randoms_data[\"weight\"] except: self.randoms_data[\"weight\"] = np.ones(len(self.randoms_data[\"RA\"])) try: weight = self.randoms_data[\"weight_shape_sample\"] except: if one_random_sample: self.randoms_data[\"weight_shape_sample\"] = self.randoms_data[\"weight\"] # in case weights are given else: self.randoms_data[\"weight_shape_sample\"] = np.ones(len(self.randoms_data[\"RA_shape_sample\"])) if calc_errors: if jk_patches == None: if num_jk != None: jk_patches = self.assign_jackknife_patches(data, self.randoms_data, num_jk) else: raise ValueError(\"Set calc_errors to False, or provide either jk_patches or num_jk input.\") else: if one_random_sample: jk_patches[\"randoms_position\"] = jk_patches[\"randoms\"] jk_patches[\"randoms_shape\"] = jk_patches[\"randoms\"] self.data_dir = data try: weight = self.data_dir[\"weight\"] except: self.data_dir[\"weight\"] = np.ones(len(self.data_dir[\"RA\"])) try: weight = self.data_dir[\"weight_shape_sample\"] except: self.data_dir[\"weight_shape_sample\"] = np.ones(len(self.data_dir[\"RA_shape_sample\"])) num_samples = {} # Needed to correct for different number of randoms and galaxies/clusters in data if masks == None: num_samples[\"D\"] = len(self.data_dir[\"RA\"]) num_samples[\"S\"] = len(self.data_dir[\"RA_shape_sample\"]) else: num_samples[\"D\"] = len(self.data_dir[\"RA\"][masks[\"RA\"]]) num_samples[\"S\"] = len(self.data_dir[\"RA_shape_sample\"][masks[\"RA_shape_sample\"]]) if masks_randoms == None: num_samples[\"R_D\"] = len(self.randoms_data[\"RA\"]) num_samples[\"R_S\"] = len(self.randoms_data[\"RA_shape_sample\"]) else: num_samples[\"R_D\"] = len(self.randoms_data[\"RA\"][masks_randoms[\"RA\"]]) num_samples[\"R_S\"] = len(self.randoms_data[\"RA_shape_sample\"][masks_randoms[\"RA_shape_sample\"]]) # print(self.data_dir,self.randoms_data) # Shape-position combinations: # S+D (Cg+, Gg+) # S+R (Cg+, Gg+) if corr_type == \"g+\" or corr_type == \"both\": # S+D self.data = self.data_dir self._measure_xi_r_mur_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, rp_cut=rp_cut, cosmology=cosmology) # S+R self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"e1\": self.data_dir[\"e1\"], \"e2\": self.data_dir[\"e2\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } # print(self.data) self._measure_xi_r_mur_lightcone_brute(masks=masks, dataset_name=f\"{dataset_name}_randoms\", over_h=over_h, rp_cut=rp_cut, cosmology=cosmology) # Position-position combinations: # SD (Cgg, Ggg) # SR (Cg+, Cgg, Ggg) # RD (Cgg, Ggg) # RR (Cgg, Gg+, Ggg) if corr_type == \"gg\": # already have it for 'both' # SD (Cgg, Ggg) self.data = { \"Redshift\": self.data_dir[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.data_dir[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.data_dir[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"weight\": self.data_dir[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._count_pairs_xi_r_mur_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology, data_suffix=\"_DD\", rp_cut=rp_cut) # SR (Cg+, Cgg, Ggg) - watch name (Obs estimator) # if g+ or both, already have it self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._count_pairs_xi_r_mur_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology, rp_cut=rp_cut, data_suffix=\"_SR\") if corr_type == \"gg\" or corr_type == \"both\": # RD (Cgg, Ggg) self.data = { \"Redshift\": self.data_dir[\"Redshift\"], \"Redshift_shape_sample\": self.randoms_data[\"Redshift_shape_sample\"], \"RA\": self.data_dir[\"RA\"], \"RA_shape_sample\": self.randoms_data[\"RA_shape_sample\"], \"DEC\": self.data_dir[\"DEC\"], \"DEC_shape_sample\": self.randoms_data[\"DEC_shape_sample\"], \"weight\": self.data_dir[\"weight\"], \"weight_shape_sample\": self.randoms_data[\"weight_shape_sample\"] } self._count_pairs_xi_r_mur_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology, rp_cut=rp_cut, data_suffix=\"_RD\") if IA_estimator == \"galaxies\" or corr_type == \"gg\" or corr_type == \"both\": # RR (Cgg, Gg+, Ggg) self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.randoms_data[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.randoms_data[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.randoms_data[\"DEC_shape_sample\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.randoms_data[\"weight_shape_sample\"] } self._count_pairs_xi_r_mur_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology, rp_cut=rp_cut, data_suffix=\"_RR\") self._obs_estimator([corr_type, \"multipoles\"], IA_estimator, dataset_name, f\"{dataset_name}_randoms\", num_samples) self._measure_multipoles(corr_type=corr_type, dataset_name=dataset_name, return_output=False) if calc_errors: self.num_samples = {} min_patch, max_patch = int(min(jk_patches[\"shape\"])), int(max(jk_patches[\"shape\"])) for n in np.arange(min_patch, max_patch + 1): self.num_samples[f\"{n}\"] = {} # Shape-position combinations: # S+D (Cg+, Gg+) # S+R (Cg+, Gg+) if corr_type == \"g+\" or corr_type == \"both\": # S+D self.data = self.data_dir self._measure_jackknife_realisations_lightcone_multiprocessing(patches_pos=jk_patches[\"position\"], patches_shape=jk_patches[\"shape\"], corr_type=[corr_type, \"multipoles\"], masks=masks, dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, rp_cut=rp_cut, cosmology=cosmology, count_pairs=False, num_sample_names=[\"S\", \"D\"]) # S+R self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"e1\": self.data_dir[\"e1\"], \"e2\": self.data_dir[\"e2\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing( patches_pos=jk_patches[\"randoms_position\"], patches_shape=jk_patches[\"shape\"], corr_type=[corr_type, \"multipoles\"], masks=masks, dataset_name=f\"{dataset_name}_randoms\", num_nodes=self.num_nodes, over_h=over_h, rp_cut=rp_cut, cosmology=cosmology, count_pairs=False, num_sample_names=[\"S\", \"R_D\"]) # Position-position combinations: # SD (Cgg, Ggg) # SR (Cg+, Cgg, Ggg) # RD (Cgg, Ggg) # RR (Cgg, Gg+, Ggg) if corr_type == \"gg\": # already have it for 'both' # SD (Cgg, Ggg) self.data = { \"Redshift\": self.data_dir[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.data_dir[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.data_dir[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"weight\": self.data_dir[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing(patches_pos=jk_patches[\"position\"], patches_shape=jk_patches[\"shape\"], corr_type=[\"gg\", \"multipoles\"], dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=True, rp_cut=rp_cut, data_suffix=\"_DD\", num_sample_names=[\"S\", \"D\"]) # SR (Cg+, Cgg, Ggg) - watch name (Obs estimator) # if g+ or both, already have it self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing( patches_pos=jk_patches[\"randoms_position\"], patches_shape=jk_patches[\"shape\"], corr_type=[\"gg\", \"multipoles\"], dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=True, rp_cut=rp_cut, data_suffix=\"_SR\", num_sample_names=[\"S\", \"R_D\"]) if corr_type == \"gg\" or corr_type == \"both\": # RD (Cgg, Ggg) self.data = { \"Redshift\": self.data_dir[\"Redshift\"], \"Redshift_shape_sample\": self.randoms_data[\"Redshift_shape_sample\"], \"RA\": self.data_dir[\"RA\"], \"RA_shape_sample\": self.randoms_data[\"RA_shape_sample\"], \"DEC\": self.data_dir[\"DEC\"], \"DEC_shape_sample\": self.randoms_data[\"DEC_shape_sample\"], \"weight\": self.data_dir[\"weight\"], \"weight_shape_sample\": self.randoms_data[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing(patches_pos=jk_patches[\"position\"], patches_shape=jk_patches[ \"randoms_shape\"], corr_type=[\"gg\", \"multipoles\"], dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=True, rp_cut=rp_cut, data_suffix=\"_RD\", num_sample_names=[\"R_S\", \"D\"]) if IA_estimator == \"galaxies\" or corr_type == \"gg\" or corr_type == \"both\": # RR (Cgg, Gg+, Ggg) self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.randoms_data[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.randoms_data[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.randoms_data[\"DEC_shape_sample\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.randoms_data[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing( patches_pos=jk_patches[\"randoms_position\"], patches_shape=jk_patches[\"randoms_shape\"], corr_type=[\"gg\", \"multipoles\"], dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=True, rp_cut=rp_cut, data_suffix=\"_RR\", num_sample_names=[\"R_S\", \"R_D\"]) self._measure_jackknife_covariance_lightcone(IA_estimator=IA_estimator, max_patch=max(jk_patches['shape']), min_patch=min(jk_patches[\"shape\"]), corr_type=[corr_type, \"multipoles\"], dataset_name=dataset_name, randoms_suf=\"_randoms\") self.data = data return __init__(data, randoms_data, output_file_name, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, num_nodes=1) The init method of the MeasureIALightcone class. Parameters: randoms_data ( dict or NoneType ) \u2013 Dictionary with data of the randoms needed for lightcone-type measurements. The keywords are: 'Redshift' and 'Redshift_shape_sample': (N_p) and (N_s) ndarray with redshifts of position and shape samples. 'RA' and 'RA_shape_sample': (N_p) and (N_s) ndarray with RA coordinate of position and shape samples. 'DEC' and 'DEC_shape_sample': (N_p) and (N_s) ndarray with DEC coordinate of position and shape samples. If only 'Redshift', 'RA' and 'DEC' are added, the sample will be used for both position and shape sample randoms. num_nodes ( int , default: 1 ) \u2013 Number of cores to be used in multiprocessing. Default is 1. Notes Constructor parameters 'data', 'output_file_name', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', are passed to MeasureIABase. Source code in src/measureia/measure_IA.py def __init__( self, data, randoms_data, output_file_name, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, num_nodes=1, ): \"\"\" The __init__ method of the MeasureIALightcone class. Parameters ---------- randoms_data : dict or NoneType Dictionary with data of the randoms needed for lightcone-type measurements. The keywords are: 'Redshift' and 'Redshift_shape_sample': (N_p) and (N_s) ndarray with redshifts of position and shape samples. 'RA' and 'RA_shape_sample': (N_p) and (N_s) ndarray with RA coordinate of position and shape samples. 'DEC' and 'DEC_shape_sample': (N_p) and (N_s) ndarray with DEC coordinate of position and shape samples. If only 'Redshift', 'RA' and 'DEC' are added, the sample will be used for both position and shape sample randoms. num_nodes : int, optional Number of cores to be used in multiprocessing. Default is 1. Notes ----- Constructor parameters 'data', 'output_file_name', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', are passed to MeasureIABase. \"\"\" super().__init__(data, output_file_name, False, None, separation_limits, num_bins_r, num_bins_pi, pi_max, None, False) self.num_nodes = num_nodes self.randoms_data = randoms_data self.data_dir = None self.num_samples = None return measure_xi_w(IA_estimator, dataset_name, corr_type, jk_patches=None, num_jk=None, measure_cov=True, masks=None, masks_randoms=None, cosmology=None, over_h=False) Measures xi_gg, xi_g+ and w_gg, w_g+ including jackknife covariance if desired for lightcone data. Manages the various _measure_xi_rp_pi_obs and _measure_jackknife_covariance options in MeasureWObservations and MeasureJackknife. Parameters: IA_estimator ( str ) \u2013 Choose which type of xi estimator is used. Choose from \"clusters\" or \"galaxies\". dataset_name ( str ) \u2013 Name of the dataset in the output file. corr_type ( str ) \u2013 Type of correlation to be measured. Choose from [g+, gg, both]. randoms_data ( dict or NoneType ) \u2013 Dictionary that includes the randoms data in the same form as the data dictionary. jk_patches ( dict or NoneType , default: None ) \u2013 Dictionary with entries of the jackknife patch numbers (ndarray) for each sample, named \"position\", \"shape\" and \"random\". Default is None. num_jk ( int , default: None ) \u2013 Number of jackknife patches to be generated internally. Default is None. measure_cov ( bool , default: True ) \u2013 If True, jackknife errors are calculated. Default is True. masks ( dict or NoneType , default: None ) \u2013 Dictionary of mask information in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. masks_randoms ( dict or NoneType , default: None ) \u2013 Dictionary of mask information for the randoms data in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. cosmology ( pyccl cosmology object or NoneType , default: None ) \u2013 Pyccl cosmology to use in the calculation. If None (default), the cosmology is used: ccl.Cosmology(Omega_c=0.225, Omega_b=0.045, sigma8=0.8, h=0.7, n_s=1.0) over_h ( bool , default: False ) \u2013 If True, the units are assumed to be in not-over-h and converted to over-h units. Default is False. Source code in src/measureia/measure_IA.py def measure_xi_w(self, IA_estimator, dataset_name, corr_type, jk_patches=None, num_jk=None, measure_cov=True, masks=None, masks_randoms=None, cosmology=None, over_h=False): \"\"\"Measures xi_gg, xi_g+ and w_gg, w_g+ including jackknife covariance if desired for lightcone data. Manages the various _measure_xi_rp_pi_obs and _measure_jackknife_covariance options in MeasureWObservations and MeasureJackknife. Parameters ---------- IA_estimator : str Choose which type of xi estimator is used. Choose from \"clusters\" or \"galaxies\". dataset_name : str Name of the dataset in the output file. corr_type : str Type of correlation to be measured. Choose from [g+, gg, both]. randoms_data : dict or NoneType Dictionary that includes the randoms data in the same form as the data dictionary. jk_patches : dict or NoneType, optional Dictionary with entries of the jackknife patch numbers (ndarray) for each sample, named \"position\", \"shape\" and \"random\". Default is None. num_jk : int, optional Number of jackknife patches to be generated internally. Default is None. measure_cov : bool, optional If True, jackknife errors are calculated. Default is True. masks : dict or NoneType, optional Dictionary of mask information in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. masks_randoms : dict or NoneType, optional Dictionary of mask information for the randoms data in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. cosmology : pyccl cosmology object or NoneType, optional Pyccl cosmology to use in the calculation. If None (default), the cosmology is used: ccl.Cosmology(Omega_c=0.225, Omega_b=0.045, sigma8=0.8, h=0.7, n_s=1.0) over_h : bool, optional If True, the units are assumed to be in not-over-h and converted to over-h units. Default is False. \"\"\" if IA_estimator == \"clusters\": if self.randoms_data == None: print(\"No randoms given, correlation defined as S+D/DD\") raise KeyError(\"This version does not work yet, add randoms.\") else: print(\"xi_g+ defined as S+D/SD - S+R/SR, xi_gg as (SD - RD - SR)/RR - 1\") if masks != None and masks_randoms == None: print(\"Warning, masks given for data vector but not for randoms.\") elif IA_estimator == \"galaxies\": if self.randoms_data == None: raise KeyError(\"No randoms given. Please provide input.\") else: print(\"xi_g+ defined as (S+D - S+R)/RR, xi_gg as (SD - RD - SR)/RR - 1\") if masks != None and masks_randoms == None: print(\"Warning, masks given for data vector but not for randoms.\") print(\"WARNING: this version of the code has not been fully validated. Proceed with caution.\") else: raise KeyError(\"Unknown input for IA_estimator, choose from [clusters, galaxies].\") # todo: Expand to include methods with trees and internal multiproc # todo: Checks to see if data directories include everything they need data = self.data # temporary save so it can be restored at the end of the calculation try: # Are there one or two random samples given? random_shape = self.randoms_data[\"RA_shape_sample\"] one_random_sample = False except: one_random_sample = True self.randoms_data[\"RA_shape_sample\"] = self.randoms_data[\"RA\"] self.randoms_data[\"DEC_shape_sample\"] = self.randoms_data[\"DEC\"] self.randoms_data[\"Redshift_shape_sample\"] = self.randoms_data[\"Redshift\"] try: weight = self.randoms_data[\"weight\"] except: self.randoms_data[\"weight\"] = np.ones(len(self.randoms_data[\"RA\"])) try: weight = self.randoms_data[\"weight_shape_sample\"] except: if one_random_sample: self.randoms_data[\"weight_shape_sample\"] = self.randoms_data[\"weight\"] # in case weights are given else: self.randoms_data[\"weight_shape_sample\"] = np.ones(len(self.randoms_data[\"RA_shape_sample\"])) if measure_cov: if jk_patches == None: if num_jk != None: jk_patches = self.assign_jackknife_patches(data, self.randoms_data, num_jk) else: raise ValueError(\"Set calc_errors to False, or provide either jk_patches or num_jk input.\") else: if one_random_sample: jk_patches[\"randoms_position\"] = jk_patches[\"randoms\"] jk_patches[\"randoms_shape\"] = jk_patches[\"randoms\"] self.data_dir = data try: weight = self.data_dir[\"weight\"] except: self.data_dir[\"weight\"] = np.ones(len(self.data_dir[\"RA\"])) try: weight = self.data_dir[\"weight_shape_sample\"] except: self.data_dir[\"weight_shape_sample\"] = np.ones(len(self.data_dir[\"RA_shape_sample\"])) num_samples = {} # Needed to correct for different number of randoms and galaxies/clusters in data if masks == None: num_samples[\"D\"] = len(self.data_dir[\"RA\"]) num_samples[\"S\"] = len(self.data_dir[\"RA_shape_sample\"]) else: num_samples[\"D\"] = len(self.data_dir[\"RA\"][masks[\"RA\"]]) num_samples[\"S\"] = len(self.data_dir[\"RA_shape_sample\"][masks[\"RA_shape_sample\"]]) if masks_randoms == None: num_samples[\"R_D\"] = len(self.randoms_data[\"RA\"]) num_samples[\"R_S\"] = len(self.randoms_data[\"RA_shape_sample\"]) else: num_samples[\"R_D\"] = len(self.randoms_data[\"RA\"][masks_randoms[\"RA\"]]) num_samples[\"R_S\"] = len(self.randoms_data[\"RA_shape_sample\"][masks_randoms[\"RA_shape_sample\"]]) # print(self.data_dir,self.randoms_data) # Shape-position combinations: # S+D (Cg+, Gg+) # S+R (Cg+, Gg+) if corr_type == \"g+\" or corr_type == \"both\": # S+D self.data = self.data_dir self._measure_xi_rp_pi_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology) # S+R self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"e1\": self.data_dir[\"e1\"], \"e2\": self.data_dir[\"e2\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } # print(self.data) self._measure_xi_rp_pi_lightcone_brute(masks=masks, dataset_name=f\"{dataset_name}_randoms\", over_h=over_h, cosmology=cosmology) # Position-position combinations: # SD (Cgg, Ggg) # SR (Cg+, Cgg, Ggg) # RD (Cgg, Ggg) # RR (Cgg, Gg+, Ggg) if corr_type == \"gg\": # already have it for 'both' # SD (Cgg, Ggg) self.data = { \"Redshift\": self.data_dir[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.data_dir[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.data_dir[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"weight\": self.data_dir[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._count_pairs_xi_rp_pi_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology, data_suffix=\"_DD\") # SR (Cg+, Cgg, Ggg) - watch name (Obs estimator) # if g+ or both, already have it self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._count_pairs_xi_rp_pi_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology, data_suffix=\"_SR\") if corr_type == \"gg\" or corr_type == \"both\": # RD (Cgg, Ggg) self.data = { \"Redshift\": self.data_dir[\"Redshift\"], \"Redshift_shape_sample\": self.randoms_data[\"Redshift_shape_sample\"], \"RA\": self.data_dir[\"RA\"], \"RA_shape_sample\": self.randoms_data[\"RA_shape_sample\"], \"DEC\": self.data_dir[\"DEC\"], \"DEC_shape_sample\": self.randoms_data[\"DEC_shape_sample\"], \"weight\": self.data_dir[\"weight\"], \"weight_shape_sample\": self.randoms_data[\"weight_shape_sample\"] } self._count_pairs_xi_rp_pi_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology, data_suffix=\"_RD\") if IA_estimator == \"galaxies\" or corr_type == \"gg\" or corr_type == \"both\": # RR (Cgg, Gg+, Ggg) self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.randoms_data[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.randoms_data[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.randoms_data[\"DEC_shape_sample\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.randoms_data[\"weight_shape_sample\"] } self._count_pairs_xi_rp_pi_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology, data_suffix=\"_RR\") self._obs_estimator([corr_type, \"w\"], IA_estimator, dataset_name, f\"{dataset_name}_randoms\", num_samples) self._measure_w_g_i(corr_type=corr_type, dataset_name=dataset_name, return_output=False) if measure_cov: self.num_samples = {} min_patch, max_patch = int(min(jk_patches[\"shape\"])), int(max(jk_patches[\"shape\"])) for n in np.arange(min_patch, max_patch + 1): self.num_samples[f\"{n}\"] = {} # Shape-position combinations: # S+D (Cg+, Gg+) # S+R (Cg+, Gg+) if corr_type == \"g+\" or corr_type == \"both\": # S+D self.data = self.data_dir self._measure_jackknife_realisations_lightcone_multiprocessing(patches_pos=jk_patches[\"position\"], patches_shape=jk_patches[\"shape\"], corr_type=[corr_type, \"w\"], masks=masks, dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=False, num_sample_names=[\"S\", \"D\"]) # S+R self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"e1\": self.data_dir[\"e1\"], \"e2\": self.data_dir[\"e2\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } # print(self.data) self._measure_jackknife_realisations_lightcone_multiprocessing( patches_pos=jk_patches[\"randoms_position\"], patches_shape=jk_patches[\"shape\"], corr_type=[corr_type, \"w\"], masks=masks, dataset_name=f\"{dataset_name}_randoms\", num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=False, num_sample_names=[\"S\", \"R_D\"]) # Position-position combinations: # SD (Cgg, Ggg) # SR (Cg+, Cgg, Ggg) # RD (Cgg, Ggg) # RR (Cgg, Gg+, Ggg) if corr_type == \"gg\": # already have it for 'both' # SD (Cgg, Ggg) self.data = { \"Redshift\": self.data_dir[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.data_dir[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.data_dir[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"weight\": self.data_dir[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing(patches_pos=jk_patches[\"position\"], patches_shape=jk_patches[\"shape\"], corr_type=[\"gg\", \"w\"], dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=True, data_suffix=\"_DD\", num_sample_names=[\"S\", \"D\"]) # SR (Cg+, Cgg, Ggg) - watch name (Obs estimator) # if g+ or both, already have it self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing( patches_pos=jk_patches[\"randoms_position\"], patches_shape=jk_patches[\"shape\"], corr_type=[\"gg\", \"w\"], dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=True, data_suffix=\"_SR\", num_sample_names=[\"S\", \"R_D\"]) if corr_type == \"gg\" or corr_type == \"both\": # RD (Cgg, Ggg) self.data = { \"Redshift\": self.data_dir[\"Redshift\"], \"Redshift_shape_sample\": self.randoms_data[\"Redshift_shape_sample\"], \"RA\": self.data_dir[\"RA\"], \"RA_shape_sample\": self.randoms_data[\"RA_shape_sample\"], \"DEC\": self.data_dir[\"DEC\"], \"DEC_shape_sample\": self.randoms_data[\"DEC_shape_sample\"], \"weight\": self.data_dir[\"weight\"], \"weight_shape_sample\": self.randoms_data[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing(patches_pos=jk_patches[\"position\"], patches_shape=jk_patches[ \"randoms_shape\"], corr_type=[\"gg\", \"w\"], dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=True, data_suffix=\"_RD\", num_sample_names=[\"R_S\", \"D\"]) if IA_estimator == \"galaxies\" or corr_type == \"gg\" or corr_type == \"both\": # RR (Cgg, Gg+, Ggg) self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.randoms_data[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.randoms_data[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.randoms_data[\"DEC_shape_sample\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.randoms_data[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing( patches_pos=jk_patches[\"randoms_position\"], patches_shape=jk_patches[\"randoms_shape\"], corr_type=[\"gg\", \"w\"], dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=True, data_suffix=\"_RR\", num_sample_names=[\"R_S\", \"R_D\"]) self._measure_jackknife_covariance_lightcone(IA_estimator=IA_estimator, max_patch=max(jk_patches['shape']), min_patch=min(jk_patches[\"shape\"]), corr_type=[corr_type, \"w\"], dataset_name=dataset_name, randoms_suf=\"_randoms\") self.data = data return measure_xi_multipoles(IA_estimator, dataset_name, corr_type, jk_patches=None, num_jk=None, calc_errors=True, masks=None, masks_randoms=None, cosmology=None, over_h=False, rp_cut=None) Measures multipoles including jackknife covariance if desired for lightcone data. Manages the various _measure_xi_r_mu_r_obs and _measure_jackknife_covariance options in MeasureMultipolesObservations and MeasureJackknife. Parameters: IA_estimator ( str ) \u2013 Choose which type of xi estimator is used. Choose from \"clusters\" or \"galaxies\". dataset_name ( str ) \u2013 Name of the dataset in the output file. corr_type ( str ) \u2013 Type of correlation to be measured. Choose from [g+, gg, both]. randoms_data ( dict or NoneType ) \u2013 Dictionary that includes the randoms data in the same form as the data dictionary. jk_patches ( dict or NoneType , default: None ) \u2013 Dictionary with entries of the jackknife patch numbers (ndarray) for each sample, named \"position\", \"shape\" and \"random\". Default is None. num_jk ( int , default: None ) \u2013 Number of jackknife patches to be generated internally. Default is None. measure_cov ( bool ) \u2013 If True, jackknife errors are calculated. Default is True. masks ( dict or NoneType , default: None ) \u2013 Dictionary of mask information in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. masks_randoms ( dict or NoneType , default: None ) \u2013 Dictionary of mask information for the randoms data in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. cosmology ( pyccl cosmology object or NoneType , default: None ) \u2013 Pyccl cosmology to use in the calculation. If None (default), the cosmology is used: ccl.Cosmology(Omega_c=0.225, Omega_b=0.045, sigma8=0.8, h=0.7, n_s=1.0) over_h ( bool , default: False ) \u2013 If True, the units are assumed to be in not-over-h and converted to over-h units. Default is False. rp_cut ( float or NoneType , default: None ) \u2013 Applies a minimum r_p value condition for pairs to be included. Default is None. Source code in src/measureia/measure_IA.py def measure_xi_multipoles(self, IA_estimator, dataset_name, corr_type, jk_patches=None, num_jk=None, calc_errors=True, masks=None, masks_randoms=None, cosmology=None, over_h=False, rp_cut=None): \"\"\"Measures multipoles including jackknife covariance if desired for lightcone data. Manages the various _measure_xi_r_mu_r_obs and _measure_jackknife_covariance options in MeasureMultipolesObservations and MeasureJackknife. Parameters ---------- IA_estimator : str Choose which type of xi estimator is used. Choose from \"clusters\" or \"galaxies\". dataset_name : str Name of the dataset in the output file. corr_type : str Type of correlation to be measured. Choose from [g+, gg, both]. randoms_data : dict or NoneType Dictionary that includes the randoms data in the same form as the data dictionary. jk_patches : dict or NoneType, optional Dictionary with entries of the jackknife patch numbers (ndarray) for each sample, named \"position\", \"shape\" and \"random\". Default is None. num_jk : int, optional Number of jackknife patches to be generated internally. Default is None. measure_cov : bool, optional If True, jackknife errors are calculated. Default is True. masks : dict or NoneType, optional Dictionary of mask information in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. masks_randoms : dict or NoneType, optional Dictionary of mask information for the randoms data in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. cosmology : pyccl cosmology object or NoneType, optional Pyccl cosmology to use in the calculation. If None (default), the cosmology is used: ccl.Cosmology(Omega_c=0.225, Omega_b=0.045, sigma8=0.8, h=0.7, n_s=1.0) over_h : bool, optional If True, the units are assumed to be in not-over-h and converted to over-h units. Default is False. rp_cut : float or NoneType, optional Applies a minimum r_p value condition for pairs to be included. Default is None. Returns ------- \"\"\" if IA_estimator == \"clusters\": if self.randoms_data == None: print(\"No randoms given, correlation defined as S+D/DD\") raise KeyError(\"This version does not work yet, add randoms.\") else: print(\"xi_g+ defined as S+D/SD - S+R/SR, xi_gg as (SD - RD - SR)/RR - 1\") if masks != None and masks_randoms == None: print(\"Warning, masks given for data vector but not for randoms.\") elif IA_estimator == \"galaxies\": if self.randoms_data == None: raise KeyError(\"No randoms given. Please provide input.\") else: print(\"xi_g+ defined as (S+D - S+R)/RR, xi_gg as (SD - RD - SR)/RR - 1\") if masks != None and masks_randoms == None: print(\"Warning, masks given for data vector but not for randoms.\") print(\"WARNING: this version of the code has not been fully validated. Proceed with caution.\") else: raise KeyError(\"Unknown input for IA_estimator, choose from [clusters, galaxies].\") # todo: Expand to include methods with trees and internal multiproc # todo: Checks to see if data directories include everything they need data = self.data # temporary save so it can be restored at the end of the calculation try: # Are there one or two random samples given? random_shape = self.randoms_data[\"RA_shape_sample\"] one_random_sample = False except: one_random_sample = True self.randoms_data[\"RA_shape_sample\"] = self.randoms_data[\"RA\"] self.randoms_data[\"DEC_shape_sample\"] = self.randoms_data[\"DEC\"] self.randoms_data[\"Redshift_shape_sample\"] = self.randoms_data[\"Redshift\"] try: weight = self.randoms_data[\"weight\"] except: self.randoms_data[\"weight\"] = np.ones(len(self.randoms_data[\"RA\"])) try: weight = self.randoms_data[\"weight_shape_sample\"] except: if one_random_sample: self.randoms_data[\"weight_shape_sample\"] = self.randoms_data[\"weight\"] # in case weights are given else: self.randoms_data[\"weight_shape_sample\"] = np.ones(len(self.randoms_data[\"RA_shape_sample\"])) if calc_errors: if jk_patches == None: if num_jk != None: jk_patches = self.assign_jackknife_patches(data, self.randoms_data, num_jk) else: raise ValueError(\"Set calc_errors to False, or provide either jk_patches or num_jk input.\") else: if one_random_sample: jk_patches[\"randoms_position\"] = jk_patches[\"randoms\"] jk_patches[\"randoms_shape\"] = jk_patches[\"randoms\"] self.data_dir = data try: weight = self.data_dir[\"weight\"] except: self.data_dir[\"weight\"] = np.ones(len(self.data_dir[\"RA\"])) try: weight = self.data_dir[\"weight_shape_sample\"] except: self.data_dir[\"weight_shape_sample\"] = np.ones(len(self.data_dir[\"RA_shape_sample\"])) num_samples = {} # Needed to correct for different number of randoms and galaxies/clusters in data if masks == None: num_samples[\"D\"] = len(self.data_dir[\"RA\"]) num_samples[\"S\"] = len(self.data_dir[\"RA_shape_sample\"]) else: num_samples[\"D\"] = len(self.data_dir[\"RA\"][masks[\"RA\"]]) num_samples[\"S\"] = len(self.data_dir[\"RA_shape_sample\"][masks[\"RA_shape_sample\"]]) if masks_randoms == None: num_samples[\"R_D\"] = len(self.randoms_data[\"RA\"]) num_samples[\"R_S\"] = len(self.randoms_data[\"RA_shape_sample\"]) else: num_samples[\"R_D\"] = len(self.randoms_data[\"RA\"][masks_randoms[\"RA\"]]) num_samples[\"R_S\"] = len(self.randoms_data[\"RA_shape_sample\"][masks_randoms[\"RA_shape_sample\"]]) # print(self.data_dir,self.randoms_data) # Shape-position combinations: # S+D (Cg+, Gg+) # S+R (Cg+, Gg+) if corr_type == \"g+\" or corr_type == \"both\": # S+D self.data = self.data_dir self._measure_xi_r_mur_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, rp_cut=rp_cut, cosmology=cosmology) # S+R self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"e1\": self.data_dir[\"e1\"], \"e2\": self.data_dir[\"e2\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } # print(self.data) self._measure_xi_r_mur_lightcone_brute(masks=masks, dataset_name=f\"{dataset_name}_randoms\", over_h=over_h, rp_cut=rp_cut, cosmology=cosmology) # Position-position combinations: # SD (Cgg, Ggg) # SR (Cg+, Cgg, Ggg) # RD (Cgg, Ggg) # RR (Cgg, Gg+, Ggg) if corr_type == \"gg\": # already have it for 'both' # SD (Cgg, Ggg) self.data = { \"Redshift\": self.data_dir[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.data_dir[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.data_dir[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"weight\": self.data_dir[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._count_pairs_xi_r_mur_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology, data_suffix=\"_DD\", rp_cut=rp_cut) # SR (Cg+, Cgg, Ggg) - watch name (Obs estimator) # if g+ or both, already have it self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._count_pairs_xi_r_mur_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology, rp_cut=rp_cut, data_suffix=\"_SR\") if corr_type == \"gg\" or corr_type == \"both\": # RD (Cgg, Ggg) self.data = { \"Redshift\": self.data_dir[\"Redshift\"], \"Redshift_shape_sample\": self.randoms_data[\"Redshift_shape_sample\"], \"RA\": self.data_dir[\"RA\"], \"RA_shape_sample\": self.randoms_data[\"RA_shape_sample\"], \"DEC\": self.data_dir[\"DEC\"], \"DEC_shape_sample\": self.randoms_data[\"DEC_shape_sample\"], \"weight\": self.data_dir[\"weight\"], \"weight_shape_sample\": self.randoms_data[\"weight_shape_sample\"] } self._count_pairs_xi_r_mur_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology, rp_cut=rp_cut, data_suffix=\"_RD\") if IA_estimator == \"galaxies\" or corr_type == \"gg\" or corr_type == \"both\": # RR (Cgg, Gg+, Ggg) self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.randoms_data[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.randoms_data[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.randoms_data[\"DEC_shape_sample\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.randoms_data[\"weight_shape_sample\"] } self._count_pairs_xi_r_mur_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology, rp_cut=rp_cut, data_suffix=\"_RR\") self._obs_estimator([corr_type, \"multipoles\"], IA_estimator, dataset_name, f\"{dataset_name}_randoms\", num_samples) self._measure_multipoles(corr_type=corr_type, dataset_name=dataset_name, return_output=False) if calc_errors: self.num_samples = {} min_patch, max_patch = int(min(jk_patches[\"shape\"])), int(max(jk_patches[\"shape\"])) for n in np.arange(min_patch, max_patch + 1): self.num_samples[f\"{n}\"] = {} # Shape-position combinations: # S+D (Cg+, Gg+) # S+R (Cg+, Gg+) if corr_type == \"g+\" or corr_type == \"both\": # S+D self.data = self.data_dir self._measure_jackknife_realisations_lightcone_multiprocessing(patches_pos=jk_patches[\"position\"], patches_shape=jk_patches[\"shape\"], corr_type=[corr_type, \"multipoles\"], masks=masks, dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, rp_cut=rp_cut, cosmology=cosmology, count_pairs=False, num_sample_names=[\"S\", \"D\"]) # S+R self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"e1\": self.data_dir[\"e1\"], \"e2\": self.data_dir[\"e2\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing( patches_pos=jk_patches[\"randoms_position\"], patches_shape=jk_patches[\"shape\"], corr_type=[corr_type, \"multipoles\"], masks=masks, dataset_name=f\"{dataset_name}_randoms\", num_nodes=self.num_nodes, over_h=over_h, rp_cut=rp_cut, cosmology=cosmology, count_pairs=False, num_sample_names=[\"S\", \"R_D\"]) # Position-position combinations: # SD (Cgg, Ggg) # SR (Cg+, Cgg, Ggg) # RD (Cgg, Ggg) # RR (Cgg, Gg+, Ggg) if corr_type == \"gg\": # already have it for 'both' # SD (Cgg, Ggg) self.data = { \"Redshift\": self.data_dir[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.data_dir[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.data_dir[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"weight\": self.data_dir[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing(patches_pos=jk_patches[\"position\"], patches_shape=jk_patches[\"shape\"], corr_type=[\"gg\", \"multipoles\"], dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=True, rp_cut=rp_cut, data_suffix=\"_DD\", num_sample_names=[\"S\", \"D\"]) # SR (Cg+, Cgg, Ggg) - watch name (Obs estimator) # if g+ or both, already have it self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing( patches_pos=jk_patches[\"randoms_position\"], patches_shape=jk_patches[\"shape\"], corr_type=[\"gg\", \"multipoles\"], dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=True, rp_cut=rp_cut, data_suffix=\"_SR\", num_sample_names=[\"S\", \"R_D\"]) if corr_type == \"gg\" or corr_type == \"both\": # RD (Cgg, Ggg) self.data = { \"Redshift\": self.data_dir[\"Redshift\"], \"Redshift_shape_sample\": self.randoms_data[\"Redshift_shape_sample\"], \"RA\": self.data_dir[\"RA\"], \"RA_shape_sample\": self.randoms_data[\"RA_shape_sample\"], \"DEC\": self.data_dir[\"DEC\"], \"DEC_shape_sample\": self.randoms_data[\"DEC_shape_sample\"], \"weight\": self.data_dir[\"weight\"], \"weight_shape_sample\": self.randoms_data[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing(patches_pos=jk_patches[\"position\"], patches_shape=jk_patches[ \"randoms_shape\"], corr_type=[\"gg\", \"multipoles\"], dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=True, rp_cut=rp_cut, data_suffix=\"_RD\", num_sample_names=[\"R_S\", \"D\"]) if IA_estimator == \"galaxies\" or corr_type == \"gg\" or corr_type == \"both\": # RR (Cgg, Gg+, Ggg) self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.randoms_data[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.randoms_data[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.randoms_data[\"DEC_shape_sample\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.randoms_data[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing( patches_pos=jk_patches[\"randoms_position\"], patches_shape=jk_patches[\"randoms_shape\"], corr_type=[\"gg\", \"multipoles\"], dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=True, rp_cut=rp_cut, data_suffix=\"_RR\", num_sample_names=[\"R_S\", \"R_D\"]) self._measure_jackknife_covariance_lightcone(IA_estimator=IA_estimator, max_patch=max(jk_patches['shape']), min_patch=min(jk_patches[\"shape\"]), corr_type=[corr_type, \"multipoles\"], dataset_name=dataset_name, randoms_suf=\"_randoms\") self.data = data return","title":"MeasureIALightcone"},{"location":"api/measureIALightcone/#measure","text":"","title":"Measure"},{"location":"api/measureIALightcone/#measureia.MeasureIALightcone","text":"Bases: MeasureJackknife Manages the IA correlation function measurement methods used in the MeasureIA package based on speed and input. This class is used to call the methods that measure w_gg, w_g+ and multipoles for simulations (and observations), with lightcone data. Depending on the input parameters, various correlations incl covariance estimates are measured for given data. Attributes: data_dir ( dict or NoneType ) \u2013 Temporary storage space for added data directory to allow for flexibility in passing data or randoms to internal methods. num_samples ( dict or NoneType ) \u2013 Dictionary containing the numbers of objects for each sample for lightcone-type measurements. Filled internally, no input needed. Methods: Name Description measure_xi_w Compute projected correlations \\(w_{gg}\\) and/or \\(w_{g+}\\) . measure_xi_multipoles Compute multipoles of the correlation functions, \\(\\tilde{\\xi}_{gg,0}\\) and/or \\(\\tilde{\\xi}_{g+,2}\\) . Notes Inherits attributes from 'SimInfo', where none are used in this class. Inherits attributes from 'MeasureIABase', where 'data', 'output_file_name', 'Num_position', 'Num_shape', 'r_min', 'r_max', 'num_bins_r', 'num_bins_pi', 'r_bins', 'pi_bins', 'mu_r_bins' are used. Source code in src/measureia/measure_IA.py class MeasureIALightcone(MeasureJackknife): r\"\"\"Manages the IA correlation function measurement methods used in the MeasureIA package based on speed and input. This class is used to call the methods that measure w_gg, w_g+ and multipoles for simulations (and observations), with lightcone data. Depending on the input parameters, various correlations incl covariance estimates are measured for given data. Attributes ---------- data_dir : dict or NoneType Temporary storage space for added data directory to allow for flexibility in passing data or randoms to internal methods. num_samples : dict or NoneType Dictionary containing the numbers of objects for each sample for lightcone-type measurements. Filled internally, no input needed. Methods ------- measure_xi_w() Compute projected correlations $w_{gg}$ and/or $w_{g+}$. measure_xi_multipoles() Compute multipoles of the correlation functions, $\\tilde{\\xi}_{gg,0}$ and/or $\\tilde{\\xi}_{g+,2}$. Notes ----- Inherits attributes from 'SimInfo', where none are used in this class. Inherits attributes from 'MeasureIABase', where 'data', 'output_file_name', 'Num_position', 'Num_shape', 'r_min', 'r_max', 'num_bins_r', 'num_bins_pi', 'r_bins', 'pi_bins', 'mu_r_bins' are used. \"\"\" def __init__( self, data, randoms_data, output_file_name, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, num_nodes=1, ): \"\"\" The __init__ method of the MeasureIALightcone class. Parameters ---------- randoms_data : dict or NoneType Dictionary with data of the randoms needed for lightcone-type measurements. The keywords are: 'Redshift' and 'Redshift_shape_sample': (N_p) and (N_s) ndarray with redshifts of position and shape samples. 'RA' and 'RA_shape_sample': (N_p) and (N_s) ndarray with RA coordinate of position and shape samples. 'DEC' and 'DEC_shape_sample': (N_p) and (N_s) ndarray with DEC coordinate of position and shape samples. If only 'Redshift', 'RA' and 'DEC' are added, the sample will be used for both position and shape sample randoms. num_nodes : int, optional Number of cores to be used in multiprocessing. Default is 1. Notes ----- Constructor parameters 'data', 'output_file_name', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', are passed to MeasureIABase. \"\"\" super().__init__(data, output_file_name, False, None, separation_limits, num_bins_r, num_bins_pi, pi_max, None, False) self.num_nodes = num_nodes self.randoms_data = randoms_data self.data_dir = None self.num_samples = None return def measure_xi_w(self, IA_estimator, dataset_name, corr_type, jk_patches=None, num_jk=None, measure_cov=True, masks=None, masks_randoms=None, cosmology=None, over_h=False): \"\"\"Measures xi_gg, xi_g+ and w_gg, w_g+ including jackknife covariance if desired for lightcone data. Manages the various _measure_xi_rp_pi_obs and _measure_jackknife_covariance options in MeasureWObservations and MeasureJackknife. Parameters ---------- IA_estimator : str Choose which type of xi estimator is used. Choose from \"clusters\" or \"galaxies\". dataset_name : str Name of the dataset in the output file. corr_type : str Type of correlation to be measured. Choose from [g+, gg, both]. randoms_data : dict or NoneType Dictionary that includes the randoms data in the same form as the data dictionary. jk_patches : dict or NoneType, optional Dictionary with entries of the jackknife patch numbers (ndarray) for each sample, named \"position\", \"shape\" and \"random\". Default is None. num_jk : int, optional Number of jackknife patches to be generated internally. Default is None. measure_cov : bool, optional If True, jackknife errors are calculated. Default is True. masks : dict or NoneType, optional Dictionary of mask information in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. masks_randoms : dict or NoneType, optional Dictionary of mask information for the randoms data in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. cosmology : pyccl cosmology object or NoneType, optional Pyccl cosmology to use in the calculation. If None (default), the cosmology is used: ccl.Cosmology(Omega_c=0.225, Omega_b=0.045, sigma8=0.8, h=0.7, n_s=1.0) over_h : bool, optional If True, the units are assumed to be in not-over-h and converted to over-h units. Default is False. \"\"\" if IA_estimator == \"clusters\": if self.randoms_data == None: print(\"No randoms given, correlation defined as S+D/DD\") raise KeyError(\"This version does not work yet, add randoms.\") else: print(\"xi_g+ defined as S+D/SD - S+R/SR, xi_gg as (SD - RD - SR)/RR - 1\") if masks != None and masks_randoms == None: print(\"Warning, masks given for data vector but not for randoms.\") elif IA_estimator == \"galaxies\": if self.randoms_data == None: raise KeyError(\"No randoms given. Please provide input.\") else: print(\"xi_g+ defined as (S+D - S+R)/RR, xi_gg as (SD - RD - SR)/RR - 1\") if masks != None and masks_randoms == None: print(\"Warning, masks given for data vector but not for randoms.\") print(\"WARNING: this version of the code has not been fully validated. Proceed with caution.\") else: raise KeyError(\"Unknown input for IA_estimator, choose from [clusters, galaxies].\") # todo: Expand to include methods with trees and internal multiproc # todo: Checks to see if data directories include everything they need data = self.data # temporary save so it can be restored at the end of the calculation try: # Are there one or two random samples given? random_shape = self.randoms_data[\"RA_shape_sample\"] one_random_sample = False except: one_random_sample = True self.randoms_data[\"RA_shape_sample\"] = self.randoms_data[\"RA\"] self.randoms_data[\"DEC_shape_sample\"] = self.randoms_data[\"DEC\"] self.randoms_data[\"Redshift_shape_sample\"] = self.randoms_data[\"Redshift\"] try: weight = self.randoms_data[\"weight\"] except: self.randoms_data[\"weight\"] = np.ones(len(self.randoms_data[\"RA\"])) try: weight = self.randoms_data[\"weight_shape_sample\"] except: if one_random_sample: self.randoms_data[\"weight_shape_sample\"] = self.randoms_data[\"weight\"] # in case weights are given else: self.randoms_data[\"weight_shape_sample\"] = np.ones(len(self.randoms_data[\"RA_shape_sample\"])) if measure_cov: if jk_patches == None: if num_jk != None: jk_patches = self.assign_jackknife_patches(data, self.randoms_data, num_jk) else: raise ValueError(\"Set calc_errors to False, or provide either jk_patches or num_jk input.\") else: if one_random_sample: jk_patches[\"randoms_position\"] = jk_patches[\"randoms\"] jk_patches[\"randoms_shape\"] = jk_patches[\"randoms\"] self.data_dir = data try: weight = self.data_dir[\"weight\"] except: self.data_dir[\"weight\"] = np.ones(len(self.data_dir[\"RA\"])) try: weight = self.data_dir[\"weight_shape_sample\"] except: self.data_dir[\"weight_shape_sample\"] = np.ones(len(self.data_dir[\"RA_shape_sample\"])) num_samples = {} # Needed to correct for different number of randoms and galaxies/clusters in data if masks == None: num_samples[\"D\"] = len(self.data_dir[\"RA\"]) num_samples[\"S\"] = len(self.data_dir[\"RA_shape_sample\"]) else: num_samples[\"D\"] = len(self.data_dir[\"RA\"][masks[\"RA\"]]) num_samples[\"S\"] = len(self.data_dir[\"RA_shape_sample\"][masks[\"RA_shape_sample\"]]) if masks_randoms == None: num_samples[\"R_D\"] = len(self.randoms_data[\"RA\"]) num_samples[\"R_S\"] = len(self.randoms_data[\"RA_shape_sample\"]) else: num_samples[\"R_D\"] = len(self.randoms_data[\"RA\"][masks_randoms[\"RA\"]]) num_samples[\"R_S\"] = len(self.randoms_data[\"RA_shape_sample\"][masks_randoms[\"RA_shape_sample\"]]) # print(self.data_dir,self.randoms_data) # Shape-position combinations: # S+D (Cg+, Gg+) # S+R (Cg+, Gg+) if corr_type == \"g+\" or corr_type == \"both\": # S+D self.data = self.data_dir self._measure_xi_rp_pi_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology) # S+R self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"e1\": self.data_dir[\"e1\"], \"e2\": self.data_dir[\"e2\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } # print(self.data) self._measure_xi_rp_pi_lightcone_brute(masks=masks, dataset_name=f\"{dataset_name}_randoms\", over_h=over_h, cosmology=cosmology) # Position-position combinations: # SD (Cgg, Ggg) # SR (Cg+, Cgg, Ggg) # RD (Cgg, Ggg) # RR (Cgg, Gg+, Ggg) if corr_type == \"gg\": # already have it for 'both' # SD (Cgg, Ggg) self.data = { \"Redshift\": self.data_dir[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.data_dir[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.data_dir[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"weight\": self.data_dir[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._count_pairs_xi_rp_pi_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology, data_suffix=\"_DD\") # SR (Cg+, Cgg, Ggg) - watch name (Obs estimator) # if g+ or both, already have it self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._count_pairs_xi_rp_pi_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology, data_suffix=\"_SR\") if corr_type == \"gg\" or corr_type == \"both\": # RD (Cgg, Ggg) self.data = { \"Redshift\": self.data_dir[\"Redshift\"], \"Redshift_shape_sample\": self.randoms_data[\"Redshift_shape_sample\"], \"RA\": self.data_dir[\"RA\"], \"RA_shape_sample\": self.randoms_data[\"RA_shape_sample\"], \"DEC\": self.data_dir[\"DEC\"], \"DEC_shape_sample\": self.randoms_data[\"DEC_shape_sample\"], \"weight\": self.data_dir[\"weight\"], \"weight_shape_sample\": self.randoms_data[\"weight_shape_sample\"] } self._count_pairs_xi_rp_pi_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology, data_suffix=\"_RD\") if IA_estimator == \"galaxies\" or corr_type == \"gg\" or corr_type == \"both\": # RR (Cgg, Gg+, Ggg) self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.randoms_data[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.randoms_data[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.randoms_data[\"DEC_shape_sample\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.randoms_data[\"weight_shape_sample\"] } self._count_pairs_xi_rp_pi_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology, data_suffix=\"_RR\") self._obs_estimator([corr_type, \"w\"], IA_estimator, dataset_name, f\"{dataset_name}_randoms\", num_samples) self._measure_w_g_i(corr_type=corr_type, dataset_name=dataset_name, return_output=False) if measure_cov: self.num_samples = {} min_patch, max_patch = int(min(jk_patches[\"shape\"])), int(max(jk_patches[\"shape\"])) for n in np.arange(min_patch, max_patch + 1): self.num_samples[f\"{n}\"] = {} # Shape-position combinations: # S+D (Cg+, Gg+) # S+R (Cg+, Gg+) if corr_type == \"g+\" or corr_type == \"both\": # S+D self.data = self.data_dir self._measure_jackknife_realisations_lightcone_multiprocessing(patches_pos=jk_patches[\"position\"], patches_shape=jk_patches[\"shape\"], corr_type=[corr_type, \"w\"], masks=masks, dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=False, num_sample_names=[\"S\", \"D\"]) # S+R self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"e1\": self.data_dir[\"e1\"], \"e2\": self.data_dir[\"e2\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } # print(self.data) self._measure_jackknife_realisations_lightcone_multiprocessing( patches_pos=jk_patches[\"randoms_position\"], patches_shape=jk_patches[\"shape\"], corr_type=[corr_type, \"w\"], masks=masks, dataset_name=f\"{dataset_name}_randoms\", num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=False, num_sample_names=[\"S\", \"R_D\"]) # Position-position combinations: # SD (Cgg, Ggg) # SR (Cg+, Cgg, Ggg) # RD (Cgg, Ggg) # RR (Cgg, Gg+, Ggg) if corr_type == \"gg\": # already have it for 'both' # SD (Cgg, Ggg) self.data = { \"Redshift\": self.data_dir[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.data_dir[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.data_dir[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"weight\": self.data_dir[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing(patches_pos=jk_patches[\"position\"], patches_shape=jk_patches[\"shape\"], corr_type=[\"gg\", \"w\"], dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=True, data_suffix=\"_DD\", num_sample_names=[\"S\", \"D\"]) # SR (Cg+, Cgg, Ggg) - watch name (Obs estimator) # if g+ or both, already have it self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing( patches_pos=jk_patches[\"randoms_position\"], patches_shape=jk_patches[\"shape\"], corr_type=[\"gg\", \"w\"], dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=True, data_suffix=\"_SR\", num_sample_names=[\"S\", \"R_D\"]) if corr_type == \"gg\" or corr_type == \"both\": # RD (Cgg, Ggg) self.data = { \"Redshift\": self.data_dir[\"Redshift\"], \"Redshift_shape_sample\": self.randoms_data[\"Redshift_shape_sample\"], \"RA\": self.data_dir[\"RA\"], \"RA_shape_sample\": self.randoms_data[\"RA_shape_sample\"], \"DEC\": self.data_dir[\"DEC\"], \"DEC_shape_sample\": self.randoms_data[\"DEC_shape_sample\"], \"weight\": self.data_dir[\"weight\"], \"weight_shape_sample\": self.randoms_data[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing(patches_pos=jk_patches[\"position\"], patches_shape=jk_patches[ \"randoms_shape\"], corr_type=[\"gg\", \"w\"], dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=True, data_suffix=\"_RD\", num_sample_names=[\"R_S\", \"D\"]) if IA_estimator == \"galaxies\" or corr_type == \"gg\" or corr_type == \"both\": # RR (Cgg, Gg+, Ggg) self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.randoms_data[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.randoms_data[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.randoms_data[\"DEC_shape_sample\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.randoms_data[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing( patches_pos=jk_patches[\"randoms_position\"], patches_shape=jk_patches[\"randoms_shape\"], corr_type=[\"gg\", \"w\"], dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=True, data_suffix=\"_RR\", num_sample_names=[\"R_S\", \"R_D\"]) self._measure_jackknife_covariance_lightcone(IA_estimator=IA_estimator, max_patch=max(jk_patches['shape']), min_patch=min(jk_patches[\"shape\"]), corr_type=[corr_type, \"w\"], dataset_name=dataset_name, randoms_suf=\"_randoms\") self.data = data return def measure_xi_multipoles(self, IA_estimator, dataset_name, corr_type, jk_patches=None, num_jk=None, calc_errors=True, masks=None, masks_randoms=None, cosmology=None, over_h=False, rp_cut=None): \"\"\"Measures multipoles including jackknife covariance if desired for lightcone data. Manages the various _measure_xi_r_mu_r_obs and _measure_jackknife_covariance options in MeasureMultipolesObservations and MeasureJackknife. Parameters ---------- IA_estimator : str Choose which type of xi estimator is used. Choose from \"clusters\" or \"galaxies\". dataset_name : str Name of the dataset in the output file. corr_type : str Type of correlation to be measured. Choose from [g+, gg, both]. randoms_data : dict or NoneType Dictionary that includes the randoms data in the same form as the data dictionary. jk_patches : dict or NoneType, optional Dictionary with entries of the jackknife patch numbers (ndarray) for each sample, named \"position\", \"shape\" and \"random\". Default is None. num_jk : int, optional Number of jackknife patches to be generated internally. Default is None. measure_cov : bool, optional If True, jackknife errors are calculated. Default is True. masks : dict or NoneType, optional Dictionary of mask information in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. masks_randoms : dict or NoneType, optional Dictionary of mask information for the randoms data in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. cosmology : pyccl cosmology object or NoneType, optional Pyccl cosmology to use in the calculation. If None (default), the cosmology is used: ccl.Cosmology(Omega_c=0.225, Omega_b=0.045, sigma8=0.8, h=0.7, n_s=1.0) over_h : bool, optional If True, the units are assumed to be in not-over-h and converted to over-h units. Default is False. rp_cut : float or NoneType, optional Applies a minimum r_p value condition for pairs to be included. Default is None. Returns ------- \"\"\" if IA_estimator == \"clusters\": if self.randoms_data == None: print(\"No randoms given, correlation defined as S+D/DD\") raise KeyError(\"This version does not work yet, add randoms.\") else: print(\"xi_g+ defined as S+D/SD - S+R/SR, xi_gg as (SD - RD - SR)/RR - 1\") if masks != None and masks_randoms == None: print(\"Warning, masks given for data vector but not for randoms.\") elif IA_estimator == \"galaxies\": if self.randoms_data == None: raise KeyError(\"No randoms given. Please provide input.\") else: print(\"xi_g+ defined as (S+D - S+R)/RR, xi_gg as (SD - RD - SR)/RR - 1\") if masks != None and masks_randoms == None: print(\"Warning, masks given for data vector but not for randoms.\") print(\"WARNING: this version of the code has not been fully validated. Proceed with caution.\") else: raise KeyError(\"Unknown input for IA_estimator, choose from [clusters, galaxies].\") # todo: Expand to include methods with trees and internal multiproc # todo: Checks to see if data directories include everything they need data = self.data # temporary save so it can be restored at the end of the calculation try: # Are there one or two random samples given? random_shape = self.randoms_data[\"RA_shape_sample\"] one_random_sample = False except: one_random_sample = True self.randoms_data[\"RA_shape_sample\"] = self.randoms_data[\"RA\"] self.randoms_data[\"DEC_shape_sample\"] = self.randoms_data[\"DEC\"] self.randoms_data[\"Redshift_shape_sample\"] = self.randoms_data[\"Redshift\"] try: weight = self.randoms_data[\"weight\"] except: self.randoms_data[\"weight\"] = np.ones(len(self.randoms_data[\"RA\"])) try: weight = self.randoms_data[\"weight_shape_sample\"] except: if one_random_sample: self.randoms_data[\"weight_shape_sample\"] = self.randoms_data[\"weight\"] # in case weights are given else: self.randoms_data[\"weight_shape_sample\"] = np.ones(len(self.randoms_data[\"RA_shape_sample\"])) if calc_errors: if jk_patches == None: if num_jk != None: jk_patches = self.assign_jackknife_patches(data, self.randoms_data, num_jk) else: raise ValueError(\"Set calc_errors to False, or provide either jk_patches or num_jk input.\") else: if one_random_sample: jk_patches[\"randoms_position\"] = jk_patches[\"randoms\"] jk_patches[\"randoms_shape\"] = jk_patches[\"randoms\"] self.data_dir = data try: weight = self.data_dir[\"weight\"] except: self.data_dir[\"weight\"] = np.ones(len(self.data_dir[\"RA\"])) try: weight = self.data_dir[\"weight_shape_sample\"] except: self.data_dir[\"weight_shape_sample\"] = np.ones(len(self.data_dir[\"RA_shape_sample\"])) num_samples = {} # Needed to correct for different number of randoms and galaxies/clusters in data if masks == None: num_samples[\"D\"] = len(self.data_dir[\"RA\"]) num_samples[\"S\"] = len(self.data_dir[\"RA_shape_sample\"]) else: num_samples[\"D\"] = len(self.data_dir[\"RA\"][masks[\"RA\"]]) num_samples[\"S\"] = len(self.data_dir[\"RA_shape_sample\"][masks[\"RA_shape_sample\"]]) if masks_randoms == None: num_samples[\"R_D\"] = len(self.randoms_data[\"RA\"]) num_samples[\"R_S\"] = len(self.randoms_data[\"RA_shape_sample\"]) else: num_samples[\"R_D\"] = len(self.randoms_data[\"RA\"][masks_randoms[\"RA\"]]) num_samples[\"R_S\"] = len(self.randoms_data[\"RA_shape_sample\"][masks_randoms[\"RA_shape_sample\"]]) # print(self.data_dir,self.randoms_data) # Shape-position combinations: # S+D (Cg+, Gg+) # S+R (Cg+, Gg+) if corr_type == \"g+\" or corr_type == \"both\": # S+D self.data = self.data_dir self._measure_xi_r_mur_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, rp_cut=rp_cut, cosmology=cosmology) # S+R self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"e1\": self.data_dir[\"e1\"], \"e2\": self.data_dir[\"e2\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } # print(self.data) self._measure_xi_r_mur_lightcone_brute(masks=masks, dataset_name=f\"{dataset_name}_randoms\", over_h=over_h, rp_cut=rp_cut, cosmology=cosmology) # Position-position combinations: # SD (Cgg, Ggg) # SR (Cg+, Cgg, Ggg) # RD (Cgg, Ggg) # RR (Cgg, Gg+, Ggg) if corr_type == \"gg\": # already have it for 'both' # SD (Cgg, Ggg) self.data = { \"Redshift\": self.data_dir[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.data_dir[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.data_dir[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"weight\": self.data_dir[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._count_pairs_xi_r_mur_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology, data_suffix=\"_DD\", rp_cut=rp_cut) # SR (Cg+, Cgg, Ggg) - watch name (Obs estimator) # if g+ or both, already have it self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._count_pairs_xi_r_mur_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology, rp_cut=rp_cut, data_suffix=\"_SR\") if corr_type == \"gg\" or corr_type == \"both\": # RD (Cgg, Ggg) self.data = { \"Redshift\": self.data_dir[\"Redshift\"], \"Redshift_shape_sample\": self.randoms_data[\"Redshift_shape_sample\"], \"RA\": self.data_dir[\"RA\"], \"RA_shape_sample\": self.randoms_data[\"RA_shape_sample\"], \"DEC\": self.data_dir[\"DEC\"], \"DEC_shape_sample\": self.randoms_data[\"DEC_shape_sample\"], \"weight\": self.data_dir[\"weight\"], \"weight_shape_sample\": self.randoms_data[\"weight_shape_sample\"] } self._count_pairs_xi_r_mur_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology, rp_cut=rp_cut, data_suffix=\"_RD\") if IA_estimator == \"galaxies\" or corr_type == \"gg\" or corr_type == \"both\": # RR (Cgg, Gg+, Ggg) self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.randoms_data[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.randoms_data[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.randoms_data[\"DEC_shape_sample\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.randoms_data[\"weight_shape_sample\"] } self._count_pairs_xi_r_mur_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology, rp_cut=rp_cut, data_suffix=\"_RR\") self._obs_estimator([corr_type, \"multipoles\"], IA_estimator, dataset_name, f\"{dataset_name}_randoms\", num_samples) self._measure_multipoles(corr_type=corr_type, dataset_name=dataset_name, return_output=False) if calc_errors: self.num_samples = {} min_patch, max_patch = int(min(jk_patches[\"shape\"])), int(max(jk_patches[\"shape\"])) for n in np.arange(min_patch, max_patch + 1): self.num_samples[f\"{n}\"] = {} # Shape-position combinations: # S+D (Cg+, Gg+) # S+R (Cg+, Gg+) if corr_type == \"g+\" or corr_type == \"both\": # S+D self.data = self.data_dir self._measure_jackknife_realisations_lightcone_multiprocessing(patches_pos=jk_patches[\"position\"], patches_shape=jk_patches[\"shape\"], corr_type=[corr_type, \"multipoles\"], masks=masks, dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, rp_cut=rp_cut, cosmology=cosmology, count_pairs=False, num_sample_names=[\"S\", \"D\"]) # S+R self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"e1\": self.data_dir[\"e1\"], \"e2\": self.data_dir[\"e2\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing( patches_pos=jk_patches[\"randoms_position\"], patches_shape=jk_patches[\"shape\"], corr_type=[corr_type, \"multipoles\"], masks=masks, dataset_name=f\"{dataset_name}_randoms\", num_nodes=self.num_nodes, over_h=over_h, rp_cut=rp_cut, cosmology=cosmology, count_pairs=False, num_sample_names=[\"S\", \"R_D\"]) # Position-position combinations: # SD (Cgg, Ggg) # SR (Cg+, Cgg, Ggg) # RD (Cgg, Ggg) # RR (Cgg, Gg+, Ggg) if corr_type == \"gg\": # already have it for 'both' # SD (Cgg, Ggg) self.data = { \"Redshift\": self.data_dir[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.data_dir[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.data_dir[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"weight\": self.data_dir[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing(patches_pos=jk_patches[\"position\"], patches_shape=jk_patches[\"shape\"], corr_type=[\"gg\", \"multipoles\"], dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=True, rp_cut=rp_cut, data_suffix=\"_DD\", num_sample_names=[\"S\", \"D\"]) # SR (Cg+, Cgg, Ggg) - watch name (Obs estimator) # if g+ or both, already have it self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing( patches_pos=jk_patches[\"randoms_position\"], patches_shape=jk_patches[\"shape\"], corr_type=[\"gg\", \"multipoles\"], dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=True, rp_cut=rp_cut, data_suffix=\"_SR\", num_sample_names=[\"S\", \"R_D\"]) if corr_type == \"gg\" or corr_type == \"both\": # RD (Cgg, Ggg) self.data = { \"Redshift\": self.data_dir[\"Redshift\"], \"Redshift_shape_sample\": self.randoms_data[\"Redshift_shape_sample\"], \"RA\": self.data_dir[\"RA\"], \"RA_shape_sample\": self.randoms_data[\"RA_shape_sample\"], \"DEC\": self.data_dir[\"DEC\"], \"DEC_shape_sample\": self.randoms_data[\"DEC_shape_sample\"], \"weight\": self.data_dir[\"weight\"], \"weight_shape_sample\": self.randoms_data[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing(patches_pos=jk_patches[\"position\"], patches_shape=jk_patches[ \"randoms_shape\"], corr_type=[\"gg\", \"multipoles\"], dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=True, rp_cut=rp_cut, data_suffix=\"_RD\", num_sample_names=[\"R_S\", \"D\"]) if IA_estimator == \"galaxies\" or corr_type == \"gg\" or corr_type == \"both\": # RR (Cgg, Gg+, Ggg) self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.randoms_data[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.randoms_data[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.randoms_data[\"DEC_shape_sample\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.randoms_data[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing( patches_pos=jk_patches[\"randoms_position\"], patches_shape=jk_patches[\"randoms_shape\"], corr_type=[\"gg\", \"multipoles\"], dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=True, rp_cut=rp_cut, data_suffix=\"_RR\", num_sample_names=[\"R_S\", \"R_D\"]) self._measure_jackknife_covariance_lightcone(IA_estimator=IA_estimator, max_patch=max(jk_patches['shape']), min_patch=min(jk_patches[\"shape\"]), corr_type=[corr_type, \"multipoles\"], dataset_name=dataset_name, randoms_suf=\"_randoms\") self.data = data return","title":"MeasureIALightcone"},{"location":"api/measureIALightcone/#measureia.MeasureIALightcone.__init__","text":"The init method of the MeasureIALightcone class. Parameters: randoms_data ( dict or NoneType ) \u2013 Dictionary with data of the randoms needed for lightcone-type measurements. The keywords are: 'Redshift' and 'Redshift_shape_sample': (N_p) and (N_s) ndarray with redshifts of position and shape samples. 'RA' and 'RA_shape_sample': (N_p) and (N_s) ndarray with RA coordinate of position and shape samples. 'DEC' and 'DEC_shape_sample': (N_p) and (N_s) ndarray with DEC coordinate of position and shape samples. If only 'Redshift', 'RA' and 'DEC' are added, the sample will be used for both position and shape sample randoms. num_nodes ( int , default: 1 ) \u2013 Number of cores to be used in multiprocessing. Default is 1. Notes Constructor parameters 'data', 'output_file_name', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', are passed to MeasureIABase. Source code in src/measureia/measure_IA.py def __init__( self, data, randoms_data, output_file_name, separation_limits=[0.1, 20.0], num_bins_r=8, num_bins_pi=20, pi_max=None, num_nodes=1, ): \"\"\" The __init__ method of the MeasureIALightcone class. Parameters ---------- randoms_data : dict or NoneType Dictionary with data of the randoms needed for lightcone-type measurements. The keywords are: 'Redshift' and 'Redshift_shape_sample': (N_p) and (N_s) ndarray with redshifts of position and shape samples. 'RA' and 'RA_shape_sample': (N_p) and (N_s) ndarray with RA coordinate of position and shape samples. 'DEC' and 'DEC_shape_sample': (N_p) and (N_s) ndarray with DEC coordinate of position and shape samples. If only 'Redshift', 'RA' and 'DEC' are added, the sample will be used for both position and shape sample randoms. num_nodes : int, optional Number of cores to be used in multiprocessing. Default is 1. Notes ----- Constructor parameters 'data', 'output_file_name', 'separation_limits', 'num_bins_r', 'num_bins_pi', 'pi_max', are passed to MeasureIABase. \"\"\" super().__init__(data, output_file_name, False, None, separation_limits, num_bins_r, num_bins_pi, pi_max, None, False) self.num_nodes = num_nodes self.randoms_data = randoms_data self.data_dir = None self.num_samples = None return","title":"__init__"},{"location":"api/measureIALightcone/#measureia.MeasureIALightcone.measure_xi_w","text":"Measures xi_gg, xi_g+ and w_gg, w_g+ including jackknife covariance if desired for lightcone data. Manages the various _measure_xi_rp_pi_obs and _measure_jackknife_covariance options in MeasureWObservations and MeasureJackknife. Parameters: IA_estimator ( str ) \u2013 Choose which type of xi estimator is used. Choose from \"clusters\" or \"galaxies\". dataset_name ( str ) \u2013 Name of the dataset in the output file. corr_type ( str ) \u2013 Type of correlation to be measured. Choose from [g+, gg, both]. randoms_data ( dict or NoneType ) \u2013 Dictionary that includes the randoms data in the same form as the data dictionary. jk_patches ( dict or NoneType , default: None ) \u2013 Dictionary with entries of the jackknife patch numbers (ndarray) for each sample, named \"position\", \"shape\" and \"random\". Default is None. num_jk ( int , default: None ) \u2013 Number of jackknife patches to be generated internally. Default is None. measure_cov ( bool , default: True ) \u2013 If True, jackknife errors are calculated. Default is True. masks ( dict or NoneType , default: None ) \u2013 Dictionary of mask information in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. masks_randoms ( dict or NoneType , default: None ) \u2013 Dictionary of mask information for the randoms data in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. cosmology ( pyccl cosmology object or NoneType , default: None ) \u2013 Pyccl cosmology to use in the calculation. If None (default), the cosmology is used: ccl.Cosmology(Omega_c=0.225, Omega_b=0.045, sigma8=0.8, h=0.7, n_s=1.0) over_h ( bool , default: False ) \u2013 If True, the units are assumed to be in not-over-h and converted to over-h units. Default is False. Source code in src/measureia/measure_IA.py def measure_xi_w(self, IA_estimator, dataset_name, corr_type, jk_patches=None, num_jk=None, measure_cov=True, masks=None, masks_randoms=None, cosmology=None, over_h=False): \"\"\"Measures xi_gg, xi_g+ and w_gg, w_g+ including jackknife covariance if desired for lightcone data. Manages the various _measure_xi_rp_pi_obs and _measure_jackknife_covariance options in MeasureWObservations and MeasureJackknife. Parameters ---------- IA_estimator : str Choose which type of xi estimator is used. Choose from \"clusters\" or \"galaxies\". dataset_name : str Name of the dataset in the output file. corr_type : str Type of correlation to be measured. Choose from [g+, gg, both]. randoms_data : dict or NoneType Dictionary that includes the randoms data in the same form as the data dictionary. jk_patches : dict or NoneType, optional Dictionary with entries of the jackknife patch numbers (ndarray) for each sample, named \"position\", \"shape\" and \"random\". Default is None. num_jk : int, optional Number of jackknife patches to be generated internally. Default is None. measure_cov : bool, optional If True, jackknife errors are calculated. Default is True. masks : dict or NoneType, optional Dictionary of mask information in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. masks_randoms : dict or NoneType, optional Dictionary of mask information for the randoms data in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. cosmology : pyccl cosmology object or NoneType, optional Pyccl cosmology to use in the calculation. If None (default), the cosmology is used: ccl.Cosmology(Omega_c=0.225, Omega_b=0.045, sigma8=0.8, h=0.7, n_s=1.0) over_h : bool, optional If True, the units are assumed to be in not-over-h and converted to over-h units. Default is False. \"\"\" if IA_estimator == \"clusters\": if self.randoms_data == None: print(\"No randoms given, correlation defined as S+D/DD\") raise KeyError(\"This version does not work yet, add randoms.\") else: print(\"xi_g+ defined as S+D/SD - S+R/SR, xi_gg as (SD - RD - SR)/RR - 1\") if masks != None and masks_randoms == None: print(\"Warning, masks given for data vector but not for randoms.\") elif IA_estimator == \"galaxies\": if self.randoms_data == None: raise KeyError(\"No randoms given. Please provide input.\") else: print(\"xi_g+ defined as (S+D - S+R)/RR, xi_gg as (SD - RD - SR)/RR - 1\") if masks != None and masks_randoms == None: print(\"Warning, masks given for data vector but not for randoms.\") print(\"WARNING: this version of the code has not been fully validated. Proceed with caution.\") else: raise KeyError(\"Unknown input for IA_estimator, choose from [clusters, galaxies].\") # todo: Expand to include methods with trees and internal multiproc # todo: Checks to see if data directories include everything they need data = self.data # temporary save so it can be restored at the end of the calculation try: # Are there one or two random samples given? random_shape = self.randoms_data[\"RA_shape_sample\"] one_random_sample = False except: one_random_sample = True self.randoms_data[\"RA_shape_sample\"] = self.randoms_data[\"RA\"] self.randoms_data[\"DEC_shape_sample\"] = self.randoms_data[\"DEC\"] self.randoms_data[\"Redshift_shape_sample\"] = self.randoms_data[\"Redshift\"] try: weight = self.randoms_data[\"weight\"] except: self.randoms_data[\"weight\"] = np.ones(len(self.randoms_data[\"RA\"])) try: weight = self.randoms_data[\"weight_shape_sample\"] except: if one_random_sample: self.randoms_data[\"weight_shape_sample\"] = self.randoms_data[\"weight\"] # in case weights are given else: self.randoms_data[\"weight_shape_sample\"] = np.ones(len(self.randoms_data[\"RA_shape_sample\"])) if measure_cov: if jk_patches == None: if num_jk != None: jk_patches = self.assign_jackknife_patches(data, self.randoms_data, num_jk) else: raise ValueError(\"Set calc_errors to False, or provide either jk_patches or num_jk input.\") else: if one_random_sample: jk_patches[\"randoms_position\"] = jk_patches[\"randoms\"] jk_patches[\"randoms_shape\"] = jk_patches[\"randoms\"] self.data_dir = data try: weight = self.data_dir[\"weight\"] except: self.data_dir[\"weight\"] = np.ones(len(self.data_dir[\"RA\"])) try: weight = self.data_dir[\"weight_shape_sample\"] except: self.data_dir[\"weight_shape_sample\"] = np.ones(len(self.data_dir[\"RA_shape_sample\"])) num_samples = {} # Needed to correct for different number of randoms and galaxies/clusters in data if masks == None: num_samples[\"D\"] = len(self.data_dir[\"RA\"]) num_samples[\"S\"] = len(self.data_dir[\"RA_shape_sample\"]) else: num_samples[\"D\"] = len(self.data_dir[\"RA\"][masks[\"RA\"]]) num_samples[\"S\"] = len(self.data_dir[\"RA_shape_sample\"][masks[\"RA_shape_sample\"]]) if masks_randoms == None: num_samples[\"R_D\"] = len(self.randoms_data[\"RA\"]) num_samples[\"R_S\"] = len(self.randoms_data[\"RA_shape_sample\"]) else: num_samples[\"R_D\"] = len(self.randoms_data[\"RA\"][masks_randoms[\"RA\"]]) num_samples[\"R_S\"] = len(self.randoms_data[\"RA_shape_sample\"][masks_randoms[\"RA_shape_sample\"]]) # print(self.data_dir,self.randoms_data) # Shape-position combinations: # S+D (Cg+, Gg+) # S+R (Cg+, Gg+) if corr_type == \"g+\" or corr_type == \"both\": # S+D self.data = self.data_dir self._measure_xi_rp_pi_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology) # S+R self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"e1\": self.data_dir[\"e1\"], \"e2\": self.data_dir[\"e2\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } # print(self.data) self._measure_xi_rp_pi_lightcone_brute(masks=masks, dataset_name=f\"{dataset_name}_randoms\", over_h=over_h, cosmology=cosmology) # Position-position combinations: # SD (Cgg, Ggg) # SR (Cg+, Cgg, Ggg) # RD (Cgg, Ggg) # RR (Cgg, Gg+, Ggg) if corr_type == \"gg\": # already have it for 'both' # SD (Cgg, Ggg) self.data = { \"Redshift\": self.data_dir[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.data_dir[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.data_dir[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"weight\": self.data_dir[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._count_pairs_xi_rp_pi_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology, data_suffix=\"_DD\") # SR (Cg+, Cgg, Ggg) - watch name (Obs estimator) # if g+ or both, already have it self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._count_pairs_xi_rp_pi_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology, data_suffix=\"_SR\") if corr_type == \"gg\" or corr_type == \"both\": # RD (Cgg, Ggg) self.data = { \"Redshift\": self.data_dir[\"Redshift\"], \"Redshift_shape_sample\": self.randoms_data[\"Redshift_shape_sample\"], \"RA\": self.data_dir[\"RA\"], \"RA_shape_sample\": self.randoms_data[\"RA_shape_sample\"], \"DEC\": self.data_dir[\"DEC\"], \"DEC_shape_sample\": self.randoms_data[\"DEC_shape_sample\"], \"weight\": self.data_dir[\"weight\"], \"weight_shape_sample\": self.randoms_data[\"weight_shape_sample\"] } self._count_pairs_xi_rp_pi_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology, data_suffix=\"_RD\") if IA_estimator == \"galaxies\" or corr_type == \"gg\" or corr_type == \"both\": # RR (Cgg, Gg+, Ggg) self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.randoms_data[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.randoms_data[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.randoms_data[\"DEC_shape_sample\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.randoms_data[\"weight_shape_sample\"] } self._count_pairs_xi_rp_pi_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology, data_suffix=\"_RR\") self._obs_estimator([corr_type, \"w\"], IA_estimator, dataset_name, f\"{dataset_name}_randoms\", num_samples) self._measure_w_g_i(corr_type=corr_type, dataset_name=dataset_name, return_output=False) if measure_cov: self.num_samples = {} min_patch, max_patch = int(min(jk_patches[\"shape\"])), int(max(jk_patches[\"shape\"])) for n in np.arange(min_patch, max_patch + 1): self.num_samples[f\"{n}\"] = {} # Shape-position combinations: # S+D (Cg+, Gg+) # S+R (Cg+, Gg+) if corr_type == \"g+\" or corr_type == \"both\": # S+D self.data = self.data_dir self._measure_jackknife_realisations_lightcone_multiprocessing(patches_pos=jk_patches[\"position\"], patches_shape=jk_patches[\"shape\"], corr_type=[corr_type, \"w\"], masks=masks, dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=False, num_sample_names=[\"S\", \"D\"]) # S+R self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"e1\": self.data_dir[\"e1\"], \"e2\": self.data_dir[\"e2\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } # print(self.data) self._measure_jackknife_realisations_lightcone_multiprocessing( patches_pos=jk_patches[\"randoms_position\"], patches_shape=jk_patches[\"shape\"], corr_type=[corr_type, \"w\"], masks=masks, dataset_name=f\"{dataset_name}_randoms\", num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=False, num_sample_names=[\"S\", \"R_D\"]) # Position-position combinations: # SD (Cgg, Ggg) # SR (Cg+, Cgg, Ggg) # RD (Cgg, Ggg) # RR (Cgg, Gg+, Ggg) if corr_type == \"gg\": # already have it for 'both' # SD (Cgg, Ggg) self.data = { \"Redshift\": self.data_dir[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.data_dir[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.data_dir[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"weight\": self.data_dir[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing(patches_pos=jk_patches[\"position\"], patches_shape=jk_patches[\"shape\"], corr_type=[\"gg\", \"w\"], dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=True, data_suffix=\"_DD\", num_sample_names=[\"S\", \"D\"]) # SR (Cg+, Cgg, Ggg) - watch name (Obs estimator) # if g+ or both, already have it self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing( patches_pos=jk_patches[\"randoms_position\"], patches_shape=jk_patches[\"shape\"], corr_type=[\"gg\", \"w\"], dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=True, data_suffix=\"_SR\", num_sample_names=[\"S\", \"R_D\"]) if corr_type == \"gg\" or corr_type == \"both\": # RD (Cgg, Ggg) self.data = { \"Redshift\": self.data_dir[\"Redshift\"], \"Redshift_shape_sample\": self.randoms_data[\"Redshift_shape_sample\"], \"RA\": self.data_dir[\"RA\"], \"RA_shape_sample\": self.randoms_data[\"RA_shape_sample\"], \"DEC\": self.data_dir[\"DEC\"], \"DEC_shape_sample\": self.randoms_data[\"DEC_shape_sample\"], \"weight\": self.data_dir[\"weight\"], \"weight_shape_sample\": self.randoms_data[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing(patches_pos=jk_patches[\"position\"], patches_shape=jk_patches[ \"randoms_shape\"], corr_type=[\"gg\", \"w\"], dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=True, data_suffix=\"_RD\", num_sample_names=[\"R_S\", \"D\"]) if IA_estimator == \"galaxies\" or corr_type == \"gg\" or corr_type == \"both\": # RR (Cgg, Gg+, Ggg) self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.randoms_data[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.randoms_data[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.randoms_data[\"DEC_shape_sample\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.randoms_data[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing( patches_pos=jk_patches[\"randoms_position\"], patches_shape=jk_patches[\"randoms_shape\"], corr_type=[\"gg\", \"w\"], dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=True, data_suffix=\"_RR\", num_sample_names=[\"R_S\", \"R_D\"]) self._measure_jackknife_covariance_lightcone(IA_estimator=IA_estimator, max_patch=max(jk_patches['shape']), min_patch=min(jk_patches[\"shape\"]), corr_type=[corr_type, \"w\"], dataset_name=dataset_name, randoms_suf=\"_randoms\") self.data = data return","title":"measure_xi_w"},{"location":"api/measureIALightcone/#measureia.MeasureIALightcone.measure_xi_multipoles","text":"Measures multipoles including jackknife covariance if desired for lightcone data. Manages the various _measure_xi_r_mu_r_obs and _measure_jackknife_covariance options in MeasureMultipolesObservations and MeasureJackknife. Parameters: IA_estimator ( str ) \u2013 Choose which type of xi estimator is used. Choose from \"clusters\" or \"galaxies\". dataset_name ( str ) \u2013 Name of the dataset in the output file. corr_type ( str ) \u2013 Type of correlation to be measured. Choose from [g+, gg, both]. randoms_data ( dict or NoneType ) \u2013 Dictionary that includes the randoms data in the same form as the data dictionary. jk_patches ( dict or NoneType , default: None ) \u2013 Dictionary with entries of the jackknife patch numbers (ndarray) for each sample, named \"position\", \"shape\" and \"random\". Default is None. num_jk ( int , default: None ) \u2013 Number of jackknife patches to be generated internally. Default is None. measure_cov ( bool ) \u2013 If True, jackknife errors are calculated. Default is True. masks ( dict or NoneType , default: None ) \u2013 Dictionary of mask information in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. masks_randoms ( dict or NoneType , default: None ) \u2013 Dictionary of mask information for the randoms data in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. cosmology ( pyccl cosmology object or NoneType , default: None ) \u2013 Pyccl cosmology to use in the calculation. If None (default), the cosmology is used: ccl.Cosmology(Omega_c=0.225, Omega_b=0.045, sigma8=0.8, h=0.7, n_s=1.0) over_h ( bool , default: False ) \u2013 If True, the units are assumed to be in not-over-h and converted to over-h units. Default is False. rp_cut ( float or NoneType , default: None ) \u2013 Applies a minimum r_p value condition for pairs to be included. Default is None. Source code in src/measureia/measure_IA.py def measure_xi_multipoles(self, IA_estimator, dataset_name, corr_type, jk_patches=None, num_jk=None, calc_errors=True, masks=None, masks_randoms=None, cosmology=None, over_h=False, rp_cut=None): \"\"\"Measures multipoles including jackknife covariance if desired for lightcone data. Manages the various _measure_xi_r_mu_r_obs and _measure_jackknife_covariance options in MeasureMultipolesObservations and MeasureJackknife. Parameters ---------- IA_estimator : str Choose which type of xi estimator is used. Choose from \"clusters\" or \"galaxies\". dataset_name : str Name of the dataset in the output file. corr_type : str Type of correlation to be measured. Choose from [g+, gg, both]. randoms_data : dict or NoneType Dictionary that includes the randoms data in the same form as the data dictionary. jk_patches : dict or NoneType, optional Dictionary with entries of the jackknife patch numbers (ndarray) for each sample, named \"position\", \"shape\" and \"random\". Default is None. num_jk : int, optional Number of jackknife patches to be generated internally. Default is None. measure_cov : bool, optional If True, jackknife errors are calculated. Default is True. masks : dict or NoneType, optional Dictionary of mask information in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. masks_randoms : dict or NoneType, optional Dictionary of mask information for the randoms data in the same form as the data dictionary, where the masks are placed over the data to apply selections. Default is None. cosmology : pyccl cosmology object or NoneType, optional Pyccl cosmology to use in the calculation. If None (default), the cosmology is used: ccl.Cosmology(Omega_c=0.225, Omega_b=0.045, sigma8=0.8, h=0.7, n_s=1.0) over_h : bool, optional If True, the units are assumed to be in not-over-h and converted to over-h units. Default is False. rp_cut : float or NoneType, optional Applies a minimum r_p value condition for pairs to be included. Default is None. Returns ------- \"\"\" if IA_estimator == \"clusters\": if self.randoms_data == None: print(\"No randoms given, correlation defined as S+D/DD\") raise KeyError(\"This version does not work yet, add randoms.\") else: print(\"xi_g+ defined as S+D/SD - S+R/SR, xi_gg as (SD - RD - SR)/RR - 1\") if masks != None and masks_randoms == None: print(\"Warning, masks given for data vector but not for randoms.\") elif IA_estimator == \"galaxies\": if self.randoms_data == None: raise KeyError(\"No randoms given. Please provide input.\") else: print(\"xi_g+ defined as (S+D - S+R)/RR, xi_gg as (SD - RD - SR)/RR - 1\") if masks != None and masks_randoms == None: print(\"Warning, masks given for data vector but not for randoms.\") print(\"WARNING: this version of the code has not been fully validated. Proceed with caution.\") else: raise KeyError(\"Unknown input for IA_estimator, choose from [clusters, galaxies].\") # todo: Expand to include methods with trees and internal multiproc # todo: Checks to see if data directories include everything they need data = self.data # temporary save so it can be restored at the end of the calculation try: # Are there one or two random samples given? random_shape = self.randoms_data[\"RA_shape_sample\"] one_random_sample = False except: one_random_sample = True self.randoms_data[\"RA_shape_sample\"] = self.randoms_data[\"RA\"] self.randoms_data[\"DEC_shape_sample\"] = self.randoms_data[\"DEC\"] self.randoms_data[\"Redshift_shape_sample\"] = self.randoms_data[\"Redshift\"] try: weight = self.randoms_data[\"weight\"] except: self.randoms_data[\"weight\"] = np.ones(len(self.randoms_data[\"RA\"])) try: weight = self.randoms_data[\"weight_shape_sample\"] except: if one_random_sample: self.randoms_data[\"weight_shape_sample\"] = self.randoms_data[\"weight\"] # in case weights are given else: self.randoms_data[\"weight_shape_sample\"] = np.ones(len(self.randoms_data[\"RA_shape_sample\"])) if calc_errors: if jk_patches == None: if num_jk != None: jk_patches = self.assign_jackknife_patches(data, self.randoms_data, num_jk) else: raise ValueError(\"Set calc_errors to False, or provide either jk_patches or num_jk input.\") else: if one_random_sample: jk_patches[\"randoms_position\"] = jk_patches[\"randoms\"] jk_patches[\"randoms_shape\"] = jk_patches[\"randoms\"] self.data_dir = data try: weight = self.data_dir[\"weight\"] except: self.data_dir[\"weight\"] = np.ones(len(self.data_dir[\"RA\"])) try: weight = self.data_dir[\"weight_shape_sample\"] except: self.data_dir[\"weight_shape_sample\"] = np.ones(len(self.data_dir[\"RA_shape_sample\"])) num_samples = {} # Needed to correct for different number of randoms and galaxies/clusters in data if masks == None: num_samples[\"D\"] = len(self.data_dir[\"RA\"]) num_samples[\"S\"] = len(self.data_dir[\"RA_shape_sample\"]) else: num_samples[\"D\"] = len(self.data_dir[\"RA\"][masks[\"RA\"]]) num_samples[\"S\"] = len(self.data_dir[\"RA_shape_sample\"][masks[\"RA_shape_sample\"]]) if masks_randoms == None: num_samples[\"R_D\"] = len(self.randoms_data[\"RA\"]) num_samples[\"R_S\"] = len(self.randoms_data[\"RA_shape_sample\"]) else: num_samples[\"R_D\"] = len(self.randoms_data[\"RA\"][masks_randoms[\"RA\"]]) num_samples[\"R_S\"] = len(self.randoms_data[\"RA_shape_sample\"][masks_randoms[\"RA_shape_sample\"]]) # print(self.data_dir,self.randoms_data) # Shape-position combinations: # S+D (Cg+, Gg+) # S+R (Cg+, Gg+) if corr_type == \"g+\" or corr_type == \"both\": # S+D self.data = self.data_dir self._measure_xi_r_mur_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, rp_cut=rp_cut, cosmology=cosmology) # S+R self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"e1\": self.data_dir[\"e1\"], \"e2\": self.data_dir[\"e2\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } # print(self.data) self._measure_xi_r_mur_lightcone_brute(masks=masks, dataset_name=f\"{dataset_name}_randoms\", over_h=over_h, rp_cut=rp_cut, cosmology=cosmology) # Position-position combinations: # SD (Cgg, Ggg) # SR (Cg+, Cgg, Ggg) # RD (Cgg, Ggg) # RR (Cgg, Gg+, Ggg) if corr_type == \"gg\": # already have it for 'both' # SD (Cgg, Ggg) self.data = { \"Redshift\": self.data_dir[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.data_dir[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.data_dir[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"weight\": self.data_dir[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._count_pairs_xi_r_mur_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology, data_suffix=\"_DD\", rp_cut=rp_cut) # SR (Cg+, Cgg, Ggg) - watch name (Obs estimator) # if g+ or both, already have it self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._count_pairs_xi_r_mur_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology, rp_cut=rp_cut, data_suffix=\"_SR\") if corr_type == \"gg\" or corr_type == \"both\": # RD (Cgg, Ggg) self.data = { \"Redshift\": self.data_dir[\"Redshift\"], \"Redshift_shape_sample\": self.randoms_data[\"Redshift_shape_sample\"], \"RA\": self.data_dir[\"RA\"], \"RA_shape_sample\": self.randoms_data[\"RA_shape_sample\"], \"DEC\": self.data_dir[\"DEC\"], \"DEC_shape_sample\": self.randoms_data[\"DEC_shape_sample\"], \"weight\": self.data_dir[\"weight\"], \"weight_shape_sample\": self.randoms_data[\"weight_shape_sample\"] } self._count_pairs_xi_r_mur_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology, rp_cut=rp_cut, data_suffix=\"_RD\") if IA_estimator == \"galaxies\" or corr_type == \"gg\" or corr_type == \"both\": # RR (Cgg, Gg+, Ggg) self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.randoms_data[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.randoms_data[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.randoms_data[\"DEC_shape_sample\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.randoms_data[\"weight_shape_sample\"] } self._count_pairs_xi_r_mur_lightcone_brute(masks=masks, dataset_name=dataset_name, over_h=over_h, cosmology=cosmology, rp_cut=rp_cut, data_suffix=\"_RR\") self._obs_estimator([corr_type, \"multipoles\"], IA_estimator, dataset_name, f\"{dataset_name}_randoms\", num_samples) self._measure_multipoles(corr_type=corr_type, dataset_name=dataset_name, return_output=False) if calc_errors: self.num_samples = {} min_patch, max_patch = int(min(jk_patches[\"shape\"])), int(max(jk_patches[\"shape\"])) for n in np.arange(min_patch, max_patch + 1): self.num_samples[f\"{n}\"] = {} # Shape-position combinations: # S+D (Cg+, Gg+) # S+R (Cg+, Gg+) if corr_type == \"g+\" or corr_type == \"both\": # S+D self.data = self.data_dir self._measure_jackknife_realisations_lightcone_multiprocessing(patches_pos=jk_patches[\"position\"], patches_shape=jk_patches[\"shape\"], corr_type=[corr_type, \"multipoles\"], masks=masks, dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, rp_cut=rp_cut, cosmology=cosmology, count_pairs=False, num_sample_names=[\"S\", \"D\"]) # S+R self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"e1\": self.data_dir[\"e1\"], \"e2\": self.data_dir[\"e2\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing( patches_pos=jk_patches[\"randoms_position\"], patches_shape=jk_patches[\"shape\"], corr_type=[corr_type, \"multipoles\"], masks=masks, dataset_name=f\"{dataset_name}_randoms\", num_nodes=self.num_nodes, over_h=over_h, rp_cut=rp_cut, cosmology=cosmology, count_pairs=False, num_sample_names=[\"S\", \"R_D\"]) # Position-position combinations: # SD (Cgg, Ggg) # SR (Cg+, Cgg, Ggg) # RD (Cgg, Ggg) # RR (Cgg, Gg+, Ggg) if corr_type == \"gg\": # already have it for 'both' # SD (Cgg, Ggg) self.data = { \"Redshift\": self.data_dir[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.data_dir[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.data_dir[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"weight\": self.data_dir[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing(patches_pos=jk_patches[\"position\"], patches_shape=jk_patches[\"shape\"], corr_type=[\"gg\", \"multipoles\"], dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=True, rp_cut=rp_cut, data_suffix=\"_DD\", num_sample_names=[\"S\", \"D\"]) # SR (Cg+, Cgg, Ggg) - watch name (Obs estimator) # if g+ or both, already have it self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.data_dir[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.data_dir[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.data_dir[\"DEC_shape_sample\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.data_dir[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing( patches_pos=jk_patches[\"randoms_position\"], patches_shape=jk_patches[\"shape\"], corr_type=[\"gg\", \"multipoles\"], dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=True, rp_cut=rp_cut, data_suffix=\"_SR\", num_sample_names=[\"S\", \"R_D\"]) if corr_type == \"gg\" or corr_type == \"both\": # RD (Cgg, Ggg) self.data = { \"Redshift\": self.data_dir[\"Redshift\"], \"Redshift_shape_sample\": self.randoms_data[\"Redshift_shape_sample\"], \"RA\": self.data_dir[\"RA\"], \"RA_shape_sample\": self.randoms_data[\"RA_shape_sample\"], \"DEC\": self.data_dir[\"DEC\"], \"DEC_shape_sample\": self.randoms_data[\"DEC_shape_sample\"], \"weight\": self.data_dir[\"weight\"], \"weight_shape_sample\": self.randoms_data[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing(patches_pos=jk_patches[\"position\"], patches_shape=jk_patches[ \"randoms_shape\"], corr_type=[\"gg\", \"multipoles\"], dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=True, rp_cut=rp_cut, data_suffix=\"_RD\", num_sample_names=[\"R_S\", \"D\"]) if IA_estimator == \"galaxies\" or corr_type == \"gg\" or corr_type == \"both\": # RR (Cgg, Gg+, Ggg) self.data = { \"Redshift\": self.randoms_data[\"Redshift\"], \"Redshift_shape_sample\": self.randoms_data[\"Redshift_shape_sample\"], \"RA\": self.randoms_data[\"RA\"], \"RA_shape_sample\": self.randoms_data[\"RA_shape_sample\"], \"DEC\": self.randoms_data[\"DEC\"], \"DEC_shape_sample\": self.randoms_data[\"DEC_shape_sample\"], \"weight\": self.randoms_data[\"weight\"], \"weight_shape_sample\": self.randoms_data[\"weight_shape_sample\"] } self._measure_jackknife_realisations_lightcone_multiprocessing( patches_pos=jk_patches[\"randoms_position\"], patches_shape=jk_patches[\"randoms_shape\"], corr_type=[\"gg\", \"multipoles\"], dataset_name=dataset_name, num_nodes=self.num_nodes, over_h=over_h, cosmology=cosmology, count_pairs=True, rp_cut=rp_cut, data_suffix=\"_RR\", num_sample_names=[\"R_S\", \"R_D\"]) self._measure_jackknife_covariance_lightcone(IA_estimator=IA_estimator, max_patch=max(jk_patches['shape']), min_patch=min(jk_patches[\"shape\"]), corr_type=[corr_type, \"multipoles\"], dataset_name=dataset_name, randoms_suf=\"_randoms\") self.data = data return","title":"measure_xi_multipoles"}]}